{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da48254-33f2-4002-8294-b378921bdccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMinimal Google News RSS → Full-text → DataFrame pipeline\\n- Step 1: Collect news (RSS via feedparser)\\n- Step 2: Extract article body (trafilatura)\\n- Step 3: Tidy into a DataFrame (with dedup)\\n- Step 4: (Optional) Summarize via ChatOllama (Mistral)\\n\\nRequirements:\\n  pip install feedparser trafilatura pandas tldextract tqdm\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal Google News RSS → Full-text → DataFrame pipeline\n",
    "- Step 1: Collect news (RSS via feedparser)\n",
    "- Step 2: Extract article body (trafilatura)\n",
    "- Step 3: Tidy into a DataFrame (with dedup)\n",
    "- Step 4: (Optional) Summarize via ChatOllama (Mistral)\n",
    "\n",
    "Requirements:\n",
    "  pip install feedparser trafilatura pandas tldextract tqdm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb4b09ae-dee2-4033-a940-98602b70b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChatOllama ready\n",
      " LangChain is an open-source project that aims to create a decentralized, blockchain-based platform for building and deploying natural language processing (NLP) models. The goal of LangChain is to make it easier for developers to build and share NLP models, while also ensuring that the data used to train these models is secure, private, and transparent.\n",
      "\n",
      "LangChain uses a combination of smart contracts on the Ethereum blockchain and machine learning algorithms to enable the creation and deployment of NLP models in a decentralized manner. The platform allows developers to train models using their own data, while also providing tools for collaborating with others and sharing models. LangChain also includes mechanisms for incentivizing participation in the network through the use of its native cryptocurrency, LGNT.\n",
      "\n",
      "Overall, LangChain is designed to address some of the challenges associated with building and deploying NLP models, such as data privacy, security, and scalability, by leveraging blockchain technology and a decentralized approach.\n"
     ]
    }
   ],
   "source": [
    "# --- LLM Configuration (LangChain + Ollama) ---\n",
    "LLM_NAME = \"mistral\"\n",
    "TEMPERATURE = 0.1\n",
    "TIMEOUT = 60.0  # seconds\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 1) llm \n",
    "llm = ChatOllama(\n",
    "    model=LLM_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "print(\"✅ ChatOllama ready\")\n",
    "\n",
    "# 2) test\n",
    "response = llm.invoke(\"Hello! Can you summarize what LangChain is?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b5f83078-6557-40d9-85b0-153f7fdf52a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jessicahong/nlp_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73ae029-0125-447e-b168-bb1335482998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from urllib.parse import quote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0211560b-f198-4fdc-910e-1715ae323d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                title  \\\n",
      "0                   Watch: Filming of new Harry Potter series spotted in London - BBC   \n",
      "1  See photos of new-look Weasley siblings in HBO's 'Harry Potter' series - USA Today   \n",
      "\n",
      "                                                                                                  link  \\\n",
      "0  https://news.google.com/rss/articles/CBMiV0FVX3lxTE1RdWpFVGI0cXVrdDh4b181V0dpWWdvcTZBREdwUmNpbm9...   \n",
      "1  https://news.google.com/rss/articles/CBMipAFBVV95cUxNaGJGb2xJa2J5UmRzQVJYdkFQck1pVTFkZW9pRkVTXzV...   \n",
      "\n",
      "                       published     source  \n",
      "0  Tue, 19 Aug 2025 14:38:00 GMT        BBC  \n",
      "1  Tue, 19 Aug 2025 19:30:00 GMT  USA Today  \n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: RSS Function ---\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from urllib.parse import quote\n",
    "\n",
    "def google_news_rss_min_df_en(query: str = \"Harry Potter\", limit: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch basic metadata from Google News (US/English) RSS.\n",
    "    Returns: DataFrame[title, link, published, source]\n",
    "    \"\"\"\n",
    "    url = f\"https://news.google.com/rss/search?q={quote(query)}&hl=en&gl=US&ceid=US:en\"\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    rows = []\n",
    "    for entry in feed.entries[:limit]:\n",
    "        title = (getattr(entry, \"title\", \"\") or \"\").strip()\n",
    "        link = getattr(entry, \"link\", \"\") or \"\"\n",
    "        published = getattr(entry, \"published\", \"\") or \"\"\n",
    "        source = getattr(getattr(entry, \"source\", None), \"title\", \"\") or \"\"\n",
    "        rows.append({\"title\": title, \"link\": link, \"published\": published, \"source\": source})\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"title\", \"link\", \"published\", \"source\"])\n",
    "\n",
    "    # Deduplication (first by title → then by link)\n",
    "    df = df.drop_duplicates(subset=\"title\", keep=\"first\")\n",
    "    df = df.drop_duplicates(subset=\"link\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Test run ---\n",
    "if __name__ == \"__main__\":\n",
    "    df = google_news_rss_min_df_en()  # Default query = \"Harry Potter\", 2 articles\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "336f128c-642a-4e6c-b26d-afb3eecfbf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Article Extraction---\n",
    "import trafilatura\n",
    "\n",
    "def extract_article_text(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the main body text from a news article URL.\n",
    "    - Filtering (e.g., length check) will be handled in Step 3.\n",
    "    - Returns an empty string if extraction fails.\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if not downloaded:\n",
    "            return \"\"\n",
    "        text = trafilatura.extract(\n",
    "            downloaded,\n",
    "            include_comments=False,\n",
    "            include_tables=False\n",
    "        )\n",
    "        return (text or \"\").strip()\n",
    "    except Exception:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9c4a6be-156c-4359-b7fe-b816532b9950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                title  \\\n",
      "0  See photos of new-look Weasley siblings in HBO's 'Harry Potter' series - USA Today   \n",
      "1                   Watch: Filming of new Harry Potter series spotted in London - BBC   \n",
      "\n",
      "                                                                                                  link  \n",
      "0  https://news.google.com/rss/articles/CBMipAFBVV95cUxNaGJGb2xJa2J5UmRzQVJYdkFQck1pVTFkZW9pRkVTXzV...  \n",
      "1  https://news.google.com/rss/articles/CBMiV0FVX3lxTE1RdWpFVGI0cXVrdDh4b181V0dpWWdvcTZBREdwUmNpbm9...  \n",
      "\n",
      "=== Step 2: Article Extraction Test ===\n",
      "Original: https://news.google.com/rss/articles/CBMipAFBVV95cUxNaGJGb2xJa2J5UmRzQVJYdkFQck1pVTFkZW9pRkVTXzVoWkJULVYyRlYxeUFMMG9TTjc1NU1NbjB1NWZvc3pWeF9YdHBJLURnMjZ1Y0E2Tlk4Q0UtQmxrV21pZS1yNVh1RFBPaFA1S2NsNGpReDFZNVVVcU9nYUUzM2xYVERSYjN5LWRfTERTbE0wM2U5dkFQWjZuUFMyYjEzcTR0Yw?oc=5\n",
      "Final   : https://news.google.com/rss/articles/CBMipAFBVV95cUxNaGJGb2xJa2J5UmRzQVJYdkFQck1pVTFkZW9pRkVTXzVoWkJULVYyRlYxeUFMMG9TTjc1NU1NbjB1NWZvc3pWeF9YdHBJLURnMjZ1Y0E2Tlk4Q0UtQmxrV21pZS1yNVh1RFBPaFA1S2NsNGpReDFZNVVVcU9nYUUzM2xYVERSYjN5LWRfTERTbE0wM2U5dkFQWjZuUFMyYjEzcTR0Yw?oc=5&hl=en-US&gl=US&ceid=US:en\n",
      "(No content extracted)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Article Extraction test using Step 1 output ---\n",
    "# Prereqs: google_news_rss_min_df_en, resolve_final_url, extract_article_text are defined.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Get links from Step 1\n",
    "    df = google_news_rss_min_df_en(\"Harry Potter\", limit=2)\n",
    "    print(df[[\"title\", \"link\"]])\n",
    "\n",
    "    if not df.empty:\n",
    "        # Pick the first link (use index 1 if you want the second)\n",
    "        test_url = df.iloc[0][\"link\"]\n",
    "\n",
    "        # 2) Resolve to the real article URL (not the Google News redirect)\n",
    "        final_url = resolve_final_url(test_url)\n",
    "\n",
    "        print(\"\\n=== Step 2: Article Extraction Test ===\")\n",
    "        print(\"Original:\", test_url)\n",
    "        print(\"Final   :\", final_url)\n",
    "\n",
    "        # 3) Try extraction (final URL first, then fallback to original)\n",
    "        body = extract_article_text(final_url) or extract_article_text(test_url)\n",
    "\n",
    "        if body:\n",
    "            print(\"Length:\", len(body))\n",
    "            print(\"Preview:\", body[:300], \"...\")\n",
    "        else:\n",
    "            print(\"(No content extracted)\")\n",
    "    else:\n",
    "        print(\"(No rows from Step 1)\")\n",
    "# --- Helpers ---\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0\"}  # Simple UA to avoid basic blocks\n",
    "\n",
    "def dedup_by_title(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Deduplicate rows by title (casefolded) and then by link.\n",
    "    Returns a DataFrame with stable ordering and reset index.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"__title_key\"] = df[\"title\"].fillna(\"\").str.casefold().str.strip()\n",
    "    df = df.drop_duplicates(subset=\"__title_key\", keep=\"first\")\n",
    "    df = df.drop_duplicates(subset=\"link\", keep=\"first\")\n",
    "    return df.drop(columns=\"__title_key\").reset_index(drop=True)\n",
    "\n",
    "def resolve_gnews_link(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Resolve Google News RSS redirect links by extracting the '?url=' parameter.\n",
    "    If not present, return the original URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = urlparse(url)\n",
    "        if \"news.google.com\" in p.netloc:\n",
    "            q = parse_qs(p.query)\n",
    "            if \"url\" in q and q[\"url\"]:\n",
    "                return unquote(q[\"url\"][0])\n",
    "        return url\n",
    "    except Exception:\n",
    "        return url\n",
    "\n",
    "def resolve_final_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the final canonical article URL:\n",
    "    1) Strip Google News '?url=' parameter if present (resolve_gnews_link).\n",
    "    2) Follow HTTP redirects to the ultimate destination.\n",
    "    \"\"\"\n",
    "    u = resolve_gnews_link(url)\n",
    "    try:\n",
    "        r = requests.get(u, headers=UA, timeout=12, allow_redirects=True)\n",
    "        return r.url or u\n",
    "    except Exception:\n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e4a2cbd-e5fc-4889-8164-60c32cdb50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers ---\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests                             # ✅ 추가\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0\"}          # ✅ 추가: 간단 UA\n",
    "\n",
    "def dedup_by_title(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"제목과 링크 기준으로 중복 제거(초간단).\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"__title_key\"] = df[\"title\"].fillna(\"\").str.casefold().str.strip()\n",
    "    df = df.drop_duplicates(subset=\"__title_key\", keep=\"first\")\n",
    "    df = df.drop_duplicates(subset=\"link\", keep=\"first\")\n",
    "    return df.drop(columns=\"__title_key\").reset_index(drop=True)\n",
    "\n",
    "def resolve_gnews_link(url: str) -> str:\n",
    "    \"\"\"Google News RSS 리디렉트 링크를 원본 기사 URL로 변환 (?url= 파라미터 기반).\"\"\"\n",
    "    try:\n",
    "        p = urlparse(url)\n",
    "        if \"news.google.com\" in p.netloc:\n",
    "            q = parse_qs(p.query)\n",
    "            if \"url\" in q and q[\"url\"]:\n",
    "                return unquote(q[\"url\"][0])\n",
    "        return url\n",
    "    except Exception:\n",
    "        return url\n",
    "\n",
    "def resolve_final_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    최종 원본 URL을 얻기 위해:\n",
    "    1) 먼저 resolve_gnews_link로 ?url= 파라미터 제거\n",
    "    2) 그래도 리디렉트가 남아있으면 HTTP 리디렉트 추적\n",
    "    \"\"\"\n",
    "    u = resolve_gnews_link(url)\n",
    "    try:\n",
    "        r = requests.get(u, headers=UA, timeout=12, allow_redirects=True)\n",
    "        return r.url or u\n",
    "    except Exception:\n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad6f8341-6901-42ef-96e5-fd39d98544e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== dedup_by_title ===\n",
      "               title                  link\n",
      "0      AI News Today  http://example.com/1\n",
      "1  Another AI Update  http://example.com/2\n",
      "\n",
      "=== resolve_gnews_link ===\n",
      "Before: https://news.google.com/articles/xxx?url=https%3A%2F%2Foriginalsite.com%2Farticle\n",
      "After : https://originalsite.com/article\n",
      "\n",
      "=== resolve_final_url ===\n",
      "Before: https://news.google.com/articles/xxx?url=https%3A%2F%2Foriginalsite.com%2Farticle\n",
      "After : https://originalsite.com/article\n"
     ]
    }
   ],
   "source": [
    "# --- Helpers 확인용 간단 테스트 ---\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    # 샘플 DataFrame\n",
    "    sample = pd.DataFrame([\n",
    "        {\"title\": \"AI News Today\", \"link\": \"http://example.com/1\"},\n",
    "        {\"title\": \"AI News Today\", \"link\": \"http://example.com/1\"},  # 중복\n",
    "        {\"title\": \"Another AI Update\", \"link\": \"http://example.com/2\"},\n",
    "    ])\n",
    "\n",
    "    print(\"=== dedup_by_title ===\")\n",
    "    print(dedup_by_title(sample))\n",
    "\n",
    "    print(\"\\n=== resolve_gnews_link ===\")\n",
    "    gnews_url = \"https://news.google.com/articles/xxx?url=https%3A%2F%2Foriginalsite.com%2Farticle\"\n",
    "    print(\"Before:\", gnews_url)\n",
    "    print(\"After :\", resolve_gnews_link(gnews_url))\n",
    "\n",
    "    print(\"\\n=== resolve_final_url ===\")\n",
    "    print(\"Before:\", gnews_url)\n",
    "    print(\"After :\", resolve_final_url(gnews_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71018553-1e5a-4fad-9063-f2ff77fde0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] len=   0  title=See photos of new-look Weasley siblings ...  final=Y\n",
      "[1] len=   0  title=Watch: Filming of new Harry Potter serie...  final=Y\n",
      "[2] len=   0  title='Harry Potter' TV Series First Look: Dom...  final=Y\n",
      "\n",
      "=== Final DataFrame (title, link) ===\n",
      "Empty DataFrame\n",
      "Columns: [title, link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Pipeline ---\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def build_news_df_simple(query: str, limit: int = 2, min_chars: int = 300) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Google News RSS → fulltext → DataFrame(title, link, content)\n",
    "    - Dedup title/link\n",
    "    - Resolve Google News redirects to the final URL\n",
    "    - Filter by minimum content length (min_chars)\n",
    "    \"\"\"\n",
    "    # 1) Step 1: fetch metadata\n",
    "    meta = google_news_rss_min_df_en(query, limit=limit)\n",
    "    # 2) dedup\n",
    "    meta = dedup_by_title(meta)\n",
    "\n",
    "    rows = []\n",
    "    # 3) extract article body\n",
    "    for i, r in meta.iterrows():\n",
    "        raw_link = r[\"link\"]\n",
    "        final_url = resolve_final_url(raw_link)  # ✅ use final URL (better than resolve_gnews_link)\n",
    "\n",
    "        # Try final URL first → fallback to raw RSS link\n",
    "        body = extract_article_text(final_url) or extract_article_text(raw_link)\n",
    "\n",
    "        print(f\"[{i}] len={len(body):4d}  title={r['title'][:40]}...  final={'Y' if final_url!=raw_link else 'N'}\")\n",
    "\n",
    "        # 4) length filter\n",
    "        if body and len(body) >= min_chars:\n",
    "            rows.append({\n",
    "                \"title\": r[\"title\"].strip(),\n",
    "                \"link\": final_url,  # ✅ store the real article URL\n",
    "                \"content\": body\n",
    "            })\n",
    "\n",
    "        time.sleep(0.2)  # polite delay\n",
    "\n",
    "    # 5) return final DataFrame\n",
    "    return pd.DataFrame(rows, columns=[\"title\", \"link\", \"content\"])\n",
    "\n",
    "\n",
    "# --- Run: Pipeline Test ---\n",
    "if __name__ == \"__main__\":\n",
    "    df = build_news_df_simple(\"Harry Potter\", limit=3, min_chars=300)\n",
    "    print(\"\\n=== Final DataFrame (title, link) ===\")\n",
    "    print(df[[\"title\", \"link\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bf7213a-c750-444a-9d17-9e8ce700a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Summarize with LLM  ---\n",
    "def summarize_with_llm(text: str) -> str:\n",
    "    \"\"\"Summarize the news article body into 3 concise bullet points using ChatOllama.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    prompt = (\n",
    "        \"Summarize the following news article in 3 concise bullet points.\\n\"\n",
    "        \"Focus on facts (who/what/when/where), key numbers, and outcomes.\\n\\n\"\n",
    "        f\"{text}\"\n",
    "    )\n",
    "    resp = llm.invoke(prompt)  # ⚠️ llm = ChatOllama(...) must be initialized before\n",
    "    return getattr(resp, \"content\", str(resp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2eac47c4-6a2e-44c9-9c6f-c73049fa5f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] len=   0  title=HBO’s Harry Potter Series Casts More Wea...  final=Y\n",
      "[1] len=   0  title=Watch: Filming of new Harry Potter serie...  final=Y\n",
      "\n",
      "=== Step 3 DataFrame ===\n",
      "Empty DataFrame\n",
      "Columns: [title, link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = build_news_df_simple(\"Harry Potter\", limit=2, min_chars=300)\n",
    "    print(\"\\n=== Step 3 DataFrame ===\")\n",
    "    print(df[[\"title\", \"link\"]])\n",
    "\n",
    "    if not df.empty:\n",
    "        first_article = df.iloc[0][\"content\"]\n",
    "        print(\"\\n=== Step 4: LLM Summary ===\")\n",
    "        summary = summarize_with_llm(first_article)\n",
    "        print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9600e95-3497-4391-abd5-644b436eabec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
