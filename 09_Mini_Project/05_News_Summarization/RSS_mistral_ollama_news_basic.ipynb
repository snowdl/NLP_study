{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2112ec20-0074-42b7-8c21-5ce0cc8d4924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChatOllama ready\n",
      " LangChain is an open-source project that aims to create a decentralized, blockchain-based platform for building and deploying artificial intelligence models. The goal of LangChain is to make it easier for developers to build, share, and monetize AI applications by providing a secure, transparent, and scalable infrastructure.\n",
      "\n",
      "The LangChain platform uses smart contracts on the Ethereum blockchain to manage the lifecycle of AI models, including training, deployment, and execution. This allows for decentralized storage and computation of AI models, reducing the reliance on centralized cloud providers and enabling more efficient use of resources.\n",
      "\n",
      "LangChain also includes a marketplace where developers can buy and sell AI models, as well as tools for creating and managing AI applications. The project is still in development, but it has the potential to revolutionize the way AI is developed and deployed by providing a decentralized, open-source alternative to traditional centralized platforms.\n"
     ]
    }
   ],
   "source": [
    "# --- LLM Configuration (LangChain + Ollama) ---\n",
    "LLM_NAME = \"mistral\"\n",
    "TEMPERATURE = 0.1\n",
    "TIMEOUT = 60.0  # seconds\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 1) llm \n",
    "llm = ChatOllama(\n",
    "    model=LLM_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "print(\"✅ ChatOllama ready\")\n",
    "\n",
    "# 2) test\n",
    "response = llm.invoke(\"Hello! Can you summarize what LangChain is?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a213f62-65a5-4dc2-b05b-1db364781cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from urllib.parse import quote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87498b49-c93b-4c05-83a4-3bc858e42f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: RSS ---\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from urllib.parse import quote\n",
    "\n",
    "def google_news_rss_min_df_en(query: str = \"Harry Potter\", limit: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch basic metadata from Google News RSS feed.\n",
    "    Args:\n",
    "        query (str): Search keyword (default = \"Harry Potter\")\n",
    "        limit (int): Number of articles to fetch\n",
    "    Returns:\n",
    "        DataFrame with columns [title, link, published, source]\n",
    "    \"\"\"\n",
    "    # Build Google News RSS URL (English / US region)\n",
    "    url = f\"https://news.google.com/rss/search?q={quote(query)}&hl=en&gl=US&ceid=US:en\"\n",
    "\n",
    "    # Parse the RSS feed\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    rows = []\n",
    "    for e in feed.entries[:limit]:\n",
    "        rows.append({\n",
    "            # Get article title (default = \"\")\n",
    "            \"title\": (getattr(e, \"title\", \"\") or \"\").strip(),\n",
    "            # Get article link\n",
    "            \"link\": getattr(e, \"link\", \"\") or \"\",\n",
    "            # Get publish date\n",
    "            \"published\": getattr(e, \"published\", \"\") or \"\",\n",
    "            # Get source (e.g., BBC, USA Today)\n",
    "            \"source\": getattr(getattr(e, \"source\", None), \"title\", \"\") or \"\"\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(rows, columns=[\"title\", \"link\", \"published\", \"source\"])\n",
    "\n",
    "    # Remove duplicate rows (first by title, then by link)\n",
    "    df = df.drop_duplicates(subset=\"title\").drop_duplicates(subset=\"link\").reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83fe5c8-7767-48a1-80cc-af20ebdcc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers ---\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def dedup_by_title(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    df = df.copy()\n",
    "    df[\"__k\"] = df[\"title\"].fillna(\"\").str.casefold().str.strip()\n",
    "    df = df.drop_duplicates(subset=\"__k\").drop_duplicates(subset=\"link\").drop(columns=\"__k\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def resolve_final_url(url: str) -> str:\n",
    "    # 1) ?url= 있으면 그 값 사용\n",
    "    try:\n",
    "        p = urlparse(url)\n",
    "        if \"news.google.com\" in p.netloc:\n",
    "            q = parse_qs(p.query)\n",
    "            if \"url\" in q and q[\"url\"]:\n",
    "                url = unquote(q[\"url\"][0])\n",
    "    except:\n",
    "        pass\n",
    "    # 2) HTTP 리디렉트 따라가기\n",
    "    try:\n",
    "        r = requests.get(url, headers=UA, timeout=12, allow_redirects=True)\n",
    "        return r.url or url\n",
    "    except:\n",
    "        return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4bbef02-760e-4897-aeed-79530df42897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Extract ---\n",
    "import trafilatura\n",
    "import requests   # \n",
    "make sure requests is imported\n",
    "                  # (used for fetching the webpage content)\n",
    "\n",
    "def extract_article_text(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the main text from a news article URL.\n",
    "    Args:\n",
    "        url (str): Article web link\n",
    "    Returns:\n",
    "        str: Clean article text, or empty string if failed\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Download the webpage (follow redirects)\n",
    "        r = requests.get(url, headers=UA, timeout=15, allow_redirects=True)\n",
    "\n",
    "        # If failed (error code or no content), return empty\n",
    "        if r.status_code >= 400 or not r.text:\n",
    "            return \"\"\n",
    "\n",
    "        # Extract article text (remove comments, tables, metadata)\n",
    "        txt = trafilatura.extract(\n",
    "            r.text,\n",
    "            include_comments=False,\n",
    "            include_tables=False,\n",
    "            favor_recall=True,      # try to capture as much as possible\n",
    "            with_metadata=False     # skip extra metadata\n",
    "        )\n",
    "\n",
    "        # Return stripped text (or empty if None)\n",
    "        return (txt or \"\").strip()\n",
    "    except:\n",
    "        # In case of error (e.g. timeout, parse error), return empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd24f3d1-ca9a-4cdc-a3d0-fb4f72e8fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Pipeline ---\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def build_news_df_simple(query: str = \"Harry Potter\", limit: int = 2, min_chars: int = 300) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    1) Fetch news metadata from Google News RSS\n",
    "    2) Remove duplicate titles/links\n",
    "    3) Extract article body text\n",
    "    4) Filter out short articles (by min_chars)\n",
    "    5) Return as DataFrame\n",
    "    \"\"\"\n",
    "    # Step 1: Fetch metadata (title, link, source, etc.)\n",
    "    meta = google_news_rss_min_df_en(query, limit=limit)\n",
    "\n",
    "    # Step 2: Remove duplicate titles/links\n",
    "    meta = dedup_by_title(meta)\n",
    "\n",
    "    rows = []\n",
    "    for i, r in meta.iterrows():\n",
    "        # Step 3: Resolve redirect to get final article URL\n",
    "        final_url = resolve_final_url(r[\"link\"])\n",
    "\n",
    "        # Step 4: Try extracting text (first from final_url, fallback to raw RSS link)\n",
    "        body = extract_article_text(final_url) or extract_article_text(r[\"link\"])\n",
    "\n",
    "        # Debug print: show text length and part of the title\n",
    "        print(f\"[{i}] len={len(body):4d}  {r['title'][:40]}...\")\n",
    "\n",
    "        # Step 5: Only keep if article is long enough\n",
    "        if body and len(body) >= min_chars:\n",
    "            rows.append({\n",
    "                \"title\": r[\"title\"].strip(),\n",
    "                \"link\": final_url,\n",
    "                \"content\": body\n",
    "            })\n",
    "\n",
    "        # Small delay to avoid overloading servers\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # Step 6: Return DataFrame with clean results\n",
    "    return pd.DataFrame(rows, columns=[\"title\", \"link\", \"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e688de75-11ee-4ccb-99cc-75950bff2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Summarize ---\n",
    "def summarize_with_llm(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the article text into 3 short bullet points using LLM.\n",
    "    - If text is empty, return \"\"\n",
    "    - Otherwise, send prompt to llm.invoke()\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Simple prompt asking for 3 concise bullets\n",
    "    return llm.invoke(\"Summarize in 3 short bullets:\\n\\n\" + text).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5855f6-beb4-4306-92ff-f96c8bd58a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] len=   0  ‘Harry Potter’ HBO Series Reveals Weasle...\n",
      "[1] len=   0  Watch: Filming of new Harry Potter serie...\n",
      "\n",
      "=== Titles ===\n",
      "Empty DataFrame\n",
      "Columns: [title, link]\n",
      "Index: []\n",
      "\n",
      "(No rows passed the min_chars filter)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 0) LLM 준비 (Ollama 실행 중이어야 함: ollama serve / ollama pull mistral)\n",
    "    from langchain_ollama import ChatOllama\n",
    "    llm = ChatOllama(model=\"mistral\", temperature=0.1, timeout=60.0)\n",
    "\n",
    "    # 1) 뉴스 수집 + 본문 추출\n",
    "    df = build_news_df_simple(\"Harry Potter\", limit=2, min_chars=300)\n",
    "    print(\"\\n=== Titles ===\")\n",
    "    print(df[[\"title\", \"link\"]])\n",
    "\n",
    "    # 2) 요약\n",
    "    if not df.empty:\n",
    "        print(\"\\n=== Summaries ===\")\n",
    "        for i, row in df.iterrows():\n",
    "            print(f\"\\n[{i}] {row['title']}\")\n",
    "            print(summarize_with_llm(row[\"content\"]))\n",
    "    else:\n",
    "        print(\"\\n(No rows passed the min_chars filter)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e181be9-1bda-4c31-8358-f9f6f792b71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
