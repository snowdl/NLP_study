{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90caf9c7-2c68-451b-bef9-c701c3e313ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "import random, torch\n",
    "DEVICE = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"DEVICE =\", DEVICE)\n",
    "\n",
    "def set_seed(seed=7):\n",
    "    random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b86e6251-9cc1-4d40-8a09-3e3798e7bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1 : Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76c09c3-c8ec-4040-9e6e-d3507dc8382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "MODEL_ID = \"distilgpt2\"\n",
    "\n",
    "# Load tokenizer for the given model\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# If the tokenizer does not define an EOS (end-of-sequence) token,\n",
    "# assign an empty string as a placeholder.\n",
    "if tok.eos_token_id is None:\n",
    "    tok.eos_token = \"\"\n",
    "\n",
    "# If the tokenizer does not define a PAD (padding) token,\n",
    "# reuse the EOS token as PAD. (Common practice for GPT-like models.)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Load the causal language model and move it to the chosen device (CPU, CUDA, or MPS).\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "# Enable caching of past key/values to speed up generation.\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Save the EOS token ID for later checks during generation.\n",
    "EOS_ID = tok.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07ba24e6-2d3f-44b4-89bc-42a10f0769c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP2] utils ready\n"
     ]
    }
   ],
   "source": [
    "# STEP2 — Utilities: encode / decode / greedy_next\n",
    "\n",
    "def encode(text: str):\n",
    "    \"\"\"\n",
    "    Convert text into token IDs tensor.\n",
    "    - Returns: shape [1, T] on the chosen DEVICE.\n",
    "    \"\"\"\n",
    "    return tok(text, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"\n",
    "    Convert token IDs back into text.\n",
    "    - Skip special tokens like <pad> or <eos>.\n",
    "    \"\"\"\n",
    "    return tok.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def greedy_next(ids):\n",
    "    \"\"\"\n",
    "    Select the next token using greedy decoding.\n",
    "    - Run the model on the current sequence.\n",
    "    - Take logits (scores) from the last position.\n",
    "    - Return the token ID with the highest score (argmax).\n",
    "    \"\"\"\n",
    "    logits = model(ids).logits[0, -1, :]      # last position logits\n",
    "    return int(torch.argmax(logits).item())   # ID of best token\n",
    "\n",
    "print(\"[STEP2] utils ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba75597-3134-4d43-bd28-1f64df8d7b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Greedy ===\n",
      "Artificial intelligence is changing the way humans , and it’s changing the way we interact with other people.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Artificial intelligence is changing the way humans ,\"\n",
    "ids = encode(prompt)\n",
    "start = ids.shape[1]\n",
    "\n",
    "for _ in range(30):\n",
    "    nxt = greedy_next(ids)\n",
    "    if EOS_ID is not None and nxt == EOS_ID:\n",
    "        break\n",
    "    ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "print(\"=== Greedy ===\")\n",
    "print(decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb69b8b2-05bc-44ba-a616-6ecca78e5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling(only temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d021dbef-ceaf-4bc4-a0ec-22e9e7106a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def softmax_temp(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Apply temperature scaling to logits and convert to probabilities.\n",
    "    - logits: raw scores for each vocabulary token (tensor [V]).\n",
    "    - temperature:\n",
    "        < 1.0 → sharper distribution (more deterministic).\n",
    "        > 1.0 → flatter distribution (more random).\n",
    "    - A very small floor (1e-6) is applied to avoid division by zero.\n",
    "    - Returns: probability vector (tensor [V]) that sums to 1.0.\n",
    "    \"\"\"\n",
    "    t = max(float(temperature), 1e-6)\n",
    "    return torch.softmax(logits / t, dim=-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def sample_next_temp_only(ids, temperature=0.9):\n",
    "    \"\"\"\n",
    "    Sample the next token using temperature-scaled softmax only\n",
    "    (no top-p or repetition penalties here).\n",
    "\n",
    "    Steps:\n",
    "      1. Run the model on the current sequence `ids`.\n",
    "      2. Take the logits from the last position (next-token scores).\n",
    "      3. Convert logits → probabilities using `softmax_temp`.\n",
    "      4. Draw one token index at random according to these probabilities\n",
    "         using `torch.multinomial`.\n",
    "      5. Return the chosen token ID (int).\n",
    "    \"\"\"\n",
    "    logits = model(ids).logits[0, -1, :]        # last-step logits\n",
    "    probs  = softmax_temp(logits, temperature)  # apply temperature\n",
    "    pick   = torch.multinomial(probs, 1)[0].item()  # sample 1 token ID\n",
    "    return int(pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c384dac7-eb8f-401d-8c89-301089b3e695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature actually used: 0.9\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.9\n",
    "t = max(float(temperature), 1e-6)\n",
    "print(\"Temperature actually used:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "023c6ab1-c2c4-4904-a934-4323c360683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aea26f2-2e10-4e3c-ba32-1ef06262966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample (temp only) ===\n",
      "Artificial intelligence is changing the way humans , and understanding human biology is changing the way humans are prepared for its challenges, a presentation published this week in the journal Current Biology Proceedings of the National Academy\n"
     ]
    }
   ],
   "source": [
    "ids2 = encode(prompt)\n",
    "for _ in range(30):\n",
    "    nxt = sample_next_temp_only(ids2, temperature=0.9)\n",
    "    if EOS_ID is not None and nxt == EOS_ID: break\n",
    "    ids2 = torch.cat([ids2, torch.tensor([[nxt]], device=ids2.device)], dim=1)\n",
    "\n",
    "print(\"=== Sample (temp only) ===\")\n",
    "print(decode(ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e91741f-c1dd-42da-8647-6b15a72c63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 5 — nucleus(top-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b5f40c6-4e7a-4beb-ab1e-2f1ede0ce0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def top_p_indices(probs, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Nucleus (top-p) filtering:\n",
    "    - Select smallest set of tokens whose cumulative prob ≤ top_p.\n",
    "    - Always keep the top-1 token.\n",
    "    \"\"\"\n",
    "    if top_p is None or top_p >= 1:\n",
    "        return torch.arange(probs.numel(), device=probs.device)\n",
    "\n",
    "    sp, sx = torch.sort(probs, descending=True)  # sorted probs & indices\n",
    "    csum = torch.cumsum(sp, dim=0)               # cumulative sum\n",
    "    keep = csum <= top_p\n",
    "    keep[0] = True\n",
    "    return sx[keep]                              # candidate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "233b60c6-bb77-4d86-9c42-eabc066fceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample_next(ids, temperature=0.9, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Sample the next token:\n",
    "    - Apply temperature scaling.\n",
    "    - Apply nucleus (top-p) filtering.\n",
    "    - Draw 1 token at random from the filtered set.\n",
    "    \"\"\"\n",
    "    logits = model(ids).logits[0, -1, :]      # last-step logits\n",
    "    probs  = softmax_temp(logits, temperature)\n",
    "\n",
    "    pool_ix = top_p_indices(probs, top_p)     # step 1: filter candidates\n",
    "    pool = probs[pool_ix]                     # step 2: restrict probs\n",
    "    pool = pool / pool.sum()                  # step 3: normalize\n",
    "    pick_local = torch.multinomial(pool, 1)[0].item()  # step 4: sample one\n",
    "    return int(pool_ix[pick_local].item())    # step 5: map back to vocab ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea781965-3857-4922-a4ed-c0c4cb97422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([50257])\n",
      "Sum of probs (should be ~1.0): 1.0\n",
      "Number of candidate tokens: 66\n",
      "Picked token ID: 933\n",
      "Picked token str: vern\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 문장\n",
    "prompt = \"Artificial intelligence is transforming the world because \"\n",
    "ids = encode(prompt)\n",
    "\n",
    "# 1. 모델이 준 마지막 위치 로짓\n",
    "logits = model(ids).logits[0, -1, :]\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# 2. softmax + temperature 적용\n",
    "probs = softmax_temp(logits, temperature=0.9)\n",
    "print(\"Sum of probs (should be ~1.0):\", probs.sum().item())\n",
    "\n",
    "# 3. top-p 필터링으로 후보 뽑기\n",
    "candidates = top_p_indices(probs, top_p=0.95)\n",
    "print(\"Number of candidate tokens:\", len(candidates))\n",
    "\n",
    "# 4. 실제로 하나 샘플링\n",
    "picked_id = sample_next(ids, temperature=0.9, top_p=0.95)\n",
    "print(\"Picked token ID:\", picked_id)\n",
    "print(\"Picked token str:\", tok.decode([picked_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25721bf7-a8fc-4e28-8a65-2597a9c4f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31e552e1-24e2-4fce-9174-440e419b0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    " @torch.inference_mode()\n",
    "def propose_branch(ids, span=3, temperature=0.9, top_p=0.95):\n",
    "    cur = ids.clone()\n",
    "    out = []\n",
    "    for _ in range(span):\n",
    "        t = sample_next(cur, temperature, top_p)\n",
    "        out.append(t)\n",
    "        cur = torch.cat([cur, torch.tensor([[t]], device=cur.device)], dim=1)\n",
    "    return out  # list[int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00d76d21-fe64-48fd-bfe2-45979f4dd35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed token IDs: [1133, 511, 9017, 11, 355]\n",
      "Decoded tokens: ['ute', ' their', ' minds', ',', ' as']\n",
      "Joined as text: ute their minds, as\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Artificial intelligence is changing the way humans \"\n",
    "ids = encode(prompt)\n",
    "\n",
    "branch = propose_branch(ids, span=5, temperature=0.9, top_p=0.95)\n",
    "\n",
    "print(\"Proposed token IDs:\", branch)                # 숫자 리스트\n",
    "print(\"Decoded tokens:\", [tok.decode([t]) for t in branch])  # 각각 글자로\n",
    "print(\"Joined as text:\", tok.decode(branch))        # 한 번에 이어붙인 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea27c80a-2d8a-4d85-b07b-874a873ddb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 7 — prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "749bd9a4-8495-427b-89cd-b6b9a03d32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def prefix_accept_once(ids, branch):\n",
    "    \"\"\"\n",
    "    Compare a drafted branch with greedy decoding, one token at a time.\n",
    "    \n",
    "    Process:\n",
    "      1. Start with the current sequence `ids`.\n",
    "      2. For each token `t` in the proposed branch:\n",
    "         - Compute greedy_next(cur): the model’s best next token.\n",
    "         - If greedy == proposed token:\n",
    "             → Accept it, append `t` to the sequence, and continue.\n",
    "         - If greedy != proposed token:\n",
    "             → Reject the branch at this point, \n",
    "               append the greedy token instead, and stop checking further.\n",
    "      3. Return:\n",
    "         - The new sequence with accepted tokens (and possibly one greedy token).\n",
    "         - The count of how many proposed tokens were accepted before the mismatch.\n",
    "    \n",
    "    Args:\n",
    "        ids (torch.Tensor): Current token IDs [1, T].\n",
    "        branch (list[int]): Drafted token IDs.\n",
    "    \n",
    "    Returns:\n",
    "        (torch.Tensor, int): (updated sequence, number of accepted tokens)\n",
    "    \"\"\"\n",
    "    cur = ids.clone()\n",
    "    accepted = 0\n",
    "    for t in branch:\n",
    "        # 1) Greedy prediction for the next token\n",
    "        g = greedy_next(cur)\n",
    "        if g == t:\n",
    "            # 2) If they match → accept the proposed token\n",
    "            cur = torch.cat([cur, torch.tensor([[t]], device=cur.device)], dim=1)\n",
    "            accepted += 1\n",
    "        else:\n",
    "            # 3) If mismatch → append greedy token and stop\n",
    "            cur = torch.cat([cur, torch.tensor([[g]], device=cur.device)], dim=1)\n",
    "            break\n",
    "    return cur, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca4c48b5-a128-4f55-b28f-8ab57e811aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 8 — Medusa-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "987ecb17-7203-485b-9825-a24ce8abc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_tiny(prompt, max_new_tokens=30, span=3, temperature=0.9, top_p=0.95):\n",
    "    ids = encode(prompt)\n",
    "    start = ids.shape[1]\n",
    "    steps = 0\n",
    "    max_steps = max_new_tokens * 3  \n",
    "    while ids.shape[1] - start < max_new_tokens and steps < max_steps:\n",
    "        branch = propose_branch(ids, span=span, temperature=temperature, top_p=top_p)\n",
    "        ids, _ = prefix_accept_once(ids, branch)\n",
    "        steps += 1\n",
    "    return decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d7985a6-ad8c-4a21-9bab-2aa3d58acd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Medusa-tiny ===\n",
      "In a distant future,   the world is a place where the world is a place where the world is a place where the world is a place where the world is a place\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Medusa-tiny ===\")\n",
    "print(medusa_tiny(\"In a distant future, \", max_new_tokens=30, span=3, temperature=0.9, top_p=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c70db-56dd-420d-82b0-54cdb073e850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
