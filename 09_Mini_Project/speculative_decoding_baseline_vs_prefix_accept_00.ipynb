{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57108f4-0d44-4b7f-a3db-91d8f99ba0cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '→' (U+2192) (2070700283.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    uni, bi, tri 빈도 테이블을 만들어서 trigram → bigram → unigram backoff 모델 구성.\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '→' (U+2192)\n"
     ]
    }
   ],
   "source": [
    "코드가 하는 일 요약\n",
    "\n",
    "Setup\n",
    "\n",
    "uni, bi, tri 빈도 테이블을 만들어서 trigram → bigram → unigram backoff 모델 구성.\n",
    "\n",
    "_sample_from_counts 는 temperature sampling, _argmax_from_counts 는 argmax (deterministic pick).\n",
    "\n",
    "Baseline\n",
    "\n",
    "generate_baseline: backoff 모델에서 한 토큰씩 temperature sampling (T=0.7).\n",
    "\n",
    "Speculative Step\n",
    "\n",
    "Drafter: 작은 모델 (bigram→unigram, T_draft=0.9).\n",
    "\n",
    "Verifier: 큰 모델 (tri→bi→uni, argmax).\n",
    "\n",
    "Prefix-accept 규칙: drafter 제안과 verifier 예측이 일치하면 계속, 처음 불일치 시 교체 후 STOP.\n",
    "\n",
    "실험\n",
    "\n",
    "Prompt: \"the wolf ran\".\n",
    "\n",
    "Baseline 결과와 speculative 결과 비교 출력.\n",
    "\n",
    "100회 시뮬레이션에서 평균 몇 토큰이 prefix로 accept되는지 통계까지 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d80c745-9e32-471f-ac69-9a8a9110a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#준비: 토큰과 n-gram 표 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fc2973-06fb-4da6-9031-6a99d9dd6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "random.seed(42)  # 결과 재현용(초보 친화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19020be8-d20e-4d84-9164-7ffb913b82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Corpus -> tokens\n",
    "corpus = \"the wolf ran into the forest\"\n",
    "tokens = corpus.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bd4c294-13e5-4d7c-bb66-1d9a9dfb1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Unigram / Bigram / Trigram tables\n",
    "uni = Counter(tokens)\n",
    "bi  = defaultdict(Counter)\n",
    "tri = defaultdict(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a054f2-dea8-4b60-b895-000e00a139eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(tokens, tokens[1:]):\n",
    "    bi[a][b] += 1\n",
    "\n",
    "for a, b, c in zip(tokens, tokens[1:], tokens[2:]):\n",
    "    tri[(a, b)][c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89a91de4-8edc-4356-aaf5-d65c149a7569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unigrams ===\n",
      "'the': 2\n",
      "'wolf': 1\n",
      "'ran': 1\n",
      "'into': 1\n",
      "'forest': 1\n"
     ]
    }
   ],
   "source": [
    "# 1) Unigram: count how many times each word appears\n",
    "print(\"=== Unigrams ===\")\n",
    "for word, count in uni.items():\n",
    "    print(f\"{word!r}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bf4aaea-3ebe-4a14-9985-93e94a420b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bigrams ===\n",
      "('the' -> 'wolf'): 1\n",
      "('the' -> 'forest'): 1\n",
      "('wolf' -> 'ran'): 1\n",
      "('ran' -> 'into'): 1\n",
      "('into' -> 'the'): 1\n"
     ]
    }
   ],
   "source": [
    "# 2) Bigram: count how many times a word is followed by another word\n",
    "print(\"\\n=== Bigrams ===\")\n",
    "for prev, counter in bi.items():\n",
    "    for nxt, count in counter.items():\n",
    "        print(f\"({prev!r} -> {nxt!r}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bff0de4-ca2b-4bc3-a1ac-c72575f61f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trigrams ===\n",
      "(('the', 'wolf') -> 'ran'): 1\n",
      "(('wolf', 'ran') -> 'into'): 1\n",
      "(('ran', 'into') -> 'the'): 1\n",
      "(('into', 'the') -> 'forest'): 1\n"
     ]
    }
   ],
   "source": [
    "# 3) Trigram: count how many times two words are followed by a third word\n",
    "print(\"\\n=== Trigrams ===\")\n",
    "for (w1, w2), counter in tri.items():\n",
    "    for nxt, count in counter.items():\n",
    "        print(f\"(({w1!r}, {w2!r}) -> {nxt!r}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27646a79-b02b-4718-a68e-51eb6dad82a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBackoff : \\nfalling back to a simpler model if higher-order statistics are missing.\\nTry trigram first (P(next | prev2, prev1)).\\nIf no data → back off to bigram (P(next | prev1)).\\nIf still no data → back off to unigram (P(next)).\\n*In short: “If the detailed context isn’t available, step back to a simpler context.”\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Backoff : \n",
    "falling back to a simpler model if higher-order statistics are missing.\n",
    "Try trigram first (P(next | prev2, prev1)).\n",
    "If no data → back off to bigram (P(next | prev1)).\n",
    "If still no data → back off to unigram (P(next)).\n",
    "*In short: “If the detailed context isn’t available, step back to a simpler context.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61446c1c-18b6-4aff-938c-13012a5cf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) backoff 분포 꺼내기 + 샘플/최빈 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4784987c-72da-47d9-bca1-d4bf3c0dbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(prev2, prev1):   #backoff flow (tri → bi → uni).\n",
    "    \"\"\"\n",
    "    Return the distribution of possible next words\n",
    "    using a backoff strategy:\n",
    "      1) Try trigram (prev2, prev1 → next).\n",
    "      2) If not available, fall back to bigram (prev1 → next).\n",
    "      3) If not available, fall back to unigram (overall counts).\n",
    "    \"\"\"\n",
    "    d3 = tri.get((prev2, prev1))\n",
    "    if d3: \n",
    "        return d3\n",
    "    d2 = bi.get(prev1)\n",
    "    if d2: \n",
    "        return d2\n",
    "    return uni\n",
    "\n",
    "def sample_from_counts(dist, T=1.0):   #probabilistic sampling with temperature.\n",
    "    \"\"\"\n",
    "    Sample one token from a frequency distribution (dist).\n",
    "    - Apply temperature scaling: weight = count ** (1/T).\n",
    "    - Higher T (>1.0): more random / diverse.\n",
    "    - Lower T (<1.0): more deterministic / greedy.\n",
    "    \"\"\"\n",
    "    items = list(dist.items())\n",
    "    toks  = [t for t, _ in items]\n",
    "    cnts  = [c for _, c in items]\n",
    "    weights = [c ** (1.0 / T) for c in cnts]  # temperature-adjusted weights\n",
    "    return random.choices(toks, weights=weights, k=1)[0]\n",
    "\n",
    "\n",
    "def argmax_from_counts(dist):  #deterministic “most frequent word” choice.\n",
    "    \"\"\"\n",
    "    Deterministic choice:\n",
    "    Pick the token with the highest frequency (mode of the distribution).\n",
    "    \"\"\"\n",
    "    return max(dist.items(), key=lambda kv: kv[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "481d4cd5-ad21-4c37-8d19-062d98f0df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ('wolf', 'ran')\n",
      "Distribution (counts): {'into': 1}\n",
      "\n",
      "Sampled next (T=1.0): into\n",
      "Sampled next (T=0.5, greedier): into\n",
      "Argmax next: into\n"
     ]
    }
   ],
   "source": [
    "# === Try it out ===\n",
    "prev2, prev1 = \"wolf\", \"ran\"   # context: \"... wolf ran\"\n",
    "dist = get_counts(prev2, prev1)\n",
    "\n",
    "print(\"Context:\", (prev2, prev1))\n",
    "print(\"Distribution (counts):\", dict(dist))\n",
    "\n",
    "print(\"\\nSampled next (T=1.0):\", sample_from_counts(dist, T=1.0))\n",
    "print(\"Sampled next (T=0.5, greedier):\", sample_from_counts(dist, T=0.5))\n",
    "print(\"Argmax next:\", argmax_from_counts(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78069043-b395-4d98-8f24-b05177fd7172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: ('wolf', 'ran')\n",
      "Distribution: {'into': 1}\n",
      "Sampled (T=1.0): into\n",
      "Argmax: into\n",
      "\n",
      "Context: ('ran', 'into')\n",
      "Distribution: {'the': 1}\n",
      "Sampled (T=1.0): the\n",
      "Argmax: the\n",
      "\n",
      "Context: ('the', 'wolf')\n",
      "Distribution: {'ran': 1}\n",
      "Sampled (T=1.0): ran\n",
      "Argmax: ran\n",
      "\n",
      "Context: ('hello', 'wolf')\n",
      "Distribution: {'ran': 1}\n",
      "Sampled (T=1.0): ran\n",
      "Argmax: ran\n",
      "\n",
      "Context: ('hello', 'zzz')\n",
      "Distribution: {'the': 2, 'wolf': 1, 'ran': 1, 'into': 1, 'forest': 1}\n",
      "Sampled (T=1.0): forest\n",
      "Argmax: the\n"
     ]
    }
   ],
   "source": [
    "# --- Test contexts ---\n",
    "contexts = [\n",
    "    (\"wolf\", \"ran\"),   # exact trigram match\n",
    "    (\"ran\", \"into\"),   # trigram -> \"the\"\n",
    "    (\"the\", \"wolf\"),   # trigram -> \"ran\"\n",
    "    (\"hello\", \"wolf\"), # trigram missing -> backoff to bigram\n",
    "    (\"hello\", \"zzz\")   # trigram + bigram missing -> backoff to unigram\n",
    "]\n",
    "\n",
    "for prev2, prev1 in contexts:\n",
    "    dist = get_counts(prev2, prev1)\n",
    "    print(f\"\\nContext: ({prev2!r}, {prev1!r})\")\n",
    "    print(\"Distribution:\", dict(dist))\n",
    "    print(\"Sampled (T=1.0):\", sample_from_counts(dist))\n",
    "    print(\"Argmax:\", argmax_from_counts(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56fecee5-a6e3-4328-9595-ffc8b469dd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n**Function:** `generate_baseline(prompt_tokens, steps=5, T=0.7)`  \\ngenerate_baseline: use a backoff n-gram model (tri→bi→uni) to\\nprobabilistically append one token at a time after the given prompt,\\ncontinuing the sentence. T controls randomness (higher = more diverse).\\n\\n**Flow:**  \\n1) Ensure at least 2 tokens of context  \\n2) Get next-token distribution via backoff  \\n3) Sample with temperature `T` (higher = more random)  \\n4) Append token and slide context window\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### #3) Baseline: one-token-at-a-time sampling\n",
    "\"\"\"\n",
    "**Function:** `generate_baseline(prompt_tokens, steps=5, T=0.7)`  \n",
    "generate_baseline: use a backoff n-gram model (tri→bi→uni) to\n",
    "probabilistically append one token at a time after the given prompt,\n",
    "continuing the sentence. T controls randomness (higher = more diverse).\n",
    "\n",
    "**Flow:**  \n",
    "1) Ensure at least 2 tokens of context  \n",
    "2) Get next-token distribution via backoff  \n",
    "3) Sample with temperature `T` (higher = more random)  \n",
    "4) Append token and slide context window\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f2f2bb5-a0d1-446f-a588-5fe51f53e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baseline(prompt_tokens, steps=5, T=0.7):\n",
    "    \"\"\"\n",
    "    Baseline generator:\n",
    "    Uses a backoff n-gram model (trigram → bigram → unigram) \n",
    "    to generate text one token at a time.\n",
    "\n",
    "    Args:\n",
    "        prompt_tokens (list): starting context tokens\n",
    "        steps (int): how many tokens to generate\n",
    "        T (float): temperature (controls randomness)\n",
    "\n",
    "    Returns:\n",
    "        list: the prompt plus newly generated tokens\n",
    "    \"\"\"\n",
    "    out = list(prompt_tokens)\n",
    "\n",
    "    # Ensure at least 2 tokens for context\n",
    "    # (if the prompt is too short, duplicate the last token)\n",
    "    \"\"\"\n",
    "    “This part is a safeguard: even if the prompt contains only a single word, \n",
    "    it forces a two-token context \n",
    "    by duplicating the last token so that the backoff model can function properly\n",
    "    \"\"\"\n",
    "    if len(out) < 2:\n",
    "        out = [out[-1], out[-1]]\n",
    "\n",
    "    # Initialize context window (previous 2 tokens)\n",
    "    prev2, prev1 = out[-2], out[-1]\n",
    "\n",
    "    # Generate tokens step by step\n",
    "    for _ in range(steps):\n",
    "        dist = get_counts(prev2, prev1)     # backoff distribution\n",
    "        nxt  = sample_from_counts(dist, T)  # probabilistic sample\n",
    "        out.append(nxt)\n",
    "\n",
    "        # Shift context window\n",
    "        prev2, prev1 = prev1, nxt\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01dd5fe4-bb81-4063-a870-eec2fdaf1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Speculative (core): draft → verify (prefix-accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd5ef82c-107d-4587-9c8b-af5564beb427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ensure context (>=2 tokens)\n",
    "def ensure_two_token_context(prompt_tokens):\n",
    "    \"\"\"Safeguard: if prompt has 1 token, duplicate it to make 2.\"\"\"\n",
    "    ctx = list(prompt_tokens)\n",
    "    if len(ctx) < 2:\n",
    "        ctx = [ctx[-1], ctx[-1]]\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51bf0df2-6a2f-4f9a-9729-155de5199e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Small-model distribution (bigram -> unigram)\n",
    "def small_model_dist(prev1):\n",
    "    \"\"\"Return drafter's dist: bigram if available, else unigram.\"\"\"\n",
    "    return bi.get(prev1, uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b0ed038-624f-4f9f-9d4d-21b617d4d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Draft ONE token (small model)\n",
    "def draft_one(prev1, T_draft):\n",
    "    \"\"\"Sample one next token from small-model dist.\"\"\"\n",
    "    return sample_from_counts(small_model_dist(prev1), T_draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fb9f36c-aea1-44aa-9445-dd24112cd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Make draft of k tokens (small model advances its own context)\n",
    "def make_draft(context, k=5, T_draft=0.9):\n",
    "    \"\"\"Return (draft, draft_trace[(prev2, prev1, t), ...]).\"\"\"\n",
    "    prev2, prev1 = context[-2], context[-1]\n",
    "    draft, trace = [], []\n",
    "    for _ in range(k):\n",
    "        t = draft_one(prev1, T_draft)\n",
    "        draft.append(t)\n",
    "        trace.append((prev2, prev1, t))\n",
    "        prev2, prev1 = prev1, t\n",
    "    return draft, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f3b61c9-69cd-45ee-ae72-d5b1ebc77d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Predict next by large model (argmax over backoff dist)\n",
    "def predict_next_argmax(prev2, prev1):\n",
    "    \"\"\"Large model: tri->bi->uni, pick argmax next token.\"\"\"\n",
    "    dist = get_counts(prev2, prev1)\n",
    "    return max(dist.items(), key=lambda kv: kv[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8585e24a-8375-4535-8684-24770b161e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Compare one step (prefix-accept rule on a single token)\n",
    "def compare_step(prev2, prev1, draft_t):\n",
    "    \"\"\"\n",
    "    Return (accepted_token, ok_flag, verify_token).\n",
    "    If match -> accept draft; else -> replace with verify token.\n",
    "    \"\"\"\n",
    "    v = predict_next_argmax(prev2, prev1)\n",
    "    ok = (draft_t == v)\n",
    "    return (draft_t if ok else v), ok, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0f66d6f-9e3a-4cd7-a40e-1d28fbda22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Verify whole draft with prefix-accept\n",
    "def verify_prefix_accept(context, draft):\n",
    "    \"\"\"\n",
    "    Compare left->right; on first mismatch, replace and STOP.\n",
    "    Return (accepted_tokens, verify_log[(p2,p1,t,v,ok), ...]).\n",
    "    \"\"\"\n",
    "    accepted, log = [], []\n",
    "    prev2, prev1 = context[-2], context[-1]\n",
    "    for t in draft:\n",
    "        chosen, ok, v = compare_step(prev2, prev1, t)\n",
    "        log.append((prev2, prev1, t, v, ok))\n",
    "        accepted.append(chosen)\n",
    "        if ok:\n",
    "            prev2, prev1 = prev1, chosen\n",
    "        else:\n",
    "            break\n",
    "    return accepted, log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4dc00e9-ba5b-4040-9c41-5012d9401750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Orchestrator (with optional trace printing)\n",
    "def speculative_step(prompt_tokens, k=5, T_draft=0.9, trace=True):\n",
    "    \"\"\"\n",
    "    #4) Speculative (core): draft -> verify (prefix-accept)\n",
    "      - Drafter: bigram (else unigram) + higher T\n",
    "      - Verifier: tri->bi->uni + argmax\n",
    "      - Rule: first mismatch => replace & STOP\n",
    "    Returns: (draft, accepted, final_sequence)\n",
    "    \"\"\"\n",
    "    context = ensure_two_token_context(prompt_tokens)\n",
    "    draft, draft_trace = make_draft(context, k=k, T_draft=T_draft)\n",
    "    accepted, verify_log = verify_prefix_accept(context, draft)\n",
    "\n",
    "    if trace:\n",
    "        print(\"=== Draft stage (small model) ===\")\n",
    "        for i, (p2, p1, t) in enumerate(draft_trace, 1):\n",
    "            print(f\"[D{i}] prev2='{p2}' prev1='{p1}' -> draft='{t}'\")\n",
    "        print(\"\\n=== Verify stage (large model, prefix-accept) ===\")\n",
    "        for i, (p2, p1, t, v, ok) in enumerate(verify_log, 1):\n",
    "            status = \"ACCEPT\" if ok else \"REPLACE+STOP\"\n",
    "            print(f\"[V{i}] prev2='{p2}' prev1='{p1}'  draft='{t}'  verify='{v}'  ->  {status}\")\n",
    "        print(\"\\nDraft   :\", draft)\n",
    "        print(\"Accepted:\", accepted)\n",
    "        print(\"Final   :\", \" \".join(prompt_tokens + accepted))\n",
    "\n",
    "    return draft, accepted, prompt_tokens + accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83095050-f2fe-4e39-9f80-06f567876224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c74787a2-ea30-436a-a442-ad8a940d8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_step(prompt_tokens, k=5, T_draft=0.9, trace=True):\n",
    "    \"\"\"\n",
    "    Drafter(작은 모델): bigram(없으면 unigram) + 높은 T -> 다양성\n",
    "    Verifier(큰 모델): trigram->bigram->unigram + argmax -> 결정적\n",
    "    규칙: 왼쪽→오른쪽으로 비교하다가 '첫 불일치'에서 교체하고 종료\n",
    "    \"\"\"\n",
    "    # drafter = 작은 모델: bigram 우선, 없으면 unigram\n",
    "    def draft_next(prev1):\n",
    "        dist_small = bi.get(prev1, uni)\n",
    "        return sample_from_counts(dist_small, T_draft)\n",
    "\n",
    "    # 1) draft 만들기\n",
    "    context = list(prompt_tokens)\n",
    "    if len(context) < 2:\n",
    "        context = [context[-1], context[-1]]\n",
    "    prev2, prev1 = context[-2], context[-1]\n",
    "\n",
    "    draft = []\n",
    "    for _ in range(k):\n",
    "        t = draft_next(prev1)\n",
    "        draft.append(t)\n",
    "        prev2, prev1 = prev1, t  # drafter는 자신의 제안으로 문맥을 진전\n",
    "\n",
    "    # 2) verify: 진짜 프롬프트에서 다시 시작\n",
    "    accepted = []\n",
    "    prev2, prev1 = context[-2], context[-1]\n",
    "    log = []\n",
    "\n",
    "    for t in draft:\n",
    "        # 큰 모델의 ‘결정적’ 예측\n",
    "        v = argmax_from_counts(get_counts(prev2, prev1))\n",
    "        ok = (t == v)\n",
    "        log.append((prev2, prev1, t, v, ok))\n",
    "        if ok:\n",
    "            accepted.append(t)\n",
    "            prev2, prev1 = prev1, t  # 문맥 확장\n",
    "        else:\n",
    "            accepted.append(v)       # 교체 후 즉시 종료\n",
    "            break\n",
    "\n",
    "    if trace:\n",
    "        for i,(p2,p1,t,v,ok) in enumerate(log,1):\n",
    "            status = \"ACCEPT\" if ok else \"REPLACE+STOP\"\n",
    "            print(f\"[{i}] prev2='{p2}' prev1='{p1}'  draft='{t}'  verify='{v}'  ->  {status}\")\n",
    "        print(\"Draft   :\", draft)\n",
    "        print(\"Accepted:\", accepted)\n",
    "        print(\"Final   :\", \" \".join(prompt_tokens + accepted))\n",
    "\n",
    "    return draft, accepted, prompt_tokens + accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2a01c29-5489-4141-b49f-50349b9d78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) 한 번 실행해서 느낌 잡기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de5e1b28-bb01-4f4f-8dcc-85ff6c0567f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Baseline (T=0.7), next 5 tokens ----\n",
      "Baseline: the wolf ran into the forest the forest\n",
      "\n",
      "---- Speculative (k=5, T_draft=0.9) ----\n",
      "[1] prev2='wolf' prev1='ran'  draft='into'  verify='into'  ->  ACCEPT\n",
      "[2] prev2='ran' prev1='into'  draft='the'  verify='the'  ->  ACCEPT\n",
      "[3] prev2='into' prev1='the'  draft='wolf'  verify='forest'  ->  REPLACE+STOP\n",
      "Draft   : ['into', 'the', 'wolf', 'ran', 'into']\n",
      "Accepted: ['into', 'the', 'forest']\n",
      "Final   : the wolf ran into the forest\n"
     ]
    }
   ],
   "source": [
    "prompt = [\"the\", \"wolf\", \"ran\"]\n",
    "\n",
    "print(\"---- Baseline (T=0.7), next 5 tokens ----\")\n",
    "print(\"Baseline:\", \" \".join(generate_baseline(prompt, steps=5, T=0.7)))\n",
    "\n",
    "print(\"\\n---- Speculative (k=5, T_draft=0.9) ----\")\n",
    "_ = speculative_step(prompt, k=5, T_draft=0.9, trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd7dac31-62b7-4e70-988d-4520f1ede044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted length over 100 trials: 3.72 / 5\n"
     ]
    }
   ],
   "source": [
    "trials = 100\n",
    "total = 0\n",
    "for _ in range(trials):\n",
    "    _, acc, _ = speculative_step([\"the\",\"wolf\",\"ran\"], k=5, T_draft=0.9, trace=False)\n",
    "    total += len(acc)\n",
    "print(f\"Average accepted length over {trials} trials: {total/trials:.2f} / 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc378a-35a6-411b-a6ac-841c97c5d225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
