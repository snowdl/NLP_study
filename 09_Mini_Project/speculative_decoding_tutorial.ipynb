{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9ee8d82e-34c0-4905-bf3d-cd91d7b1f525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTokenization → Build & Print n-grams\\n=> “Data preparation” stage, where you directly check the internal structure.\\nBackoff distribution + Sampling/Argmax\\n=>Experimentally confirm the “basic behavior” of the model.\\nBaseline\\n=> A simple model that just samples one token at a time and appends it.\\nSpeculative (draft → verify)\\n=> “Small model proposes, large model verifies” → experience the prefix-accept rule.\\n100-trial average\\n=> Compute statistics to see how long the prefix is typically accepted with this method.\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenization → Build & Print n-grams\n",
    "=> “Data preparation” stage, where you directly check the internal structure.\n",
    "Backoff distribution + Sampling/Argmax\n",
    "=>Experimentally confirm the “basic behavior” of the model.\n",
    "Baseline\n",
    "=> A simple model that just samples one token at a time and appends it.\n",
    "Speculative (draft → verify)\n",
    "=> “Small model proposes, large model verifies” → experience the prefix-accept rule.\n",
    "100-trial average\n",
    "=> Compute statistics to see how long the prefix is typically accepted with this method.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3df7639-4fb7-46f2-bc4f-7efa542c9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0 — Imports & Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db48bc49-4878-4433-8d64-42eff36ff464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "random.seed(42)  # same results every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8dc16385-f8dc-4516-818f-c819900aba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 — Define a tiny corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e81610bf-0dd9-4bfc-b073-95908d1bb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: corpus\n",
    "corpus = \"the wolf ran into the forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f8ceb70-4be3-4eca-a5ae-9d605febe621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'wolf', 'ran', 'into', 'the', 'forest']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: tokenize\n",
    "tokens = corpus.lower().split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "426f852c-bb6f-471c-800f-8aa4481b8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94c795df-40c7-402f-b105-751c6c152dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unigrams ===\n",
      "'the': 2\n",
      "'wolf': 1\n",
      "'ran': 1\n",
      "'into': 1\n",
      "'forest': 1\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Unigram counts\n",
    "# Count how many times each word appears in the tokens.\n",
    "# Print as \"word : count\"\n",
    "uni = Counter(tokens)\n",
    "print(\"=== Unigrams ===\")\n",
    "for w, c in uni.items():\n",
    "    print(f\"{w!r}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "71f33849-7b6d-458a-b88a-598f49dbeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init bigram & trigram tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "32ea96c9-0498-4c42-ad01-5ff2e4da5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: init bigram & trigram\n",
    "#defaultdict(Counter) → a dictionary that automatically creates an empty Counter for any new key.\n",
    "bi  = defaultdict(Counter)            # prev -> Counter(next)\n",
    "tri = defaultdict(Counter)            # (prev2, prev1) -> Counter(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "865b88aa-3d4f-41bc-9009-2fa3213ab646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill bigram counts\n",
    "#Check every consecutive word pair, count occurrences, then print the bigram counts.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39fc7f00-5d50-44c9-ac7d-2c2b55bb735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bigrams ===\n",
      "('the' -> 'wolf'): 1\n",
      "('the' -> 'forest'): 1\n",
      "('wolf' -> 'ran'): 1\n",
      "('ran' -> 'into'): 1\n",
      "('into' -> 'the'): 1\n"
     ]
    }
   ],
   "source": [
    "# Step 5: fill bigram\n",
    "# Go through each pair of consecutive words (a, b).\n",
    "# Count how many times word 'a' is followed by word 'b'.\n",
    "for a, b in zip(tokens, tokens[1:]):\n",
    "    bi[a][b] += 1\n",
    "\n",
    "# Print the bigram table: (previous_word -> next_word): count\n",
    "print(\"=== Bigrams ===\")\n",
    "for prev, counter in bi.items():\n",
    "    for nxt, c in counter.items():\n",
    "        print(f\"({prev!r} -> {nxt!r}): {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "47da57f5-babb-4706-8327-421c715c5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill trigram counts\n",
    "#“Check every consecutive triplet, count how often a two-word context (w1, w2) is followed by another word\n",
    "# then print the trigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a5970b83-59b5-450b-94de-5a49b9a24339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trigrams ===\n",
      "(('the', 'wolf') -> 'ran'): 1\n",
      "(('wolf', 'ran') -> 'into'): 1\n",
      "(('ran', 'into') -> 'the'): 1\n",
      "(('into', 'the') -> 'forest'): 1\n"
     ]
    }
   ],
   "source": [
    "# Step 6: fill trigram\n",
    "# Go through each triplet of consecutive words (a, b, c).\n",
    "# Count how many times the pair (a, b) is followed by word c.\n",
    "for a, b, c in zip(tokens, tokens[1:], tokens[2:]):\n",
    "    tri[(a, b)][c] += 1\n",
    "\n",
    "# Print the trigram table: ((word1, word2) -> next_word): count\n",
    "print(\"=== Trigrams ===\")\n",
    "for (w1, w2), counter in tri.items():\n",
    "    for nxt, c in counter.items():\n",
    "        print(f\"(({w1!r}, {w2!r}) -> {nxt!r}): {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eb4ea4f3-46fc-4e38-bbd8-f4a9eb032b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#“Backoff = use trigram if possible, else bigram, else unigram.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9061e355-5c00-4d89-8436-570a50f575ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backoff: get next-token distribution# Step 7: backoff distribution\n",
    "# Rule:\n",
    "# 1) First, try trigram counts using (prev2, prev1).\n",
    "# 2) If not available, fall back to bigram counts using (prev1).\n",
    "# 3) If still not available, fall back to unigram counts.\n",
    "def next_counts_backoff(prev2, prev1):\n",
    "    d3 = tri.get((prev2, prev1))\n",
    "    if d3:\n",
    "        return d3\n",
    "    d2 = bi.get(prev1)\n",
    "    if d2:\n",
    "        return d2\n",
    "    return uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2befe-209b-496b-b784-28d49d21f733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "41edbdce-43d8-4edd-9ed1-237e67bc5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: backoff distribution\n",
    "# Try trigram((prev2, prev1)), else bigram(prev1), else unigram\n",
    "def next_counts_backoff(prev2, prev1):\n",
    "    d3 = tri.get((prev2, prev1))\n",
    "    if d3:\n",
    "        return d3\n",
    "    d2 = bi.get(prev1)\n",
    "    if d2:\n",
    "        return d2\n",
    "    return uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb3dbe2c-cfcf-4bad-bfd3-2b6ca85f5a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: ('wolf', 'ran')\n",
      "Distribution: {'into': 2}\n",
      "\n",
      "Context: ('ran', 'into')\n",
      "Distribution: {'the': 2}\n",
      "\n",
      "Context: ('the', 'wolf')\n",
      "Distribution: {'ran': 2}\n",
      "\n",
      "Context: ('hello', 'wolf')\n",
      "Distribution: {'ran': 2}\n",
      "\n",
      "Context: ('hello', 'zzz')\n",
      "Distribution: {'the': 2, 'wolf': 1, 'ran': 1, 'into': 1, 'forest': 1}\n"
     ]
    }
   ],
   "source": [
    "# Test contexts for backoff\n",
    "contexts = [\n",
    "    (\"wolf\", \"ran\"),   # exact trigram match → 'into'\n",
    "    (\"ran\", \"into\"),   # trigram match → 'the'\n",
    "    (\"the\", \"wolf\"),   # trigram match → 'ran'\n",
    "    (\"hello\", \"wolf\"), # trigram missing → fallback to bigram\n",
    "    (\"hello\", \"zzz\")   # both missing → fallback to unigram\n",
    "]\n",
    "\n",
    "for prev2, prev1 in contexts:\n",
    "    dist = next_counts_backoff(prev2, prev1)\n",
    "    print(f\"\\nContext: ({prev2!r}, {prev1!r})\")\n",
    "    print(\"Distribution:\", dict(dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d2785-0ea6-4c42-aefd-ff021ee88ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60cac1ba-30e3-41bd-a17e-d090391365c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n“Turn counts into probabilities, adjust sharpness with temperature T, then randomly pick one token.”\\nHigher T (>1.0): flatter distribution → more random / diverse\\nLower T (<1.0): sharper distribution → more deterministic / greedy\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Temperature sampling\n",
    "\"\"\"\n",
    "“Turn counts into probabilities, adjust sharpness with temperature T, then randomly pick one token.”\n",
    "Higher T (>1.0): flatter distribution → more random / diverse\n",
    "Lower T (<1.0): sharper distribution → more deterministic / greedy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e54dd77-df21-4ada-8430-c339a9dec4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: temperature sampling (probabilistic)\n",
    "def sample_from_counts(dist, T=1.0):\n",
    "    # Convert frequency counts into sampling weights\n",
    "    # Apply temperature scaling: weight = count ** (1/T)\n",
    "    items  = list(dist.items())\n",
    "    toks   = [t for t, _ in items]              # candidate tokens\n",
    "    cnts   = [c for _, c in items]              # their counts\n",
    "    weights = [(c if c > 0 else 1e-9) ** (1.0 / T) for c in cnts]\n",
    "    # Randomly choose one token according to weights\n",
    "    return random.choices(toks, weights=weights, k=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e5746ce-5fab-4076-ae9a-85e8013ce45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ('hello', 'zzz')\n",
      "Distribution (counts): {'the': 2, 'wolf': 1, 'ran': 1, 'into': 1, 'forest': 1}\n",
      "\n",
      "Sampled next (T=1.0): the\n",
      "Sampled next (T=0.5): ran\n",
      "Sampled next (T=2.0): into\n"
     ]
    }
   ],
   "source": [
    "# Example check for Step 8: temperature sampling\n",
    "prev2, prev1 = \"hello\", \"zzz\"   # context \"... wolf ran\"\n",
    "dist = next_counts_backoff(prev2, prev1)\n",
    "\n",
    "print(\"Context:\", (prev2, prev1))\n",
    "print(\"Distribution (counts):\", dict(dist))\n",
    "\n",
    "# Try different temperatures\n",
    "print(\"\\nSampled next (T=1.0):\", sample_from_counts(dist, T=1.0))\n",
    "print(\"Sampled next (T=0.5):\", sample_from_counts(dist, T=0.5))  # greedier\n",
    "print(\"Sampled next (T=2.0):\", sample_from_counts(dist, T=2.0))  # more random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c5076f15-22b3-497b-a362-e1e40f21ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Argmax (deterministic pick)\n",
    "#“Look at all tokens in the distribution and return the one with the largest count.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc5e1110-ebdd-4551-91d3-95909a131833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: argmax (most frequent)\n",
    "# Pick the token with the highest count (the mode of the distribution).\n",
    "def pick_most_frequent(dist):\n",
    "    return max(dist.items(), key=lambda kv: kv[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eaead2f9-b741-44e4-8582-feeb7c85e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure we have 2-token context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a47c3953-f3d8-4ef2-999a-8b115b8139c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: safeguard (if prompt has 1 token, duplicate it)\n",
    "# Ensure at least 2 tokens are available for trigram/bigram context.\n",
    "# If the prompt is only 1 word, duplicate it to create a pair.\n",
    "def ensure_two_token_context(seq):\n",
    "    if len(seq) < 2:\n",
    "        return [seq[-1], seq[-1]]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe742004-8543-4711-be0b-0d30704e04c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['wolf', 'ran'] -> Output: ['wolf', 'ran']\n",
      "Input: ['wolf'] -> Output: ['wolf', 'wolf']\n",
      "Input: ['the', 'wolf', 'ran'] -> Output: ['the', 'wolf', 'ran']\n"
     ]
    }
   ],
   "source": [
    "# Test for Step 10: ensure_two_token_context\n",
    "\n",
    "examples = [\n",
    "    [\"wolf\", \"ran\"],   # already 2 tokens\n",
    "    [\"wolf\"],          # only 1 token\n",
    "    [\"the\", \"wolf\", \"ran\"]  # more than 2 tokens\n",
    "]\n",
    "\n",
    "for seq in examples:\n",
    "    fixed = ensure_two_token_context(seq)\n",
    "    print(f\"Input: {seq} -> Output: {fixed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32846c96-53ef-42a4-9d3e-a1d5a25815c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline generator (one-token-at-a-time)\n",
    "#“The baseline simply extends the prompt one token at a time, using backoff + sampling.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5f2b257-6fc0-4809-a359-374954892641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: baseline generation (tri->bi->uni + sampling)\n",
    "# Generate text step by step using the backoff model.\n",
    "# Process:\n",
    "# 1) Ensure we have at least 2 tokens for context.\n",
    "# 2) Get the next-token distribution with backoff (tri -> bi -> uni).\n",
    "# 3) Sample one token from the distribution (with temperature T).\n",
    "# 4) Append the token and slide the context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70e4abb9-6a47-4b8d-804f-e4d9b3f51a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: baseline generation (tri->bi->uni + sampling)\n",
    "def generate_baseline(prompt_tokens, steps=5, T=0.7):\n",
    "    out = list(prompt_tokens)\n",
    "    out = ensure_two_token_context(out)\n",
    "    prev2, prev1 = out[-2], out[-1]\n",
    "    for _ in range(steps):\n",
    "        dist = next_counts_backoff(prev2, prev1)\n",
    "        nxt  = sample_from_counts(dist, T)\n",
    "        out.append(nxt)\n",
    "        prev2, prev1 = prev1, nxt\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf28d266-b04d-49dc-ae77-444cd7337bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: ['the', 'wolf', 'ran']\n",
      "\n",
      "Baseline generation (T=0.7, 5 steps):\n",
      "Output: the wolf ran into the forest the wolf\n",
      "\n",
      "Baseline generation (T=1.5, 5 steps, more random):\n",
      "Output: the wolf ran into the forest into the\n"
     ]
    }
   ],
   "source": [
    "# Test for Step 11: baseline generation\n",
    "\n",
    "prompt = [\"the\", \"wolf\", \"ran\"]\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nBaseline generation (T=0.7, 5 steps):\")\n",
    "result = generate_baseline(prompt, steps=5, T=0.7)\n",
    "print(\"Output:\", \" \".join(result))\n",
    "\n",
    "print(\"\\nBaseline generation (T=1.5, 5 steps, more random):\")\n",
    "result = generate_baseline(prompt, steps=5, T=1.5)\n",
    "print(\"Output:\", \" \".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6e4511a-1ecd-4e55-ac1e-22fe9ed22019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 — Speculative helper: build a draft (small model)\n",
    "# Step 12: drafter (small model = bigram if available, else unigram)\n",
    "# Build a draft sequence of k tokens.\n",
    "# - The \"small model\" uses bigram counts if available, otherwise unigram.\n",
    "# - At each step:\n",
    "#   1) Sample the next token from the small-model distribution.\n",
    "#   2) Log the context (prev2, prev1) and the draft token.\n",
    "#   3) Append the token to the draft.\n",
    "#   4) Advance the context window.\n",
    "# Returns:\n",
    "#   draft: the list of proposed tokens\n",
    "#   trace: log of (prev2, prev1, sampled_token) for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06ac0d04-cbc3-4bd3-8dcd-384197c55fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: drafter (small model = bigram if available, else unigram)\n",
    "def build_draft(context, k=5, T_draft=0.9):\n",
    "    prev2, prev1 = context[-2], context[-1]\n",
    "    draft, trace = [], []\n",
    "    for _ in range(k):\n",
    "        dist_small = bi.get(prev1, uni)     # small model\n",
    "        t = sample_from_counts(dist_small, T_draft)\n",
    "        trace.append((prev2, prev1, t))     # log before advancing\n",
    "        draft.append(t)\n",
    "        prev2, prev1 = prev1, t             # drafter advances its own context\n",
    "    return draft, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b74126df-93d4-46c5-b118-83bd61e1c09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: ['the', 'wolf', 'ran']\n",
      "\n",
      "Draft tokens: ['into', 'the', 'forest', 'the', 'forest']\n",
      "\n",
      "Trace (prev2, prev1 -> sampled_token):\n",
      "[1] ('wolf', 'ran') -> 'into'\n",
      "[2] ('ran', 'into') -> 'the'\n",
      "[3] ('into', 'the') -> 'forest'\n",
      "[4] ('the', 'forest') -> 'the'\n",
      "[5] ('forest', 'the') -> 'forest'\n"
     ]
    }
   ],
   "source": [
    "# Test for Step 12: drafter (build_draft)\n",
    "\n",
    "prompt = [\"the\", \"wolf\", \"ran\"]\n",
    "context = ensure_two_token_context(prompt)\n",
    "\n",
    "# Generate a draft of 5 tokens\n",
    "draft, trace = build_draft(context, k=5, T_draft=0.9)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nDraft tokens:\", draft)\n",
    "print(\"\\nTrace (prev2, prev1 -> sampled_token):\")\n",
    "for i, (p2, p1, t) in enumerate(trace, 1):\n",
    "    print(f\"[{i}] ({p2!r}, {p1!r}) -> {t!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93c5c09b-fe0c-43e0-8adf-c88f007a4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 13 — Speculative helper: verifier next (large model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c40a48ed-bf9f-41ed-9208-2a8612a47430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: verifier next (large model = backoff + argmax)\n",
    "# Use the large model:\n",
    "#   - Get the backoff distribution for (prev2, prev1).\n",
    "#   - Pick the most frequent (argmax) token deterministically.\n",
    "def verifier_next(prev2, prev1):\n",
    "    return pick_most_frequent(next_counts_backoff(prev2, prev1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "17e319c3-0ad6-41ce-bbed-6a1ea8b7c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: verify draft with prefix-accept\n",
    "# Compare the draft against the verifier's predictions step by step:\n",
    "# 1) For each draft token t:\n",
    "#    - Verifier predicts v (deterministic argmax from backoff model).\n",
    "#    - If t == v → accept token and extend context.\n",
    "#    - If t != v → replace with v and STOP (prefix-accept rule).\n",
    "# 2) Log each step as (prev2, prev1, draft_token, verify_token, ok_flag).\n",
    "# Returns:\n",
    "#    accepted: list of accepted (or replaced) tokens\n",
    "#    log     : detailed verification trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad712ade-745e-4289-951c-c0052b75b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: verify draft with prefix-accept\n",
    "def prefix_accept_verify(context, draft):\n",
    "    accepted, log = [], []\n",
    "    prev2, prev1 = context[-2], context[-1]\n",
    "    for t in draft:\n",
    "        v  = verifier_next(prev2, prev1)   # deterministic prediction\n",
    "        ok = (t == v)\n",
    "        log.append((prev2, prev1, t, v, ok))\n",
    "        accepted.append(t if ok else v)\n",
    "        if ok:\n",
    "            prev2, prev1 = prev1, t        # extend context\n",
    "        else:\n",
    "            break                           # replace & STOP\n",
    "    return accepted, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a275f2e6-6883-4bf8-9056-cefabaff798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 15 — Speculative step (orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4732441a-ef53-43bb-b925-d5c258d8b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: speculative (draft -> verify) — simple version\n",
    "def speculative_step(prompt_tokens, k=5, T_draft=0.9):\n",
    "    \"\"\"\n",
    "    Speculative decoding (simplified):\n",
    "    - Drafter: small model (bigram→unigram, with temperature sampling).\n",
    "    - Verifier: large model (trigram→bigram→unigram, argmax).\n",
    "    - Rule: accept matching prefix, replace on first mismatch and stop.\n",
    "    Returns: (draft, accepted, final_sequence)\n",
    "    \"\"\"\n",
    "    ctx = ensure_two_token_context(list(prompt_tokens))\n",
    "    draft, _ = build_draft(ctx, k=k, T_draft=T_draft)\n",
    "    accepted, _ = prefix_accept_verify(ctx, draft)\n",
    "    return draft, accepted, prompt_tokens + accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d245f188-98b0-4356-802c-20af46905d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt   : ['the', 'wolf', 'ran']\n",
      "Draft    : ['into', 'the', 'wolf', 'ran', 'into']\n",
      "Accepted : ['into', 'the', 'forest']\n",
      "Final    : the wolf ran into the forest\n"
     ]
    }
   ],
   "source": [
    "# Test for Step 15: speculative_step (simple version)\n",
    "\n",
    "prompt = [\"the\", \"wolf\", \"ran\"]\n",
    "\n",
    "draft, accepted, final = speculative_step(prompt, k=5, T_draft=0.9)\n",
    "\n",
    "print(\"Prompt   :\", prompt)\n",
    "print(\"Draft    :\", draft)\n",
    "print(\"Accepted :\", accepted)\n",
    "print(\"Final    :\", \" \".join(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2604f18e-152e-4ebc-b260-24745e12a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 16 — Quick demo: Baseline vs Speculative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f55842f4-ae11-4293-a1c1-b3ae05126e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Baseline (T=0.7), next 5 tokens ----\n",
      "Baseline: the wolf ran into the forest the forest\n",
      "\n",
      "---- Speculative (k=5, T_draft=0.9) ----\n",
      "\n",
      "---- Speculative (draft -> verify) ----\n",
      "=== Draft (small model) ===\n",
      "[D1] ('wolf','ran') -> draft='into'\n",
      "[D2] ('ran','into') -> draft='the'\n",
      "[D3] ('into','the') -> draft='wolf'\n",
      "[D4] ('the','wolf') -> draft='ran'\n",
      "[D5] ('wolf','ran') -> draft='into'\n",
      "\n",
      "=== Verify (prefix-accept) ===\n",
      "[V1] ('wolf','ran') draft='into' verify='into' -> ACCEPT\n",
      "[V2] ('ran','into') draft='the' verify='the' -> ACCEPT\n",
      "[V3] ('into','the') draft='wolf' verify='forest' -> REPLACE+STOP\n",
      "\n",
      "Draft   : ['into', 'the', 'wolf', 'ran', 'into']\n",
      "Accepted: ['into', 'the', 'forest']\n",
      "Final   : the wolf ran into the forest\n"
     ]
    }
   ],
   "source": [
    "# Step 16: demo\n",
    "prompt = [\"the\", \"wolf\", \"ran\"]\n",
    "\n",
    "print(\"---- Baseline (T=0.7), next 5 tokens ----\")\n",
    "print(\"Baseline:\", \" \".join(generate_baseline(prompt, steps=5, T=0.7)))\n",
    "\n",
    "print(\"\\n---- Speculative (k=5, T_draft=0.9) ----\")\n",
    "_ = speculative_step(prompt, k=5, T_draft=0.9, trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "456f94cb-5a0f-4ac6-ac0b-fba93b6bec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 17 — 100-trial average accepted prefix length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a028e3a-f0d9-4d3f-86f3-2b8043bd1f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average accepted length over 100 trials: 3.72 / 5\n"
     ]
    }
   ],
   "source": [
    "# Step 17: measure average accepted length over 100 trials\n",
    "trials, total = 100, 0\n",
    "for _ in range(trials):\n",
    "    _, acc, _ = speculative_step([\"the\",\"wolf\",\"ran\"], k=5, T_draft=0.9, trace=False)\n",
    "    total += len(acc)\n",
    "print(f\"\\nAverage accepted length over {trials} trials: {total/trials:.2f} / 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305afb2-b317-4007-9e37-dfb4bba78cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
