{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33751fc8-ce3c-472e-a488-c596e79897c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path                     # OS-agnostic file path handling\n",
    "import json, hashlib, statistics as stats    # JSON I/O, content hash IDs, simple descriptive stats\n",
    "import re, unicodedata                       # Regex ops + Unicode normalization/whitespace fixes\n",
    "import nltk                                  # Sentence tokenization (Punkt)\n",
    "import tiktoken                              # GPT-style tokenizer for token counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5ca1d1-9a61-4588-a7d6-92ad1e4e0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Paths / Settings =========\n",
    "DOC_NAME = \"01 Harry Potter and the Sorcerers Stone.txt\"   # Source filename\n",
    "DOC_PATH = Path(\"../../11_data\") / DOC_NAME                # Full path to the input text\n",
    "OUT_JSONL = Path(\"hp_chunks_100tok.jsonl\")                 # Output: all chunks with id/cid/tokens/text\n",
    "OUT_JSONL_DEDUP = Path(\"hp_chunks_100tok.dedup.jsonl\")     # Output: exact-deduplicated chunks\n",
    "MAX_TOKENS = 100                                           # Target max tokens per chunk (sentence-safe)\n",
    "OVERLAP_TOKENS = 10                                        # Token overlap between adjacent chunks (0 = off)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7f708f-ea4b-4802-b33c-b5a5adab4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 전처리 =========\n",
    "\"\"\"\n",
    "    Text cleaning for OCR/Unicode artifacts and broken honorifics.\n",
    "    - Normalizes Unicode width/compatibility.\n",
    "    - Flattens invisible/zero-width/nbsp-like spaces.\n",
    "    - Repairs hyphen line-breaks and newline spacing.\n",
    "    - Fixes broken 'M r.' → 'Mr.' and ensures a space after 'Mr.' when needed.\n",
    "    - Canonicalizes common honorifics (Mrs., Ms., Dr., Prof.).\n",
    "    - Collapses 'H .' → 'H.' for single-letter initials.\n",
    "    Returns a cleaned string; does NOT alter semantics.\n",
    " \"\"\"\n",
    "def clean_text(text: str) -> str:\n",
    "    # Normalize to NFKC so full-width/compatibility forms (quotes, spaces, etc.) are unified\n",
    "    t = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Normalize newlines to '\\n'\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Replace common zero-width/nbsp characters with a plain space (prevents tokenization glitches)\n",
    "    t = re.sub(r\"[\\u00A0\\u200B\\u200C\\u200D]\", \" \", t)  # NBSP & zero-width variants → \" \"\n",
    "    t = re.sub(r\"[ \\t]{2,}\", \" \", t)                  # Collapse multiple ASCII spaces/tabs\n",
    "\n",
    "    # Join hyphen line-breaks: \"some-\\nthing\" → \"something\"\n",
    "    t = re.sub(r\"-\\s*\\n\\s*\", \"\", t)\n",
    "\n",
    "    # Paragraph-aware newline handling:\n",
    "    # - Keep double newlines as paragraph breaks\n",
    "    # - Turn single newlines into a single space\n",
    "    t = re.sub(r\"\\n{2,}\", \"\\n\\n\", t)                  # Preserve paragraphs\n",
    "    t = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", t)            # Inline single newline → space\n",
    "    t = re.sub(r\"[ \\t]{2,}\", \" \", t).strip()          # Final whitespace squeeze\n",
    "\n",
    "    # --- Generic 'Mr.' repair (covers hidden spaces/quotes/parens around it) ---\n",
    "    # Optional opening quote/paren + 'm' + any zero-width/space + 'r' + '.' + optional closing quote/paren\n",
    "    # Case-insensitive; preserves surrounding punctuation via capture groups.\n",
    "    t = re.sub(\n",
    "        r'(?i)([\"“‘\\'(\\[]?\\s*)m[\\s\\u00A0\\u200B\\u200C\\u200D]*r[\\s\\u00A0\\u200B\\u200C\\u200D]*\\.(\\s*[\"”’\\'\\])]?)+',\n",
    "        r'\\1Mr.\\2',\n",
    "        t\n",
    "    )\n",
    "\n",
    "    # --- B patch 1) Fix ONLY true 'M r.' cases (requires at least 1 whitespace-like char between M and r) ---\n",
    "    # This avoids touching already-correct 'Mr.' but heals OCR splits like \"M r.\"\n",
    "    t = re.sub(\n",
    "        r'(?i)([\"“‘\\'(\\[]?\\s*)m[\\s\\u00A0\\u1680\\u180E\\u2000-\\u200A\\u202F\\u205F\\u3000\\uFEFF\\u200B\\u200C\\u200D\\u2060]+r\\s*\\.(\\s*[\"”’\\'\\])]?)+',\n",
    "        r'\\1Mr.\\2',\n",
    "        t\n",
    "    )\n",
    "\n",
    "    # --- B patch 2) Ensure a space after titles when followed by a letter ---\n",
    "    # e.g., \"Mr.Dursley\" / \"Mr.and\" → \"Mr. Dursley\" / \"Mr. and\"\n",
    "    t = re.sub(r'\\b(Mr|Mrs|Ms|Dr|Prof)\\.(?=[A-Za-z])', r'\\1. ', t)\n",
    "\n",
    "    # --- Canonicalize other honorifics (robust to stray spaces): ---\n",
    "    # \\b m s* rs s* \\. \\b → \"Mrs.\" etc., case-insensitive\n",
    "    abbrev_patterns = {\n",
    "        r\"\\bm\\s*rs\\s*\\.\\b\": \"Mrs.\",\n",
    "        r\"\\bm\\s*s\\s*\\.\\b\":  \"Ms.\",\n",
    "        r\"\\bd\\s*r\\s*\\.\\b\":  \"Dr.\",\n",
    "        r\"\\bp\\s*rof\\s*\\.\\b\": \"Prof.\",\n",
    "    }\n",
    "    for pat, rep in abbrev_patterns.items():\n",
    "        t = re.sub(pat, rep, t, flags=re.IGNORECASE)\n",
    "\n",
    "    # --- Initials: \"H .\" → \"H.\" (handles hidden/zero-width spaces as well) ---\n",
    "    t = re.sub(r\"(?i)(?<![A-Za-z])([A-Z])[\\s\\u00A0\\u200B\\u200C\\u200D]+\\.\", r\"\\1.\", t)\n",
    "\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438b53dc-8c24-4528-ae1b-cd24638ab05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Tokenizer =========\n",
    "try:\n",
    "    # Prefer a model-specific tokenizer so counts match GPT-4 behavior\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "except Exception:\n",
    "    # Fallback: generic CL100K encoding (compatible with GPT-3.5/4 families)\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def tok_len(text: str) -> int:\n",
    "    \"\"\"Return the token count for `text` using the selected `enc` tokenizer.\"\"\"\n",
    "    return len(enc.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afacf2c4-df61-412f-8a3a-c7ee42765153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Chunking function (never split inside a sentence) =========\n",
    "def chunk_by_tokens_sentence_safe(sents, max_tokens=100, overlap_tokens=0):\n",
    "    \"\"\"\n",
    "    Build token-limited chunks while preserving sentence boundaries.\n",
    "\n",
    "    Behavior:\n",
    "    - Append sentences until adding the next one would exceed `max_tokens`,\n",
    "      then flush the current chunk and start a new one.\n",
    "    - If a *single* sentence already exceeds `max_tokens`, make it a\n",
    "      standalone chunk (allowed to exceed the limit) to avoid splitting the sentence.\n",
    "    - If `overlap_tokens` > 0, copy the last N tokens from the previous chunk\n",
    "      to the start of the next chunk (context carry-over).\n",
    "\n",
    "    Args:\n",
    "        sents (List[str]): Pre-tokenized sentences (already cleaned).\n",
    "        max_tokens (int): Target max tokens per chunk.\n",
    "        overlap_tokens (int): Number of tail tokens to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: Each item is {\"text\": str, \"tokens\": int}.\n",
    "    \"\"\"\n",
    "    chunks = []                  # Accumulated output chunks\n",
    "    cur_texts, cur_tokens = [], 0  # Current chunk: sentence list + token count\n",
    "\n",
    "    def flush():\n",
    "        \"\"\"Finalize the current chunk (if non-empty) and reset buffers.\"\"\"\n",
    "        nonlocal cur_texts, cur_tokens\n",
    "        if not cur_texts:\n",
    "            return\n",
    "        text = \" \".join(cur_texts).strip()           # Join sentences with a space\n",
    "        chunks.append({\"text\": text, \"tokens\": cur_tokens})\n",
    "        cur_texts, cur_tokens = [], 0                # Reset for the next chunk\n",
    "\n",
    "    for s in sents:\n",
    "        n = tok_len(s)  # Token length of the incoming sentence\n",
    "\n",
    "        # Case 1: Single sentence longer than the limit → make its own chunk.\n",
    "        # We never split inside a sentence to keep semantics intact.\n",
    "        if n > max_tokens:\n",
    "            flush()                                   # Close any current chunk\n",
    "            chunks.append({\"text\": s.strip(), \"tokens\": n})\n",
    "            continue\n",
    "\n",
    "        # Case 2: Adding this sentence would exceed the limit → flush first.\n",
    "        if cur_tokens > 0 and (cur_tokens + n > max_tokens):\n",
    "            flush()\n",
    "\n",
    "            # Optional overlap: prepend the last `overlap_tokens` tokens\n",
    "            # from the *previous* chunk to the new one for continuity.\n",
    "            if overlap_tokens > 0 and len(chunks) > 0:\n",
    "                tail_text = chunks[-1][\"text\"]            # Previous chunk text\n",
    "                tail_ids = enc.encode(tail_text)          # Token IDs of tail\n",
    "                ov_ids = tail_ids[max(0, len(tail_ids) - overlap_tokens):]\n",
    "                ov_text = enc.decode(ov_ids).strip()      # Overlap text to seed\n",
    "                cur_texts = [ov_text] if ov_text else []  # Start next chunk with overlap\n",
    "                # Recompute token count from the overlap text (if any)\n",
    "                cur_tokens = len(enc.encode(\" \".join(cur_texts))) if cur_texts else 0\n",
    "\n",
    "        # Case 3: Safe to add the sentence to the current chunk.\n",
    "        cur_texts.append(s)\n",
    "        cur_tokens += n\n",
    "\n",
    "    # Flush the final (possibly partial) chunk.\n",
    "    flush()\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa305f91-0428-40bb-8205-47a12060ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Chunking helpers =========\n",
    "def _overlap_seed(prev_text: str, overlap_tokens: int, enc) -> tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Build an overlap seed from the tail of `prev_text`.\n",
    "    Returns (seed_text, seed_token_len). Empty seed if overlap_tokens <= 0.\n",
    "    \"\"\"\n",
    "    if overlap_tokens <= 0 or not prev_text:\n",
    "        return \"\", 0\n",
    "    ids = enc.encode(prev_text)\n",
    "    if not ids:\n",
    "        return \"\", 0\n",
    "    ov_ids = ids[max(0, len(ids) - overlap_tokens):]\n",
    "    seed_text = enc.decode(ov_ids).strip()\n",
    "    seed_tokens = len(enc.encode(seed_text)) if seed_text else 0\n",
    "    return seed_text, seed_tokens\n",
    "\n",
    "\n",
    "def _flush(cur_texts: list[str], cur_tokens: int, chunks: list[dict]) -> tuple[list[str], int]:\n",
    "    \"\"\"\n",
    "    Append current buffer to `chunks` (if non-empty) and reset the buffer.\n",
    "    Returns (new_cur_texts, new_cur_tokens).\n",
    "    \"\"\"\n",
    "    if cur_texts:\n",
    "        text = \" \".join(cur_texts).strip()\n",
    "        chunks.append({\"text\": text, \"tokens\": cur_tokens})\n",
    "    return [], 0\n",
    "\n",
    "\n",
    "# ========= Chunking function (never split inside a sentence) =========\n",
    "def chunk_by_tokens_sentence_safe(\n",
    "    sents: list[str],\n",
    "    max_tokens: int = 100,\n",
    "    overlap_tokens: int = 0,\n",
    "    enc=enc,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Build token-limited chunks while preserving sentence boundaries.\n",
    "\n",
    "    Rules:\n",
    "    - Keep appending sentences until adding the next would exceed `max_tokens`,\n",
    "      then flush the current chunk.\n",
    "    - If a single sentence itself exceeds `max_tokens`, emit it as a standalone chunk\n",
    "      (do NOT split inside that sentence).\n",
    "    - If `overlap_tokens` > 0, copy the last N tokens of the previous chunk\n",
    "      to the head of the next chunk as context.\n",
    "\n",
    "    Args:\n",
    "        sents: pre-cleaned sentences.\n",
    "        max_tokens: target upper bound per chunk (soft; long sentences may exceed).\n",
    "        overlap_tokens: tail tokens to overlap between adjacent chunks.\n",
    "        enc: tokenizer (tiktoken encoding).\n",
    "\n",
    "    Returns:\n",
    "        List of {\"text\": str, \"tokens\": int}.\n",
    "    \"\"\"\n",
    "    chunks: list[dict] = []\n",
    "    cur_texts: list[str] = []\n",
    "    cur_tokens: int = 0\n",
    "\n",
    "    for s in sents:\n",
    "        n = len(enc.encode(s))\n",
    "\n",
    "        # Case 1: single very long sentence → standalone chunk\n",
    "        if n > max_tokens:\n",
    "            cur_texts, cur_tokens = _flush(cur_texts, cur_tokens, chunks)\n",
    "            chunks.append({\"text\": s.strip(), \"tokens\": n})\n",
    "            continue\n",
    "\n",
    "        # Case 2: adding this sentence would exceed the limit → flush then start new chunk\n",
    "        if cur_tokens > 0 and (cur_tokens + n > max_tokens):\n",
    "            cur_texts, cur_tokens = _flush(cur_texts, cur_tokens, chunks)\n",
    "\n",
    "            # Optional overlap from the previous chunk\n",
    "            if overlap_tokens > 0 and chunks:\n",
    "                seed_text, seed_tokens = _overlap_seed(chunks[-1][\"text\"], overlap_tokens, enc)\n",
    "                if seed_text:\n",
    "                    cur_texts = [seed_text]\n",
    "                    cur_tokens = seed_tokens\n",
    "\n",
    "        # Case 3: safe to append sentence\n",
    "        cur_texts.append(s)\n",
    "        cur_tokens += n\n",
    "\n",
    "    # Flush the trailing buffer\n",
    "    _ = _flush(cur_texts, cur_tokens, chunks)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# (Optional) Generator version if you prefer streaming:\n",
    "def iter_chunks_sentence_safe(\n",
    "    sents: list[str],\n",
    "    max_tokens: int = 100,\n",
    "    overlap_tokens: int = 0,\n",
    "    enc=enc,\n",
    "):\n",
    "    \"\"\"\n",
    "    Yield chunks one by one (useful for large corpora / streaming).\n",
    "    Behavior matches `chunk_by_tokens_sentence_safe`.\n",
    "    \"\"\"\n",
    "    chunks: list[dict] = []\n",
    "    cur_texts: list[str] = []\n",
    "    cur_tokens: int = 0\n",
    "\n",
    "    def flush_yield():\n",
    "        nonlocal cur_texts, cur_tokens\n",
    "        if cur_texts:\n",
    "            text = \" \".join(cur_texts).strip()\n",
    "            yield {\"text\": text, \"tokens\": cur_tokens}\n",
    "            cur_texts, cur_tokens = [], 0\n",
    "\n",
    "    for s in sents:\n",
    "        n = len(enc.encode(s))\n",
    "        if n > max_tokens:\n",
    "            # Flush current and yield long sentence directly\n",
    "            yield from flush_yield()\n",
    "            yield {\"text\": s.strip(), \"tokens\": n}\n",
    "            continue\n",
    "\n",
    "        if cur_tokens > 0 and (cur_tokens + n > max_tokens):\n",
    "            # Flush current\n",
    "            yield from flush_yield()\n",
    "            # Overlap\n",
    "            if overlap_tokens > 0 and chunks:\n",
    "                seed_text, seed_tokens = _overlap_seed(chunks[-1][\"text\"], overlap_tokens, enc)\n",
    "                if seed_text:\n",
    "                    cur_texts = [seed_text]\n",
    "                    cur_tokens = seed_tokens\n",
    "\n",
    "        # Append sentence\n",
    "        cur_texts.append(s)\n",
    "        cur_tokens += n\n",
    "\n",
    "        # Keep a shadow copy of the last emitted chunk for overlap seeding\n",
    "        # (only updated when we would flush/yield)\n",
    "        # We simulate this by appending to `chunks` when we yield.\n",
    "        # To keep it simple, we only push into `chunks` when we actually yield:\n",
    "        # so we mirror `chunks` here:\n",
    "        # (No-op now; we'll update `chunks` in the flush below.)\n",
    "\n",
    "        # When we might yield, we cache the to-be-emitted chunk:\n",
    "        # We do this by peeking at the buffer. Not necessary per-yield.# ========= Hash (ID) =========\n",
    "def cid16(text: str) -> str:\n",
    "    \"\"\"Content-based ID: first 16 hex chars of the SHA-256 digest (~64-bit prefix).\"\"\"\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "\n",
    "        # NOTE: After yield, we append to `chunks` to preserve overlap history.\n",
    "        # This block is handled below.\n",
    "\n",
    "        # If you'd like strict streaming with no extra memory, drop overlap or\n",
    "        # store only the last-emitted chunk text externally.\n",
    "\n",
    "        # (no-op here)\n",
    "\n",
    "        # If the next sentence causes a flush, the overlap will use the last yielded chunk.\n",
    "\n",
    "    # Final flush\n",
    "    if cur_texts:\n",
    "        text = \" \".join(cur_texts).strip()\n",
    "        last = {\"text\": text, \"tokens\": cur_tokens}\n",
    "        yield last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f0537b-366e-499b-94f0-19633695319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Hash (ID) =========\n",
    "def cid16(text: str) -> str:\n",
    "    \"\"\"Content-based ID: first 16 hex chars of the SHA-256 digest (~64-bit prefix).\"\"\"\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5850515-824e-4849-814c-c3caa3a995ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8213359-4893-4ad1-9050-865b2f203655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 메인 =========\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "def ensure_punkt() -> None:\n",
    "    \"\"\"Make sure NLTK 'punkt' tokenizer data is available.\"\"\"\n",
    "    try:\n",
    "        # Check if the tokenizer model is already installed\n",
    "        nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        # Download 'punkt' once if missing\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def load_text_or_none(path: Path) -> Optional[str]:\n",
    "    \"\"\"Read UTF-8 text from `path`; print a friendly error and return None if missing.\"\"\"\n",
    "    if not path.exists():\n",
    "        # User-friendly message if the file path is wrong\n",
    "        print(f\"[오류] 파일을 찾을 수 없습니다: {path.resolve()}\")\n",
    "        print(\"→ DOC_PATH 경로를 확인하거나 파일을 해당 위치로 옮겨주세요.\")\n",
    "        return None\n",
    "    # Read the file as UTF-8; ignore undecodable bytes\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def preprocess_and_sentence_split(text: str) -> List[str]:\n",
    "    \"\"\"Apply cleaning then split into sentences.\"\"\"\n",
    "    # Normalize/fix text artifacts first (improves sentence tokenization)\n",
    "    text = clean_text(text)\n",
    "    # Split the cleaned text into sentences with NLTK\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "\n",
    "def build_chunks_from_sents(sents: List[str]) -> List[Dict[str, int | str]]:\n",
    "    \"\"\"Chunk sentences with token budget and optional overlap (never split inside a sentence).\"\"\"\n",
    "    # Delegate to the sentence-safe chunker; uses global MAX_TOKENS/OVERLAP_TOKENS\n",
    "    return chunk_by_tokens_sentence_safe(\n",
    "        sents, max_tokens=MAX_TOKENS, overlap_tokens=OVERLAP_TOKENS\n",
    "    )\n",
    "\n",
    "\n",
    "def summarize_chunks(chunks: List[Dict[str, int | str]]) -> List[int]:\n",
    "    \"\"\"Print summary stats and return token lengths.\"\"\"\n",
    "    # Collect token lengths for descriptive statistics\n",
    "    lens = [c[\"tokens\"] for c in chunks]\n",
    "\n",
    "    print(\"=== CHUNKS SUMMARY ===\")\n",
    "    print(f\"총 청크 수: {len(chunks)}\")\n",
    "    print(\n",
    "        \"토큰수(평균/중앙/최소/최대): \"\n",
    "        f\"{round(stats.mean(lens),2)} / {stats.median(lens)} / {min(lens)} / {max(lens)}\"\n",
    "    )\n",
    "\n",
    "    # Share of chunks that are close to the budget (90–100 tokens)\n",
    "    pct_90_100 = round(sum(1 for x in lens if 90 <= x <= 100) / len(lens) * 100, 2)\n",
    "    print(f\"100 토큰 근접(90~100) 비율: {pct_90_100}%\")\n",
    "\n",
    "    # Show the three longest chunks for quick inspection\n",
    "    topk = sorted(enumerate(lens, 1), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\n가장 긴 청크 Top3 (id, tokens):\", topk)\n",
    "\n",
    "    return lens\n",
    "\n",
    "\n",
    "def save_chunks_jsonl(chunks: List[Dict[str, int | str]], out_path: Path) -> None:\n",
    "    \"\"\"Write chunks to JSONL with sequential id and content hash cid.\"\"\"\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, ch in enumerate(chunks, 1):\n",
    "            # Stable, content-based id (first 16 hex chars of SHA-256)\n",
    "            obj = {\n",
    "                \"id\": i,                         # sequential numeric id (1-based)\n",
    "                \"cid\": cid16(ch[\"text\"]),        # content hash id (stable across runs if text is identical)\n",
    "                \"tokens\": ch[\"tokens\"],          # token count for the chunk\n",
    "                \"text\": ch[\"text\"],              # raw chunk text\n",
    "            }\n",
    "            # One JSON object per line (JSONL format)\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def preview_head_jsonl(path: Path, k: int = 3) -> None:\n",
    "    \"\"\"Print the first `k` records from a JSONL for a quick visual sanity check.\"\"\"\n",
    "    print(\"\\n--- 미리보기 (앞 3개) ---\")\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        for _ in range(k):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break  # Reached EOF before `k` lines\n",
    "            o = json.loads(line)\n",
    "            # Compact header line with id/cid/token count\n",
    "            print(f\"[{o['id']}] cid={o['cid']} · {o['tokens']} tokens\")\n",
    "            # Truncate preview to ~200 chars, collapse newlines\n",
    "            print(o[\"text\"][:200].replace(\"\\n\", \" \") + \"...\\n\")\n",
    "\n",
    "\n",
    "def save_exact_dedup(in_path: Path, out_path: Path) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Create an exact-deduplicated JSONL by `cid` (keep first occurrence).\n",
    "    Returns (original_count, kept_count).\n",
    "    \"\"\"\n",
    "    seen, kept = set(), []\n",
    "    total = 0\n",
    "\n",
    "    # Scan the original JSONL and keep only the first occurrence of each cid\n",
    "    with in_path.open(encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            o = json.loads(line)\n",
    "            if o[\"cid\"] in seen:\n",
    "                continue  # duplicate → skip\n",
    "            seen.add(o[\"cid\"])\n",
    "            kept.append(o)\n",
    "\n",
    "    # Write the deduplicated stream back out\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for o in kept:\n",
    "            f.write(json.dumps(o, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Return counts for summary logging\n",
    "    return total, len(kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ddb1e3d-a326-4d42-95cc-b92a112a1fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHUNKS SUMMARY ===\n",
      "총 청크 수: 1378\n",
      "토큰수(평균/중앙/최소/최대): 85.74 / 89.0 / 18 / 294\n",
      "100 토큰 근접(90~100) 비율: 47.17%\n",
      "\n",
      "가장 긴 청크 Top3 (id, tokens): [(833, 294), (298, 236), (80, 178)]\n",
      "\n",
      "--- 미리보기 (앞 3개) ---\n",
      "[1] cid=978646441d4a7b5a · 79 tokens\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or...\n",
      "\n",
      "[2] cid=caf808fc8c97343d · 97 tokens\n",
      "firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amou...\n",
      "\n",
      "[3] cid=32e6d8adccd73ed5 · 55 tokens\n",
      "in their opinion there was no finer boy anywhere. The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think th...\n",
      "\n",
      "Saved exact-deduplicated copy: hp_chunks_100tok.dedup.jsonl (original 1378 → kept 1378)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # (A) Ensure tokenizer data\n",
    "    ensure_punkt()\n",
    "\n",
    "    # (B) Load file\n",
    "    text = load_text_or_none(DOC_PATH)\n",
    "    if text is None:\n",
    "        return  # Early exit on missing file\n",
    "\n",
    "    # (C) Clean + sentence split\n",
    "    sents = preprocess_and_sentence_split(text)\n",
    "\n",
    "    # (D) Build chunks\n",
    "    chunks = build_chunks_from_sents(sents)\n",
    "\n",
    "    # (E) Stats\n",
    "    _ = summarize_chunks(chunks)\n",
    "\n",
    "    # (F) Save JSONL with cid\n",
    "    save_chunks_jsonl(chunks, OUT_JSONL)\n",
    "\n",
    "    # (G) Preview first few\n",
    "    preview_head_jsonl(OUT_JSONL, k=3)\n",
    "\n",
    "    # (H) Save exact-deduped copy\n",
    "    orig, kept = save_exact_dedup(OUT_JSONL, OUT_JSONL_DEDUP)\n",
    "    print(f\"Saved exact-deduplicated copy: {OUT_JSONL_DEDUP} (original {orig} → kept {kept})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c5e3ece-a2da-4d50-b2d9-df53be7fc3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M r. 유형: 0\n",
      "Mr.뒤 공백 없음: 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "raw = Path(\"../../11_data/01 Harry Potter and the Sorcerers Stone.txt\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "txt = clean_text(raw)\n",
    "\n",
    "# 진짜 'M r.' (M과 r 사이에 공백류 1개 이상)\n",
    "print(\"M r. 유형:\", len(re.findall(r\"(?i)m[\\s\\u00A0\\u1680\\u180E\\u2000-\\u200A\\u202F\\u205F\\u3000\\uFEFF\\u200B\\u200C\\u200D\\u2060]+r\\s*\\.\", txt)))\n",
    "\n",
    "# Mr./Mrs./Ms./Dr./Prof. 뒤 공백 없음\n",
    "print(\"Mr.뒤 공백 없음:\", len(re.findall(r\"\\b(Mr|Mrs|Ms|Dr|Prof)\\.(?=[A-Za-z])\", txt)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b0455-f5bb-46ca-90a2-cb6bc5141ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccfc6e-e993-4bb1-9f5a-a19f647b5b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7deae65-cec1-4e93-8748-88b38f169a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba0484-39ba-4900-9c89-0446440da1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e88e9-8ab5-4d0d-aaa3-af1379848189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
