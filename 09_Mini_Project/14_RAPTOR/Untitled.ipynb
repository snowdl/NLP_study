{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e7049d-b64f-40c3-851a-1b5c7f180e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'node_info, chunk_text ë¡œë“œ\\n\\nCell 1~5 ì‹¤í–‰ (ë ˆë²¨ ì¸ë±ìŠ¤ â†’ ì„ë² ë”© â†’ ë…¸ë“œ ë­í‚¹ â†’ ë¦¬í”„ í™•ì¥ â†’ ê²€ìƒ‰/ë‹µë³€)\\n\\npretty_answer(\"ì§ˆë¬¸\")ìœ¼ë¡œ ë°”ë¡œ í…ŒìŠ¤íŠ¸\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"node_info, chunk_text ë¡œë“œ\n",
    "\n",
    "Cell 1~5 ì‹¤í–‰ (ë ˆë²¨ ì¸ë±ìŠ¤ â†’ ì„ë² ë”© â†’ ë…¸ë“œ ë­í‚¹ â†’ ë¦¬í”„ í™•ì¥ â†’ ê²€ìƒ‰/ë‹µë³€)\n",
    "\n",
    "pretty_answer(\"ì§ˆë¬¸\")ìœ¼ë¡œ ë°”ë¡œ í…ŒìŠ¤íŠ¸\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce5e960-3bf5-45e7-bd54-21aaf566177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… node_info: 6 nodes\n",
      "âœ… chunk_text: 227 chunks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path.cwd().parents[0] / \"13_RAPTOR\" if (Path.cwd().name != \"13_RAPTOR\") else Path.cwd()\n",
    "OUT  = BASE / \"outputs\"\n",
    "\n",
    "nodes_path = OUT / \"tree_nodes.jsonl\"\n",
    "chunks_path = OUT / \"chunks.jsonl\"\n",
    "\n",
    "# 1) ë…¸ë“œ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "nodes = [json.loads(l) for l in open(nodes_path, encoding=\"utf-8\")]\n",
    "node_info = {nd[\"node_id\"]: (nd[\"level\"], nd[\"children\"], nd[\"summary\"]) for nd in nodes}\n",
    "\n",
    "# 2) ì²­í¬ í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "chunk_text = {json.loads(l)[\"chunk_id\"]: json.loads(l)[\"text\"] for l in open(chunks_path, encoding=\"utf-8\")}\n",
    "\n",
    "print(\"âœ… node_info:\", len(node_info), \"nodes\")\n",
    "print(\"âœ… chunk_text:\", len(chunk_text), \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e09638-0366-4342-9d01-02dedd16c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë ˆë²¨ ì¸ë±ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bcd6962-946b-4aaf-8b6b-d58930057a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "levels = defaultdict(list)  # level -> [(node_id, summary)]\n",
    "for nid, (lvl, children, summ) in node_info.items():\n",
    "    levels[lvl].append((nid, summ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a817bc1-f698-4c5b-9e58-52ad5beb8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì„ë² ë”©(ê°„ë‹¨ TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9171ebae-6407-405b-b3b1-41af3b434a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def _fit(texts):\n",
    "    v = TfidfVectorizer(ngram_range=(1,2), max_features=60000)\n",
    "    return v, v.fit_transform(texts)\n",
    "def _score(M, v, q):  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
    "    return cosine_similarity(M, v.transform([q])).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33bfa82b-5b44-4a9b-a9aa-119455940b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 â€” ë ˆë²¨ ì„ íƒ + ë…¸ë“œ ë­í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c50c63c8-f777-4147-b43d-4569eb16d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np\n",
    "def select_start_level(q):\n",
    "    ks = sorted(levels.keys())\n",
    "    if not ks: return None\n",
    "    qlen = len(re.findall(r\"\\w+\", q))\n",
    "    return (ks[-1] if qlen<=5 else (ks[-2] if len(ks)>1 and qlen<=12 else ks[0]))\n",
    "\n",
    "def rank_nodes_at_level(q, L, topk=6):\n",
    "    items = levels.get(L, [])\n",
    "    ids = [i for i,_ in items]; txts = [t for _,t in items]\n",
    "    v, M = _fit(txts); sims = _score(M, v, q)\n",
    "    idx = np.argsort(-sims)[:topk]\n",
    "    return [ids[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c66f13ad-50bf-442d-ac28-a74ec9ca582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 â€” í™•ì¥â†’ë¦¬í”„â†’ì¬ë­í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a2dd5c6-b4c8-40b8-9d83-05750f2a4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_chunk(x): return isinstance(x,str) and x.startswith(\"C\")\n",
    "\n",
    "def expand_to_leaves(seed_ids, hops=2):\n",
    "    out, fr = [], list(seed_ids)\n",
    "    for _ in range(hops):\n",
    "        nxt=[]\n",
    "        for nid in fr:\n",
    "            if _is_chunk(nid): out.append(nid)\n",
    "            elif nid in node_info: nxt += node_info[nid][1]\n",
    "        fr = nxt\n",
    "        if not fr: break\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    seen, uniq = set(), []\n",
    "    for cid in out:\n",
    "        if cid not in seen: uniq.append(cid); seen.add(cid)\n",
    "    return uniq\n",
    "\n",
    "def rerank_chunks(q, cids, topk=5):\n",
    "    ids, txts = [], []\n",
    "    for cid in cids:\n",
    "        if cid in chunk_text: ids.append(cid); txts.append(chunk_text[cid])\n",
    "    if not txts: return []\n",
    "    v, M = _fit(txts); sims = _score(M, v, q)\n",
    "    idx = np.argsort(-sims)[:topk]\n",
    "    return [(ids[i], float(sims[i])) for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3bb0336-1f1b-4d95-a048-7917f61fef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#â€” ë©”ì¸ ê²€ìƒ‰ + ê°„ë‹¨ ë‹µë³€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2089a0b3-c0cc-4c97-a436-a6f78bc50674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Q: What strange events happened on Privet Drive?\n",
      "ğŸ“ start level: L2\n",
      "ğŸ“‘ nodes: ['L2_N0002', 'L2_N0001']\n",
      "ğŸ’¬ A: (no matching evidence)\n"
     ]
    }
   ],
   "source": [
    "def raptor_search(q, topk_nodes=6, topk_chunks=5, hops=2):\n",
    "    L = select_start_level(q)\n",
    "    if L is None: return {\"level\": None, \"nodes\": [], \"chunks\": []}\n",
    "    seeds = rank_nodes_at_level(q, L, topk=topk_nodes)\n",
    "    cand  = expand_to_leaves(seeds, hops=hops) or seeds\n",
    "    chunks = rerank_chunks(q, cand, topk=topk_chunks)\n",
    "    return {\"level\": L, \"nodes\": seeds, \"chunks\": chunks}\n",
    "\n",
    "def pretty_answer(q, sent_per_chunk=2):\n",
    "    import re, numpy as np\n",
    "    res = raptor_search(q)\n",
    "    picks=[]\n",
    "    for cid,_ in res[\"chunks\"]:\n",
    "        sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', chunk_text[cid]) if s.strip()]\n",
    "        if not sents: continue\n",
    "        v, M = _fit(sents); sims = _score(M, v, q)\n",
    "        idx = np.argsort(-sims)[:sent_per_chunk]\n",
    "        picks.append(f\"[{cid}] \" + \" \".join(sents[i] for i in idx))\n",
    "    print(f\"ğŸ” Q: {q}\\nğŸ“ start level: L{res['level']}\\nğŸ“‘ nodes: {res['nodes']}\\nğŸ’¬ A: {' '.join(picks) if picks else '(no matching evidence)'}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "pretty_answer(\"What strange events happened on Privet Drive?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "666f0c9c-6453-42ba-b851-016247b83705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeds: ['L2_N0002', 'L2_N0001']\n",
      "cand chunks (topN): []\n"
     ]
    }
   ],
   "source": [
    "res = raptor_search(\"What strange events happened on Privet Drive?\")\n",
    "print(\"seeds:\", res[\"nodes\"])\n",
    "print(\"cand chunks (topN):\", [c for c,_ in res[\"chunks\"]][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d168d909-5c76-4dfb-a8d9-31f42b3dbcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[seed] L2_N0002\n",
      "  hop0: L2_N0002 -> children(1): ['L1_N0003']\n",
      "  hop1: L1_N0003 -> children(1): ['C0005']\n",
      "\n",
      "[seed] L2_N0001\n",
      "  hop0: L2_N0001 -> children(2): ['L1_N0001', 'L1_N0002']\n",
      "  hop1: L1_N0001 -> children(2): ['C0001', 'C0002']\n",
      "  hop1: L1_N0002 -> children(2): ['C0003', 'C0004']\n"
     ]
    }
   ],
   "source": [
    "def debug_children(nids, depth=2):\n",
    "    for nid in nids:\n",
    "        cur = [nid]\n",
    "        print(f\"\\n[seed] {nid}\")\n",
    "        for d in range(depth):\n",
    "            nxt = []\n",
    "            for x in cur:\n",
    "                info = node_info.get(x)\n",
    "                kids = [] if info is None else info[1]\n",
    "                print(f\"  hop{d}: {x} -> children({len(kids)}): {kids[:6]}\")\n",
    "                nxt += kids\n",
    "            cur = nxt\n",
    "            if not cur: break\n",
    "\n",
    "# raptor_search ì „ì— í•œ ë²ˆ:\n",
    "seeds = ['L2_N0002','L2_N0001']  # ë°©ê¸ˆ ì¶œë ¥ëœ ì‹œë“œë¡œ ì‹œí—˜\n",
    "debug_children(seeds, depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480b6fa-3912-403b-aa30-e75a0c6aa4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
