{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64e68a0-908d-48c2-84ce-4aa17049958a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13_RAPTOR/\\n├─ data/\\n├─ outputs/              # 여기 있는 결과를 재사용\\n├─ notebooks/\\n│  ├─ 01_day1_tree_build.ipynb\\n│  └─ 02_day2_retrieval.ipynb   ← 여기 맨 위에 1번 셀 붙이기\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"13_RAPTOR/\n",
    "├─ data/\n",
    "├─ outputs/              # 여기 있는 결과를 재사용\n",
    "├─ notebooks/\n",
    "│  ├─ 01_day1_tree_build.ipynb\n",
    "│  └─ 02_day2_retrieval.ipynb   ← 여기 맨 위에 1번 셀 붙이기\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc1e5fe-61a8-4285-a31a-4a48c219289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 경고 끄기(선택)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4c8b80-c77a-4843-a2bf-4a3a5e890f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82d85d2-8057-4c01-b88a-b9d3ee4c6ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 로드 완료: 11 개 요약\n"
     ]
    }
   ],
   "source": [
    "BASE = Path.cwd().parents[0] / \"13_RAPTOR\" if (Path.cwd().name != \"13_RAPTOR\") else Path.cwd()\n",
    "OUT  = BASE / \"outputs\"\n",
    "\n",
    "chunks_path = OUT / \"chunks.jsonl\"\n",
    "summ_smoke  = OUT / \"chunk_summaries_smoke.jsonl\"\n",
    "summ_all    = OUT / \"chunk_summaries.jsonl\"\n",
    "nodes_path  = OUT / \"tree_nodes.jsonl\"\n",
    "\n",
    "summ_path = summ_smoke if summ_smoke.exists() else summ_all\n",
    "\n",
    "# 청크 원문\n",
    "chunk_text = {json.loads(l)[\"chunk_id\"]: json.loads(l)[\"text\"] for l in open(chunks_path, encoding=\"utf-8\")}\n",
    "# leaf 요약\n",
    "leaf_summary = {json.loads(l)[\"chunk_id\"]: json.loads(l)[\"summary\"] for l in open(summ_path, encoding=\"utf-8\")}\n",
    "# 노드 요약\n",
    "nodes = [json.loads(l) for l in open(nodes_path, encoding=\"utf-8\")]\n",
    "node_info = {nd[\"node_id\"]: (nd[\"level\"], nd[\"children\"], nd[\"summary\"]) for nd in nodes}\n",
    "\n",
    "# 검색 대상(노드 + 리프)\n",
    "corpus_ids, corpus_txt = [], []\n",
    "for nid, (_,_,summ) in node_info.items():\n",
    "    corpus_ids.append(nid); corpus_txt.append(summ)\n",
    "for cid, summ in leaf_summary.items():\n",
    "    corpus_ids.append(cid); corpus_txt.append(summ)\n",
    "\n",
    "print(\"✅ 로드 완료:\", len(corpus_ids), \"개 요약\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d01991b-2ae5-40d3-b73e-1068018cada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3. 간단 임베딩 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb393030-8bb4-40f3-be5e-2a6c7f0b2dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4914d42a8d884af9a9b9cef769befc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa523a67b9649e98bb914cc4a24fc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b381e86a9254d628836f41f0631bdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21535bf1971b4dffa830ae3d41e5d87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2830414e7cb14f7593e2f0c74b20b5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9955db00604f189a5abd8f2d5f3282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdd35e4093c43b2b5a342dc888c663b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cab67c40a84aacbb69c42d268495c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6130e9f11444a8165736e34d287a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca61225d0c514a88959643f29b58b610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b64d5639684b018e9bff2dee8046e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeaa705335544b39237f4ab451a6f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SBERT 사용\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    emb_mat = model.encode(corpus_txt, normalize_embeddings=True, show_progress_bar=True)\n",
    "    backend = \"sbert\"\n",
    "    print(\"✅ SBERT 사용\")\n",
    "except:\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\n",
    "    emb_mat = vect.fit_transform(corpus_txt)\n",
    "    backend = \"tfidf\"\n",
    "    print(\"✅ TF-IDF 사용\")\n",
    "\n",
    "def topk_in_corpus(query, k=5):\n",
    "    if backend==\"sbert\":\n",
    "        qv = model.encode([query], normalize_embeddings=True)[0]\n",
    "        sims = emb_mat @ qv\n",
    "    else:\n",
    "        qv = vect.transform([query])\n",
    "        sims = cosine_similarity(emb_mat, qv).ravel()\n",
    "    idx = np.argsort(-sims)[:k]\n",
    "    return [(corpus_ids[i], float(sims[i])) for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f79070d1-1835-464d-975b-60162c84f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4. Retrieval & 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5056b68-1ffa-406f-8e25-5f401d4368b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 간단 전처리\n",
    "def _clean(s: str) -> str:\n",
    "    s = re.sub(r\"\\bM\\s+r\\.\", \"Mr.\", s)      # \"M r.\" -> \"Mr.\"\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _split_sents(text: str):\n",
    "    # 따옴표/마침표 기준 문장 분할\n",
    "    sents = re.split(r'(?<=[.!?]\")\\s+|(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "def _keywords(q: str):\n",
    "    stop = {\"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"on\",\"at\",\"for\",\"with\",\"is\",\"are\",\"was\",\"were\",\"do\",\"does\",\"did\",\"what\",\"who\",\"where\",\"when\",\"how\",\"why\"}\n",
    "    toks = re.findall(r\"[A-Za-z']+\", q.lower())\n",
    "    return sorted({t for t in toks if t not in stop and len(t) >= 3})\n",
    "\n",
    "def is_chunk_id(x): \n",
    "    return isinstance(x, str) and x.startswith(\"C\")\n",
    "\n",
    "def descend_to_chunks(ids, max_hops=3):\n",
    "    \"\"\"노드 id들에서 시작 → 리프(청크)까지 내려가기\"\"\"\n",
    "    out, frontier = [], list(ids)\n",
    "    for _ in range(max_hops):\n",
    "        nxt = []\n",
    "        for _id in frontier:\n",
    "            if is_chunk_id(_id):\n",
    "                out.append(_id)\n",
    "            elif _id in node_info:\n",
    "                _, children, _ = node_info[_id]\n",
    "                nxt.extend(children)\n",
    "            elif _id in leaf_summary:  # 요약만 있는 리프일 수도 있음\n",
    "                out.append(_id)\n",
    "        frontier = nxt\n",
    "        if not frontier:\n",
    "            break\n",
    "    # 중복 제거, 순서 유지\n",
    "    seen, uniq = set(), []\n",
    "    for cid in out:\n",
    "        if cid not in seen:\n",
    "            uniq.append(cid); seen.add(cid)\n",
    "    return uniq\n",
    "\n",
    "def raptor_retrieve(query: str, topk_nodes=6, topk_chunks=5):\n",
    "    \"\"\"상위 후보 노드/리프 → 리프로 내려가서 청크 재랭킹\"\"\"\n",
    "    hits = topk_in_corpus(query, k=topk_nodes)              # [(id, score), ...]\n",
    "    hit_ids = [hid for hid, _ in hits]\n",
    "\n",
    "    candidate_chunks = descend_to_chunks(hit_ids, max_hops=3)\n",
    "    if not candidate_chunks:\n",
    "        candidate_chunks = [hid for hid, _ in hits if is_chunk_id(hid)]\n",
    "\n",
    "    # 청크 텍스트 기준 재랭킹 (TF-IDF)\n",
    "    cids, ctexts = [], []\n",
    "    for cid in candidate_chunks:\n",
    "        if cid in chunk_text:\n",
    "            cids.append(cid)\n",
    "            ctexts.append(_clean(chunk_text[cid]))\n",
    "    if not ctexts:\n",
    "        return {\"nodes\": hits, \"chunks\": []}\n",
    "\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_features=50000)\n",
    "    M    = vect.fit_transform(ctexts)\n",
    "    qv   = vect.transform([query])\n",
    "    sims = cosine_similarity(M, qv).ravel()\n",
    "    order = np.argsort(-sims)[:topk_chunks]\n",
    "    top_chunks = [(cids[i], float(sims[i])) for i in order]\n",
    "    return {\"nodes\": hits, \"chunks\": top_chunks}\n",
    "\n",
    "def answer_query(query: str, topk_nodes=6, topk_chunks=5, sent_per_chunk=2, max_chars=400):\n",
    "    \"\"\"dict 반환: {'retrieval':..., 'answer': str} — 질문 키워드가 포함된 문장 위주로 추출\"\"\"\n",
    "    res = raptor_retrieve(query, topk_nodes, topk_chunks)\n",
    "    kws = _keywords(query)\n",
    "    snippets = []\n",
    "\n",
    "    for cid, _ in res.get(\"chunks\", []):\n",
    "        text = _clean(chunk_text.get(cid, \"\"))\n",
    "        if not text:\n",
    "            continue\n",
    "        sents = _split_sents(text)\n",
    "        if not sents:\n",
    "            continue\n",
    "\n",
    "        # 1) 키워드 포함 문장 먼저 필터\n",
    "        key_sents = [s for s in sents if any(k in s.lower() for k in kws)] or sents\n",
    "\n",
    "        # 2) 필터된 문장들만 재랭킹(TF-IDF) 후 상위 n개 선택\n",
    "        vect = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_features=20000)\n",
    "        M = vect.fit_transform(key_sents)\n",
    "        q = vect.transform([query])\n",
    "        sims = cosine_similarity(M, q).ravel()\n",
    "        idxs = np.argsort(-sims)[:sent_per_chunk]\n",
    "\n",
    "        picked = \" \".join(key_sents[i] for i in idxs)\n",
    "        snippets.append(f\"[{cid}] {picked}\")\n",
    "\n",
    "        if len(\" \".join(snippets)) > max_chars:\n",
    "            break\n",
    "\n",
    "    # 보조: 아무 것도 못 뽑았으면 leaf summary를 1~2개 추가\n",
    "    if not snippets:\n",
    "        for hid, _ in topk_in_corpus(query, k=6):\n",
    "            if is_chunk_id(hid) and hid in leaf_summary:\n",
    "                snippets.append(f\"[{hid}-summary] {leaf_summary[hid]}\")\n",
    "                if len(snippets) >= 2:\n",
    "                    break\n",
    "\n",
    "    answer = \" \".join(snippets).strip()\n",
    "    return {\"retrieval\": res, \"answer\": answer if answer else \"(no matching evidence)\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff94ceb-9b00-4346-bb86-605ae3e5dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5. 테스트!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32416217-b50b-40f0-96aa-4a0f1e527521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_answer(query, topk_nodes=6, topk_chunks=5, sent_per_chunk=2):\n",
    "    out = answer_query(query, topk_nodes, topk_chunks, sent_per_chunk)\n",
    "    print(\"🔎 Q:\", query)\n",
    "    print(\"💬 A:\", out[\"answer\"])\n",
    "    if out[\"retrieval\"][\"chunks\"]:\n",
    "        print(\"📑 Evidence:\", [cid for cid,_ in out[\"retrieval\"][\"chunks\"]])\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f6798b-927e-4800-804c-84ea59531f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Q: Who is Harry Potter's best friend?\n",
      "💬 A: [C0003] He was sure there were lots of people called Potter who had a son called Harry. Come to think of it, he wasn’t even sure his nephew was called Harry. [C0001] Potter was Mrs. None of them noticed a large, tawny owl flutter past the window. [C0004] Rejoice, for You-Know-Who has gone at last! Dursley; she always got so upset at any mention of her sister. [C0002] Dursley couldn’t bear people who dressed in funny clothes — the getups you saw on young people! Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls.\n",
      "📑 Evidence: ['C0003', 'C0001', 'C0004', 'C0002', 'C0005']\n",
      "--------------------------------------------------------------------------------\n",
      "🔎 Q: What strange events happened on Privet Drive?\n",
      "💬 A: [C0002] There was a tabby cat standing on the corner of Privet Drive, but there wasn’t a map in sight. It was now reading the sign that said Privet Drive — no, looking at the sign; cats couldn’t read maps or signs. [C0001] Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country.\n",
      "📑 Evidence: ['C0002', 'C0001', 'C0003', 'C0004', 'C0005']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pretty_answer(\"Who is Harry Potter's best friend?\")\n",
    "pretty_answer(\"What strange events happened on Privet Drive?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da474e-8c07-467d-973f-85fbc9a6c3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
