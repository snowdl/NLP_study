{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64e68a0-908d-48c2-84ce-4aa17049958a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13_RAPTOR/\\nâ”œâ”€ data/\\nâ”œâ”€ outputs/              # ì—¬ê¸° ìˆëŠ” ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©\\nâ”œâ”€ notebooks/\\nâ”‚  â”œâ”€ 01_day1_tree_build.ipynb\\nâ”‚  â””â”€ 02_day2_retrieval.ipynb   â† ì—¬ê¸° ë§¨ ìœ„ì— 1ë²ˆ ì…€ ë¶™ì´ê¸°\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"13_RAPTOR/\n",
    "â”œâ”€ data/\n",
    "â”œâ”€ outputs/              # ì—¬ê¸° ìˆëŠ” ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©\n",
    "â”œâ”€ notebooks/\n",
    "â”‚  â”œâ”€ 01_day1_tree_build.ipynb\n",
    "â”‚  â””â”€ 02_day2_retrieval.ipynb   â† ì—¬ê¸° ë§¨ ìœ„ì— 1ë²ˆ ì…€ ë¶™ì´ê¸°\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc1e5fe-61a8-4285-a31a-4a48c219289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # ê²½ê³  ë„ê¸°(ì„ íƒ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4c8b80-c77a-4843-a2bf-4a3a5e890f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82d85d2-8057-4c01-b88a-b9d3ee4c6ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¡œë“œ ì™„ë£Œ: 11 ê°œ ìš”ì•½\n"
     ]
    }
   ],
   "source": [
    "BASE = Path.cwd().parents[0] / \"13_RAPTOR\" if (Path.cwd().name != \"13_RAPTOR\") else Path.cwd()\n",
    "OUT  = BASE / \"outputs\"\n",
    "\n",
    "chunks_path = OUT / \"chunks.jsonl\"\n",
    "summ_smoke  = OUT / \"chunk_summaries_smoke.jsonl\"\n",
    "summ_all    = OUT / \"chunk_summaries.jsonl\"\n",
    "nodes_path  = OUT / \"tree_nodes.jsonl\"\n",
    "\n",
    "summ_path = summ_smoke if summ_smoke.exists() else summ_all\n",
    "\n",
    "# ì²­í¬ ì›ë¬¸\n",
    "chunk_text = {json.loads(l)[\"chunk_id\"]: json.loads(l)[\"text\"] for l in open(chunks_path, encoding=\"utf-8\")}\n",
    "# leaf ìš”ì•½\n",
    "leaf_summary = {json.loads(l)[\"chunk_id\"]: json.loads(l)[\"summary\"] for l in open(summ_path, encoding=\"utf-8\")}\n",
    "# ë…¸ë“œ ìš”ì•½\n",
    "nodes = [json.loads(l) for l in open(nodes_path, encoding=\"utf-8\")]\n",
    "node_info = {nd[\"node_id\"]: (nd[\"level\"], nd[\"children\"], nd[\"summary\"]) for nd in nodes}\n",
    "\n",
    "# ê²€ìƒ‰ ëŒ€ìƒ(ë…¸ë“œ + ë¦¬í”„)\n",
    "corpus_ids, corpus_txt = [], []\n",
    "for nid, (_,_,summ) in node_info.items():\n",
    "    corpus_ids.append(nid); corpus_txt.append(summ)\n",
    "for cid, summ in leaf_summary.items():\n",
    "    corpus_ids.append(cid); corpus_txt.append(summ)\n",
    "\n",
    "print(\"âœ… ë¡œë“œ ì™„ë£Œ:\", len(corpus_ids), \"ê°œ ìš”ì•½\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d01991b-2ae5-40d3-b73e-1068018cada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3. ê°„ë‹¨ ì„ë² ë”© ì¸ë±ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb393030-8bb4-40f3-be5e-2a6c7f0b2dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4914d42a8d884af9a9b9cef769befc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa523a67b9649e98bb914cc4a24fc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b381e86a9254d628836f41f0631bdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21535bf1971b4dffa830ae3d41e5d87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2830414e7cb14f7593e2f0c74b20b5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9955db00604f189a5abd8f2d5f3282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdd35e4093c43b2b5a342dc888c663b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cab67c40a84aacbb69c42d268495c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6130e9f11444a8165736e34d287a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca61225d0c514a88959643f29b58b610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b64d5639684b018e9bff2dee8046e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeaa705335544b39237f4ab451a6f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT ì‚¬ìš©\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    emb_mat = model.encode(corpus_txt, normalize_embeddings=True, show_progress_bar=True)\n",
    "    backend = \"sbert\"\n",
    "    print(\"âœ… SBERT ì‚¬ìš©\")\n",
    "except:\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\n",
    "    emb_mat = vect.fit_transform(corpus_txt)\n",
    "    backend = \"tfidf\"\n",
    "    print(\"âœ… TF-IDF ì‚¬ìš©\")\n",
    "\n",
    "def topk_in_corpus(query, k=5):\n",
    "    if backend==\"sbert\":\n",
    "        qv = model.encode([query], normalize_embeddings=True)[0]\n",
    "        sims = emb_mat @ qv\n",
    "    else:\n",
    "        qv = vect.transform([query])\n",
    "        sims = cosine_similarity(emb_mat, qv).ravel()\n",
    "    idx = np.argsort(-sims)[:k]\n",
    "    return [(corpus_ids[i], float(sims[i])) for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f79070d1-1835-464d-975b-60162c84f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4. Retrieval & ë‹µë³€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5056b68-1ffa-406f-8e25-5f401d4368b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ê°„ë‹¨ ì „ì²˜ë¦¬\n",
    "def _clean(s: str) -> str:\n",
    "    s = re.sub(r\"\\bM\\s+r\\.\", \"Mr.\", s)      # \"M r.\" -> \"Mr.\"\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _split_sents(text: str):\n",
    "    # ë”°ì˜´í‘œ/ë§ˆì¹¨í‘œ ê¸°ì¤€ ë¬¸ì¥ ë¶„í• \n",
    "    sents = re.split(r'(?<=[.!?]\")\\s+|(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "def _keywords(q: str):\n",
    "    stop = {\"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"on\",\"at\",\"for\",\"with\",\"is\",\"are\",\"was\",\"were\",\"do\",\"does\",\"did\",\"what\",\"who\",\"where\",\"when\",\"how\",\"why\"}\n",
    "    toks = re.findall(r\"[A-Za-z']+\", q.lower())\n",
    "    return sorted({t for t in toks if t not in stop and len(t) >= 3})\n",
    "\n",
    "def is_chunk_id(x): \n",
    "    return isinstance(x, str) and x.startswith(\"C\")\n",
    "\n",
    "def descend_to_chunks(ids, max_hops=3):\n",
    "    \"\"\"ë…¸ë“œ idë“¤ì—ì„œ ì‹œì‘ â†’ ë¦¬í”„(ì²­í¬)ê¹Œì§€ ë‚´ë ¤ê°€ê¸°\"\"\"\n",
    "    out, frontier = [], list(ids)\n",
    "    for _ in range(max_hops):\n",
    "        nxt = []\n",
    "        for _id in frontier:\n",
    "            if is_chunk_id(_id):\n",
    "                out.append(_id)\n",
    "            elif _id in node_info:\n",
    "                _, children, _ = node_info[_id]\n",
    "                nxt.extend(children)\n",
    "            elif _id in leaf_summary:  # ìš”ì•½ë§Œ ìˆëŠ” ë¦¬í”„ì¼ ìˆ˜ë„ ìˆìŒ\n",
    "                out.append(_id)\n",
    "        frontier = nxt\n",
    "        if not frontier:\n",
    "            break\n",
    "    # ì¤‘ë³µ ì œê±°, ìˆœì„œ ìœ ì§€\n",
    "    seen, uniq = set(), []\n",
    "    for cid in out:\n",
    "        if cid not in seen:\n",
    "            uniq.append(cid); seen.add(cid)\n",
    "    return uniq\n",
    "\n",
    "def raptor_retrieve(query: str, topk_nodes=6, topk_chunks=5):\n",
    "    \"\"\"ìƒìœ„ í›„ë³´ ë…¸ë“œ/ë¦¬í”„ â†’ ë¦¬í”„ë¡œ ë‚´ë ¤ê°€ì„œ ì²­í¬ ì¬ë­í‚¹\"\"\"\n",
    "    hits = topk_in_corpus(query, k=topk_nodes)              # [(id, score), ...]\n",
    "    hit_ids = [hid for hid, _ in hits]\n",
    "\n",
    "    candidate_chunks = descend_to_chunks(hit_ids, max_hops=3)\n",
    "    if not candidate_chunks:\n",
    "        candidate_chunks = [hid for hid, _ in hits if is_chunk_id(hid)]\n",
    "\n",
    "    # ì²­í¬ í…ìŠ¤íŠ¸ ê¸°ì¤€ ì¬ë­í‚¹ (TF-IDF)\n",
    "    cids, ctexts = [], []\n",
    "    for cid in candidate_chunks:\n",
    "        if cid in chunk_text:\n",
    "            cids.append(cid)\n",
    "            ctexts.append(_clean(chunk_text[cid]))\n",
    "    if not ctexts:\n",
    "        return {\"nodes\": hits, \"chunks\": []}\n",
    "\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_features=50000)\n",
    "    M    = vect.fit_transform(ctexts)\n",
    "    qv   = vect.transform([query])\n",
    "    sims = cosine_similarity(M, qv).ravel()\n",
    "    order = np.argsort(-sims)[:topk_chunks]\n",
    "    top_chunks = [(cids[i], float(sims[i])) for i in order]\n",
    "    return {\"nodes\": hits, \"chunks\": top_chunks}\n",
    "\n",
    "def answer_query(query: str, topk_nodes=6, topk_chunks=5, sent_per_chunk=2, max_chars=400):\n",
    "    \"\"\"dict ë°˜í™˜: {'retrieval':..., 'answer': str} â€” ì§ˆë¬¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ìœ„ì£¼ë¡œ ì¶”ì¶œ\"\"\"\n",
    "    res = raptor_retrieve(query, topk_nodes, topk_chunks)\n",
    "    kws = _keywords(query)\n",
    "    snippets = []\n",
    "\n",
    "    for cid, _ in res.get(\"chunks\", []):\n",
    "        text = _clean(chunk_text.get(cid, \"\"))\n",
    "        if not text:\n",
    "            continue\n",
    "        sents = _split_sents(text)\n",
    "        if not sents:\n",
    "            continue\n",
    "\n",
    "        # 1) í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥ ë¨¼ì € í•„í„°\n",
    "        key_sents = [s for s in sents if any(k in s.lower() for k in kws)] or sents\n",
    "\n",
    "        # 2) í•„í„°ëœ ë¬¸ì¥ë“¤ë§Œ ì¬ë­í‚¹(TF-IDF) í›„ ìƒìœ„ nê°œ ì„ íƒ\n",
    "        vect = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_features=20000)\n",
    "        M = vect.fit_transform(key_sents)\n",
    "        q = vect.transform([query])\n",
    "        sims = cosine_similarity(M, q).ravel()\n",
    "        idxs = np.argsort(-sims)[:sent_per_chunk]\n",
    "\n",
    "        picked = \" \".join(key_sents[i] for i in idxs)\n",
    "        snippets.append(f\"[{cid}] {picked}\")\n",
    "\n",
    "        if len(\" \".join(snippets)) > max_chars:\n",
    "            break\n",
    "\n",
    "    # ë³´ì¡°: ì•„ë¬´ ê²ƒë„ ëª» ë½‘ì•˜ìœ¼ë©´ leaf summaryë¥¼ 1~2ê°œ ì¶”ê°€\n",
    "    if not snippets:\n",
    "        for hid, _ in topk_in_corpus(query, k=6):\n",
    "            if is_chunk_id(hid) and hid in leaf_summary:\n",
    "                snippets.append(f\"[{hid}-summary] {leaf_summary[hid]}\")\n",
    "                if len(snippets) >= 2:\n",
    "                    break\n",
    "\n",
    "    answer = \" \".join(snippets).strip()\n",
    "    return {\"retrieval\": res, \"answer\": answer if answer else \"(no matching evidence)\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff94ceb-9b00-4346-bb86-605ae3e5dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5. í…ŒìŠ¤íŠ¸!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32416217-b50b-40f0-96aa-4a0f1e527521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_answer(query, topk_nodes=6, topk_chunks=5, sent_per_chunk=2):\n",
    "    out = answer_query(query, topk_nodes, topk_chunks, sent_per_chunk)\n",
    "    print(\"ğŸ” Q:\", query)\n",
    "    print(\"ğŸ’¬ A:\", out[\"answer\"])\n",
    "    if out[\"retrieval\"][\"chunks\"]:\n",
    "        print(\"ğŸ“‘ Evidence:\", [cid for cid,_ in out[\"retrieval\"][\"chunks\"]])\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f6798b-927e-4800-804c-84ea59531f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Q: Who is Harry Potter's best friend?\n",
      "ğŸ’¬ A: [C0003] He was sure there were lots of people called Potter who had a son called Harry. Come to think of it, he wasnâ€™t even sure his nephew was called Harry. [C0001] Potter was Mrs. None of them noticed a large, tawny owl flutter past the window. [C0004] Rejoice, for You-Know-Who has gone at last! Dursley; she always got so upset at any mention of her sister. [C0002] Dursley couldnâ€™t bear people who dressed in funny clothes â€” the getups you saw on young people! Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls.\n",
      "ğŸ“‘ Evidence: ['C0003', 'C0001', 'C0004', 'C0002', 'C0005']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ” Q: What strange events happened on Privet Drive?\n",
      "ğŸ’¬ A: [C0002] There was a tabby cat standing on the corner of Privet Drive, but there wasnâ€™t a map in sight. It was now reading the sign that said Privet Drive â€” no, looking at the sign; cats couldnâ€™t read maps or signs. [C0001] Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country.\n",
      "ğŸ“‘ Evidence: ['C0002', 'C0001', 'C0003', 'C0004', 'C0005']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pretty_answer(\"Who is Harry Potter's best friend?\")\n",
    "pretty_answer(\"What strange events happened on Privet Drive?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da474e-8c07-467d-973f-85fbc9a6c3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
