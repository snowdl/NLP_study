{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1d3898d-157c-4812-b91c-3eed34ada074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# B0. Set base path\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "BASE = Path.home() / \"NLP_study/09_Mini_Project/13_RAPTOR\"\n",
    "CHUNKS_PATH = BASE / \"outputs/chunks.jsonl\"\n",
    "#print(\" CHUNKS_PATH:\", CHUNKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af9b3b7-2cb9-4a7a-98c3-66173370f33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " exists? True\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# B1. Check if file exists\n",
    "# ============================================\n",
    "import os\n",
    "print(\" exists?\", os.path.exists(CHUNKS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "881f875f-97f9-4790-841c-ffc4b2f50bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#chunks: 227\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# B2. Read JSONL file\n",
    "# Each line is a JSON object (one chunk)\n",
    "# ============================================\n",
    "import json\n",
    "\n",
    "def read_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "chunks_raw = read_jsonl(CHUNKS_PATH)\n",
    "print(\"#chunks:\", len(chunks_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc02fef6-efa5-4b6c-90de-c9313200a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# B3. Helper functions to extract ID and text\n",
    "# Handles different possible key names\n",
    "# ============================================\n",
    "def get_chunk_id(rec):\n",
    "    for k in (\"id\", \"chunk_id\", \"cid\"):\n",
    "        if k in rec:\n",
    "            return rec[k]\n",
    "    return None\n",
    "\n",
    "def get_chunk_text(rec):\n",
    "    for k in (\"text\", \"content\", \"body\"):\n",
    "        if k in rec:\n",
    "            return rec[k]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5189d56-ffb3-4b55-9738-db7c0e94b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Raw: {'chunk_id': 'C0001', 'text': 'Mr. and Mrs. Dursley, of number four, Privet Drive...'}\n",
      "   → get_chunk_id: C0001\n",
      "   → get_chunk_text: Mr. and Mrs. Dursley, of number four, Privet Drive...\n",
      "------------------------------------------------------------\n",
      "2. Raw: {'cid': 'C0002', 'content': 'Dudley was now having a tantrum and throwing his cereal...'}\n",
      "   → get_chunk_id: C0002\n",
      "   → get_chunk_text: Dudley was now having a tantrum and throwing his cereal...\n",
      "------------------------------------------------------------\n",
      "3. Raw: {'id': 'C0003', 'body': 'There was a tabby cat standing on the corner of Privet Drive...'}\n",
      "   → get_chunk_id: C0003\n",
      "   → get_chunk_text: There was a tabby cat standing on the corner of Privet Drive...\n",
      "------------------------------------------------------------\n",
      "4. Raw: {'name': 'C0004', 'desc': 'This one has no valid keys'}\n",
      "   → get_chunk_id: None\n",
      "   → get_chunk_text: \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample raw chunks with different key names\n",
    "sample_chunks = [\n",
    "    {\"chunk_id\": \"C0001\", \"text\": \"Mr. and Mrs. Dursley, of number four, Privet Drive...\"},\n",
    "    {\"cid\": \"C0002\", \"content\": \"Dudley was now having a tantrum and throwing his cereal...\"},\n",
    "    {\"id\": \"C0003\", \"body\": \"There was a tabby cat standing on the corner of Privet Drive...\"},\n",
    "    {\"name\": \"C0004\", \"desc\": \"This one has no valid keys\"}  # should fail gracefully\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "for i, rec in enumerate(sample_chunks, 1):\n",
    "    cid = get_chunk_id(rec)\n",
    "    txt = get_chunk_text(rec)\n",
    "    print(f\"{i}. Raw: {rec}\")\n",
    "    print(f\"   → get_chunk_id: {cid}\")\n",
    "    print(f\"   → get_chunk_text: {txt[:60]}{'...' if len(txt) > 60 else ''}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10a0d733-b738-428a-82c1-3a9cf7aa376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# B4. Tokenizer\n",
    "# - Lowercase\n",
    "# - Extract alphanumeric tokens\n",
    "# ============================================\n",
    "import re\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return set(re.findall(r\"[a-z0-9]+\", s.lower()))\n",
    "\n",
    "# Optional: Unicode version (for Korean, etc.)\n",
    "# def tokenize(s: str):\n",
    "#     return set(re.findall(r\"[0-9\\w]+\", s.lower(), flags=re.UNICODE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18bd2272-bb18-4eb7-9752-21ec24592f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# B5. Scoring function\n",
    "# - overlap: number of shared tokens\n",
    "# - normalization: divide by log of text length\n",
    "#   (to avoid bias towards long texts)\n",
    "# ============================================\n",
    "import math\n",
    "\n",
    "def simple_score(query: str, text: str) -> float:\n",
    "    q = tokenize(query)\n",
    "    t = tokenize(text)\n",
    "    if not t:\n",
    "        return 0.0\n",
    "    overlap = len(q & t)\n",
    "    return overlap / (1.0 + math.log(1 + len(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3538d390-37d7-441f-9576-e8cc64f66822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# B6. Search function\n",
    "# - Compute score for each chunk\n",
    "# - Return top-k results sorted by score\n",
    "# ============================================\n",
    "def search_chunks_simple(query: str, chunks_raw, topk=5, min_score=0.0):\n",
    "    scored = []\n",
    "    for rec in chunks_raw:\n",
    "        cid = get_chunk_id(rec)\n",
    "        text = get_chunk_text(rec)\n",
    "        if not cid or not text:\n",
    "            continue\n",
    "        s = simple_score(query, text)\n",
    "        if s > min_score:\n",
    "            scored.append((s, cid, text))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return scored[:topk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e611a62-0916-431e-91f5-6831997bb0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. C0001 | score=0.7958\n",
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d exp...\n",
      "------------------------------------------------------------\n",
      "2. C0221 | score=0.6495\n",
      "I was unfortunate enough in my youth to come across a vomit flavored one, and since then I’m afraid I’ve rather lost my liking for them — but I think I’ll be sa...\n",
      "------------------------------------------------------------\n",
      "3. C0030 | score=0.6453\n",
      "After a minute of confused fighting, in which everyone got hit a lot by the Smelting stick, Uncle Vernon straightened up, gasping for breath, with Harry’s lette...\n",
      "------------------------------------------------------------\n",
      "4. C0026 | score=0.6442\n",
      "Uncle Vernon opened his newspaper as usual and Dudley banged his Smelting stick, which he carried everywhere, on the table. They heard the click of the mail slo...\n",
      "------------------------------------------------------------\n",
      "5. C0023 | score=0.6414\n",
      "He managed to say, “Go — cupboard — stay — no meals,” before he collapsed into a chair, and Aunt Petunia had to run and get him a large brandy. Harry lay in his...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# B7. Run search and display results\n",
    "# ============================================\n",
    "query = \"What strange events happened on Privet Drive?\"\n",
    "hits2 = search_chunks_simple(query, chunks_raw, topk=5)\n",
    "\n",
    "for rank, (score, cid, text) in enumerate(hits2, 1):\n",
    "    preview = text[:160] + (\"...\" if len(text) > 160 else \"\")\n",
    "    print(f\"{rank}. {cid} | score={score:.4f}\")\n",
    "    print(preview)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17c9b042-b333-4d63-b557-36327eeb2686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Answer:\n",
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense. Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spy\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# B8. Simple answer generator\n",
    "# - Concatenate top chunk texts\n",
    "# - Limit output to max_chars\n",
    "# ============================================\n",
    "def simple_answer(results, max_chars=600):\n",
    "    buf, used = [], 0\n",
    "    for score, cid, txt in results:\n",
    "        if used >= max_chars:\n",
    "            break\n",
    "        take = max_chars - used\n",
    "        snippet = txt[:take]\n",
    "        buf.append(snippet)\n",
    "        used += len(snippet)\n",
    "    return \" \".join(buf) if buf else \"No evidence found.\"\n",
    "\n",
    "print(\"\\n💬 Answer:\")\n",
    "print(simple_answer(hits2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22bca222-8db4-4841-ab1d-981c818e774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {\"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"for\",\"at\",\"by\",\"with\",\"is\",\"are\",\"was\",\"were\"}\n",
    "\n",
    "def tokenize_nostop(s: str):\n",
    "    toks = tokenize(s)\n",
    "    return toks - STOPWORDS\n",
    "\n",
    "def simple_score_nostop(query: str, text: str) -> float:\n",
    "    q = tokenize_nostop(query)\n",
    "    t = tokenize_nostop(text)\n",
    "    if not t:\n",
    "        return 0.0\n",
    "    overlap = len(q & t)\n",
    "    return overlap / (1.0 + math.log(1 + len(t)))\n",
    "\n",
    "# 쓰려면 simple_score를 simple_score_nostop으로 바꾸면 됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5ac95c-e35d-4a2e-b5fa-ee44dbb5f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_EXPAND = {\n",
    "    \"strange\": {\"odd\",\"unusual\",\"mysterious\"},\n",
    "    \"events\": {\"incidents\",\"happenings\",\"occurrences\"},\n",
    "}\n",
    "\n",
    "def expand_tokens(tokens):\n",
    "    out = set(tokens)\n",
    "    for t in list(tokens):\n",
    "        out |= QUERY_EXPAND.get(t, set())\n",
    "    return out\n",
    "\n",
    "def simple_score_expanded(query: str, text: str) -> float:\n",
    "    q = expand_tokens(tokenize(query))\n",
    "    t = tokenize(text)\n",
    "    if not t:\n",
    "        return 0.0\n",
    "    overlap = len(q & t)\n",
    "    return overlap / (1.0 + math.log(1 + len(t)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90dec61-5263-4cb1-be80-0af2bd1366ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
