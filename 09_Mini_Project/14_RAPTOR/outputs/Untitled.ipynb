{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4a7b74-c484-4efc-80fb-04ec00e136e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "BASE = Path.home() / \"gitclone/NLP_study/09_Mini_Project/13_RAPTOR\"\n",
    "\n",
    "# ì´ì œ outputs í•œ ë²ˆë§Œ ë¶™ì´ì„¸ìš”\n",
    "chunks_raw = read_jsonl(BASE / \"outputs/chunks.jsonl\")\n",
    "nodes_raw  = read_jsonl(BASE / \"outputs/tree_nodes.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45af3fcf-866a-4155-af69-7f6409a7e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, itertools\n",
    "from collections import Counter\n",
    "\n",
    "def read_jsonl(path):\n",
    "    out = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def sniff_keys(recs, n=50):\n",
    "    \"\"\"ì•žë¶€ë¶„ nê°œ ë ˆì½”ë“œì˜ í‚¤ë¥¼ ì§‘ê³„í•´ì„œ ì–´ë–¤ í‚¤ë“¤ì´ ìžˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.\"\"\"\n",
    "    cnt = Counter()\n",
    "    for r in recs[:n]:\n",
    "        cnt.update(r.keys())\n",
    "    return cnt\n",
    "\n",
    "def pick_first_key(d, candidates, default=None):\n",
    "    \"\"\"ì‚¬ì „ dì—ì„œ í›„ë³´ í‚¤ë“¤ ì¤‘ ìµœì´ˆë¡œ ì¡´ìž¬í•˜ëŠ” í‚¤ë¥¼ ê³¨ë¼ ë°˜í™˜\"\"\"\n",
    "    for k in candidates:\n",
    "        if k in d:\n",
    "            return k\n",
    "    return default\n",
    "\n",
    "def standardize_nodes(nodes):\n",
    "    \"\"\"\n",
    "    ë‹¤ì–‘í•œ ìŠ¤í‚¤ë§ˆë¥¼ í—ˆìš©:\n",
    "      - id: ['id','node_id','nid','uid','name']\n",
    "      - summary: ['summary','node_summary','text','desc','description','title']\n",
    "      - children: ['children','child_ids','kids','edges','links']\n",
    "    ë°˜í™˜: [{id, summary, children(list of ids)}...]\n",
    "    \"\"\"\n",
    "    std = []\n",
    "    for n in nodes:\n",
    "        id_key = pick_first_key(n, ['id','node_id','nid','uid','name'])\n",
    "        sum_key = pick_first_key(n, ['summary','node_summary','text','desc','description','title'])\n",
    "        ch_key = pick_first_key(n, ['children','child_ids','kids','edges','links'])\n",
    "\n",
    "        node_id = n.get(id_key, None)\n",
    "        summary = n.get(sum_key, \"\")\n",
    "        children = n.get(ch_key, [])\n",
    "\n",
    "        # childrenì´ ë¬¸ìžì—´ í•˜ë‚˜ë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° ë³´ì •\n",
    "        if isinstance(children, str):\n",
    "            children = [children]\n",
    "        # childrenì´ Noneì¸ ê²½ìš° ë³´ì •\n",
    "        if children is None:\n",
    "            children = []\n",
    "\n",
    "        std.append({\n",
    "            \"id\": node_id,\n",
    "            \"summary\": summary if isinstance(summary, str) else str(summary),\n",
    "            \"children\": children\n",
    "        })\n",
    "    # idê°€ ì—†ëŠ” ë ˆì½”ë“œëŠ” ì œê±°\n",
    "    std = [x for x in std if x[\"id\"]]\n",
    "    return std\n",
    "\n",
    "def standardize_chunks(chunks):\n",
    "    \"\"\"\n",
    "    ë‹¤ì–‘í•œ ìŠ¤í‚¤ë§ˆ í—ˆìš©:\n",
    "      - id: ['id','chunk_id','cid','name']\n",
    "      - text: ['text','content','raw','body']\n",
    "    ë°˜í™˜: {chunk_id: text}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for c in chunks:\n",
    "        id_key = pick_first_key(c, ['id','chunk_id','cid','name'])\n",
    "        txt_key = pick_first_key(c, ['text','content','raw','body'])\n",
    "\n",
    "        cid = c.get(id_key, None)\n",
    "        txt = c.get(txt_key, \"\")\n",
    "        if cid:\n",
    "            out[cid] = txt if isinstance(txt, str) else str(txt)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f334e311-00b6-4689-8f18-7d349e2aa7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== chunks.jsonl key freq (head) ==\n",
      "Counter({'chunk_id': 20, 'text': 20, 'tokens': 20})\n",
      "\n",
      "== tree_nodes.jsonl key freq (head) ==\n",
      "Counter({'node_id': 6, 'level': 6, 'children': 6, 'summary': 6})\n",
      "\n",
      "[Sample node] -> {'id': 'L1_N0001', 'summary': 'This is the story of the Dursleys and the Potters.', 'children': ['C0001', 'C0002']}\n",
      "[Sample chunk text exists?] True\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ì½ê¸°\n",
    "chunks_raw = read_jsonl(BASE / \"outputs/chunks.jsonl\")\n",
    "nodes_raw  = read_jsonl(BASE / \"outputs/tree_nodes.jsonl\")\n",
    "\n",
    "# ìŠ¤í‚¤ë§ˆ íŒŒì•…(ì°¸ê³  ì¶œë ¥)\n",
    "print(\"== chunks.jsonl key freq (head) ==\")\n",
    "print(sniff_keys(chunks_raw, n=20))\n",
    "print(\"\\n== tree_nodes.jsonl key freq (head) ==\")\n",
    "print(sniff_keys(nodes_raw, n=20))\n",
    "\n",
    "# í‘œì¤€í™”\n",
    "nodes = standardize_nodes(nodes_raw)\n",
    "chunk_map = standardize_chunks(chunks_raw)\n",
    "\n",
    "# í‘œë³¸ í™•ì¸\n",
    "print(\"\\n[Sample node] ->\", nodes[0] if nodes else \"NO NODES\")\n",
    "some_chunk_id = next((cid for cid in chunk_map.keys() if cid.startswith(\"C\")), None)\n",
    "print(\"[Sample chunk text exists?]\", some_chunk_id is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431ccc24-da7a-4616-a748-f5a30324cb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Node: L1_N0003\n",
      "ðŸ“ Summary: Dudley and Petunia Dursley had a strange day. \n",
      "ðŸ”— Chunks: ['C0005']\n",
      "   [1] When Dudley had been put to bed, he went into the living room in time to catch the last report on the evening news:\n",
      "\n",
      "â€œAnd finally, bird-watchers everywhere have...\n",
      "------------------------------------------------------------\n",
      "ðŸ“Œ Node: L3_N0001\n",
      "ðŸ“ Summary: All images are copyrighted. \n",
      "ðŸ”— Chunks: []\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def raptor_search(query, nodes, chunk_map, topk_nodes=2, max_chunks_per_node=3):\n",
    "    # 1) ë…¸ë“œ ìš”ì•½ ê¸°ë°˜ ê²€ìƒ‰\n",
    "    ids = [n[\"id\"] for n in nodes]\n",
    "    sums = [n[\"summary\"] if n[\"summary\"] else \"\" for n in nodes]\n",
    "\n",
    "    # ìš”ì•½ ì „ë¶€ ë¹„ì–´ìžˆë‹¤ë©´ ì•ˆì „íƒˆì¶œ\n",
    "    if not any(sums):\n",
    "        return []\n",
    "\n",
    "    vec = TfidfVectorizer().fit(sums + [query])\n",
    "    qv = vec.transform([query])\n",
    "    nv = vec.transform(sums)\n",
    "    sims = cosine_similarity(qv, nv)[0]\n",
    "\n",
    "    top_idx = sims.argsort()[-topk_nodes:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        node = nodes[idx]\n",
    "        # 2) ìžì‹ ì¤‘ ì²­í¬IDë§Œ ì¶”ì¶œ (Cë¡œ ì‹œìž‘í•œë‹¤ê³  ê°€ì •)\n",
    "        child_chunk_ids = [c for c in node[\"children\"] if isinstance(c, str) and c.startswith(\"C\")]\n",
    "        # 3) í•´ë‹¹ ì²­í¬ í…ìŠ¤íŠ¸ ëª¨ìœ¼ê¸°\n",
    "        texts = [chunk_map[cid] for cid in child_chunk_ids if cid in chunk_map]\n",
    "        # ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ\n",
    "        texts = texts[:max_chunks_per_node]\n",
    "        results.append({\n",
    "            \"node_id\": node[\"id\"],\n",
    "            \"node_summary\": node[\"summary\"],\n",
    "            \"linked_chunk_ids\": child_chunk_ids[:max_chunks_per_node],\n",
    "            \"chunk_texts\": texts\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "query = \"What strange events happened on Privet Drive?\"\n",
    "res = raptor_search(query, nodes, chunk_map, topk_nodes=2, max_chunks_per_node=2)\n",
    "\n",
    "for r in res:\n",
    "    print(\"ðŸ“Œ Node:\", r[\"node_id\"])\n",
    "    print(\"ðŸ“ Summary:\", r[\"node_summary\"][:180], \"...\" if len(r[\"node_summary\"])>180 else \"\")\n",
    "    print(\"ðŸ”— Chunks:\", r[\"linked_chunk_ids\"])\n",
    "    for i, t in enumerate(r[\"chunk_texts\"], 1):\n",
    "        print(f\"   [{i}] {t[:160]}{'...' if len(t)>160 else ''}\")\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1dffc-bdfa-4110-8d88-4c1917fd7b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
