{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4a7b74-c484-4efc-80fb-04ec00e136e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "BASE = Path.home() / \"gitclone/NLP_study/09_Mini_Project/13_RAPTOR\"\n",
    "\n",
    "# 이제 outputs 한 번만 붙이세요\n",
    "chunks_raw = read_jsonl(BASE / \"outputs/chunks.jsonl\")\n",
    "nodes_raw  = read_jsonl(BASE / \"outputs/tree_nodes.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45af3fcf-866a-4155-af69-7f6409a7e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, itertools\n",
    "from collections import Counter\n",
    "\n",
    "def read_jsonl(path):\n",
    "    out = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def sniff_keys(recs, n=50):\n",
    "    \"\"\"앞부분 n개 레코드의 키를 집계해서 어떤 키들이 있는지 보여줍니다.\"\"\"\n",
    "    cnt = Counter()\n",
    "    for r in recs[:n]:\n",
    "        cnt.update(r.keys())\n",
    "    return cnt\n",
    "\n",
    "def pick_first_key(d, candidates, default=None):\n",
    "    \"\"\"사전 d에서 후보 키들 중 최초로 존재하는 키를 골라 반환\"\"\"\n",
    "    for k in candidates:\n",
    "        if k in d:\n",
    "            return k\n",
    "    return default\n",
    "\n",
    "def standardize_nodes(nodes):\n",
    "    \"\"\"\n",
    "    다양한 스키마를 허용:\n",
    "      - id: ['id','node_id','nid','uid','name']\n",
    "      - summary: ['summary','node_summary','text','desc','description','title']\n",
    "      - children: ['children','child_ids','kids','edges','links']\n",
    "    반환: [{id, summary, children(list of ids)}...]\n",
    "    \"\"\"\n",
    "    std = []\n",
    "    for n in nodes:\n",
    "        id_key = pick_first_key(n, ['id','node_id','nid','uid','name'])\n",
    "        sum_key = pick_first_key(n, ['summary','node_summary','text','desc','description','title'])\n",
    "        ch_key = pick_first_key(n, ['children','child_ids','kids','edges','links'])\n",
    "\n",
    "        node_id = n.get(id_key, None)\n",
    "        summary = n.get(sum_key, \"\")\n",
    "        children = n.get(ch_key, [])\n",
    "\n",
    "        # children이 문자열 하나로 들어오는 경우 보정\n",
    "        if isinstance(children, str):\n",
    "            children = [children]\n",
    "        # children이 None인 경우 보정\n",
    "        if children is None:\n",
    "            children = []\n",
    "\n",
    "        std.append({\n",
    "            \"id\": node_id,\n",
    "            \"summary\": summary if isinstance(summary, str) else str(summary),\n",
    "            \"children\": children\n",
    "        })\n",
    "    # id가 없는 레코드는 제거\n",
    "    std = [x for x in std if x[\"id\"]]\n",
    "    return std\n",
    "\n",
    "def standardize_chunks(chunks):\n",
    "    \"\"\"\n",
    "    다양한 스키마 허용:\n",
    "      - id: ['id','chunk_id','cid','name']\n",
    "      - text: ['text','content','raw','body']\n",
    "    반환: {chunk_id: text}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for c in chunks:\n",
    "        id_key = pick_first_key(c, ['id','chunk_id','cid','name'])\n",
    "        txt_key = pick_first_key(c, ['text','content','raw','body'])\n",
    "\n",
    "        cid = c.get(id_key, None)\n",
    "        txt = c.get(txt_key, \"\")\n",
    "        if cid:\n",
    "            out[cid] = txt if isinstance(txt, str) else str(txt)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f334e311-00b6-4689-8f18-7d349e2aa7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== chunks.jsonl key freq (head) ==\n",
      "Counter({'chunk_id': 20, 'text': 20, 'tokens': 20})\n",
      "\n",
      "== tree_nodes.jsonl key freq (head) ==\n",
      "Counter({'node_id': 6, 'level': 6, 'children': 6, 'summary': 6})\n",
      "\n",
      "[Sample node] -> {'id': 'L1_N0001', 'summary': 'This is the story of the Dursleys and the Potters.', 'children': ['C0001', 'C0002']}\n",
      "[Sample chunk text exists?] True\n"
     ]
    }
   ],
   "source": [
    "# 파일 읽기\n",
    "chunks_raw = read_jsonl(BASE / \"outputs/chunks.jsonl\")\n",
    "nodes_raw  = read_jsonl(BASE / \"outputs/tree_nodes.jsonl\")\n",
    "\n",
    "# 스키마 파악(참고 출력)\n",
    "print(\"== chunks.jsonl key freq (head) ==\")\n",
    "print(sniff_keys(chunks_raw, n=20))\n",
    "print(\"\\n== tree_nodes.jsonl key freq (head) ==\")\n",
    "print(sniff_keys(nodes_raw, n=20))\n",
    "\n",
    "# 표준화\n",
    "nodes = standardize_nodes(nodes_raw)\n",
    "chunk_map = standardize_chunks(chunks_raw)\n",
    "\n",
    "# 표본 확인\n",
    "print(\"\\n[Sample node] ->\", nodes[0] if nodes else \"NO NODES\")\n",
    "some_chunk_id = next((cid for cid in chunk_map.keys() if cid.startswith(\"C\")), None)\n",
    "print(\"[Sample chunk text exists?]\", some_chunk_id is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431ccc24-da7a-4616-a748-f5a30324cb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Node: L1_N0003\n",
      "📝 Summary: Dudley and Petunia Dursley had a strange day. \n",
      "🔗 Chunks: ['C0005']\n",
      "   [1] When Dudley had been put to bed, he went into the living room in time to catch the last report on the evening news:\n",
      "\n",
      "“And finally, bird-watchers everywhere have...\n",
      "------------------------------------------------------------\n",
      "📌 Node: L3_N0001\n",
      "📝 Summary: All images are copyrighted. \n",
      "🔗 Chunks: []\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def raptor_search(query, nodes, chunk_map, topk_nodes=2, max_chunks_per_node=3):\n",
    "    # 1) 노드 요약 기반 검색\n",
    "    ids = [n[\"id\"] for n in nodes]\n",
    "    sums = [n[\"summary\"] if n[\"summary\"] else \"\" for n in nodes]\n",
    "\n",
    "    # 요약 전부 비어있다면 안전탈출\n",
    "    if not any(sums):\n",
    "        return []\n",
    "\n",
    "    vec = TfidfVectorizer().fit(sums + [query])\n",
    "    qv = vec.transform([query])\n",
    "    nv = vec.transform(sums)\n",
    "    sims = cosine_similarity(qv, nv)[0]\n",
    "\n",
    "    top_idx = sims.argsort()[-topk_nodes:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        node = nodes[idx]\n",
    "        # 2) 자식 중 청크ID만 추출 (C로 시작한다고 가정)\n",
    "        child_chunk_ids = [c for c in node[\"children\"] if isinstance(c, str) and c.startswith(\"C\")]\n",
    "        # 3) 해당 청크 텍스트 모으기\n",
    "        texts = [chunk_map[cid] for cid in child_chunk_ids if cid in chunk_map]\n",
    "        # 너무 길면 일부만\n",
    "        texts = texts[:max_chunks_per_node]\n",
    "        results.append({\n",
    "            \"node_id\": node[\"id\"],\n",
    "            \"node_summary\": node[\"summary\"],\n",
    "            \"linked_chunk_ids\": child_chunk_ids[:max_chunks_per_node],\n",
    "            \"chunk_texts\": texts\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 실행 예시\n",
    "query = \"What strange events happened on Privet Drive?\"\n",
    "res = raptor_search(query, nodes, chunk_map, topk_nodes=2, max_chunks_per_node=2)\n",
    "\n",
    "for r in res:\n",
    "    print(\"📌 Node:\", r[\"node_id\"])\n",
    "    print(\"📝 Summary:\", r[\"node_summary\"][:180], \"...\" if len(r[\"node_summary\"])>180 else \"\")\n",
    "    print(\"🔗 Chunks:\", r[\"linked_chunk_ids\"])\n",
    "    for i, t in enumerate(r[\"chunk_texts\"], 1):\n",
    "        print(f\"   [{i}] {t[:160]}{'...' if len(t)>160 else ''}\")\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1dffc-bdfa-4110-8d88-4c1917fd7b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
