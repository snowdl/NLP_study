{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd37d194-9aab-4f76-96cd-7f9d18c3ac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n BASE = 13_RAPTOR/\\n ├── DATA   → 13_RAPTOR/data\\n │    └── harry_potter.txt   (원본 문서)\\n │\\n ├── SRC    → 13_RAPTOR/src\\n │    ├── chunking.py\\n │    ├── summarize_chunks.py\\n │    └── build_tree.py      (코드 모듈들)\\n │\\n └── OUT    → 13_RAPTOR/outputs\\n      ├── chunks.jsonl            (Step 1 결과: 문서 → 청크)\\n      ├── chunk_summaries.jsonl   (Step 2 결과: 청크 → 요약)\\n      ├── tree_nodes.jsonl        (Step 3 결과: 트리 구조 노드)\\n      └── tree_root.json          (Step 3 결과: 최종 루트 요약)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " BASE = 13_RAPTOR/\n",
    " ├── DATA   → 13_RAPTOR/data\n",
    " │    └── harry_potter.txt   (원본 문서)\n",
    " │\n",
    " ├── SRC    → 13_RAPTOR/src\n",
    " │    ├── chunking.py\n",
    " │    ├── summarize_chunks.py\n",
    " │    └── build_tree.py      (코드 모듈들)\n",
    " │\n",
    " └── OUT    → 13_RAPTOR/outputs\n",
    "      ├── chunks.jsonl            (Step 1 결과: 문서 → 청크)\n",
    "      ├── chunk_summaries.jsonl   (Step 2 결과: 청크 → 요약)\n",
    "      ├── tree_nodes.jsonl        (Step 3 결과: 트리 구조 노드)\n",
    "      └── tree_root.json          (Step 3 결과: 최종 루트 요약)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f426e557-4b80-4320-b910-421b7494e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece tokenizers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fec32f3-cac1-4ba0-a2fa-2fb22be934f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0. Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b595d9-8500-489c-95de-4783f2541a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json              # Provides functions for working with JSON data (load, dump, etc.)\n",
    "from pathlib import Path # Object-oriented approach to handle file system paths\n",
    "from tqdm.auto import tqdm  # Displays progress bars in loops (auto chooses best interface for Jupyter/terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00afaf3c-e89e-407e-835a-e51ebbf3e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path.cwd()                 # Current working directory (e.g., /.../09_Mini_Project/13_RAPTOR)\n",
    "DATA = BASE / \"data\"              # Path to the \"data\" subfolder inside BASE\n",
    "OUT  = BASE / \"outputs\"           # Path to the \"outputs\" subfolder inside BASE\n",
    "SRC  = BASE / \"src\"               # Path to the \"src\" (source code) subfolder inside BASE\n",
    "\n",
    "# Create the \"outputs\" directory if it doesn’t exist yet\n",
    "OUT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8480c67c-ddae-48b6-be1d-5de75a2689f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR\n",
      "DATA: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/data\n",
      "OUT : /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs\n"
     ]
    }
   ],
   "source": [
    "print(\"BASE:\", BASE)\n",
    "print(\"DATA:\", DATA)\n",
    "print(\"OUT :\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66f6a4d-abc7-457e-b5cd-3759ade35795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSentence Splitting Function\\nSimple sentence segmentation function.\\nSplits text into sentences based on punctuation marks (., ?, !).\\n    \\nArgs:\\ntext (str): Input text string to be split.\\nReturns:\\nlist: A list of sentences after splitting.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1. Chunking\n",
    "\"\"\"\n",
    "Sentence Splitting Function\n",
    "Simple sentence segmentation function.\n",
    "Splits text into sentences based on punctuation marks (., ?, !).\n",
    "    \n",
    "Args:\n",
    "text (str): Input text string to be split.\n",
    "Returns:\n",
    "list: A list of sentences after splitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e3f1a8-ac26-48a6-871d-f9b767c64974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# === Sentence Splitting Function ===\n",
    "def split_sentences(text: str):\n",
    "    # Split text whenever a period, question mark, or exclamation mark is followed by whitespace\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    \n",
    "    # Remove any empty strings that may appear after splitting\n",
    "    return [s for s in sents if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f024d8-0603-4ae3-bb4e-0f7bc426b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Hello world! How are you doing today? I'm working on RAPTOR.\n",
      "Split sentences: ['Hello world!', 'How are you doing today?', \"I'm working on RAPTOR.\"]\n"
     ]
    }
   ],
   "source": [
    "# === Example check ===\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Hello world! How are you doing today? I'm working on RAPTOR.\"\n",
    "    print(\"Input text:\", sample_text)\n",
    "    print(\"Split sentences:\", split_sentences(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba9d9d5-77ff-4184-9a5b-e34bec2db318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreate text chunks by concatenating sentences until the max character limit is reached.\\nWhen the current chunk exceeds `max_chars`, a new chunk is started.\\n    \\nArgs:\\nsents (list of str): List of sentences to combine into chunks.\\nmax_chars (int, optional): Maximum number of characters allowed per chunk. Defaults to 2000.\\n    \\nReturns:\\nlist: A list of text chunks, each containing one or more sentences.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create text chunks by concatenating sentences until the max character limit is reached.\n",
    "When the current chunk exceeds `max_chars`, a new chunk is started.\n",
    "    \n",
    "Args:\n",
    "sents (list of str): List of sentences to combine into chunks.\n",
    "max_chars (int, optional): Maximum number of characters allowed per chunk. Defaults to 2000.\n",
    "    \n",
    "Returns:\n",
    "list: A list of text chunks, each containing one or more sentences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "531ee492-14df-454d-b70f-352212702544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Chunk Creation Function ===\n",
    "def chunk_by_sentences(sents, max_chars=2000):\n",
    "    chunks, cur, cur_len = [], [], 0\n",
    "    \n",
    "    # Iterate over sentences\n",
    "    for s in sents:\n",
    "        # If adding the sentence would exceed the limit, finalize current chunk\n",
    "        if cur_len + len(s) > max_chars and cur:\n",
    "            chunks.append(\" \".join(cur))  # Save the current chunk\n",
    "            cur, cur_len = [], 0          # Reset for the next chunk\n",
    "        \n",
    "        # Add the sentence to the current chunk\n",
    "        cur.append(s)\n",
    "        cur_len += len(s) + 1  # +1 accounts for the space between sentences\n",
    "    \n",
    "    # Append any remaining sentences as the last chunk\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1784a4a2-63b5-4cdf-8a1d-43a6bc8ea970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Sentences:\n",
      "1: Hello world!\n",
      "2: How are you doing today?\n",
      "3: I'm working on RAPTOR.\n",
      "4: It helps with document chunking.\n",
      "5: Sometimes the text can be very long, so we need to split it into chunks.\n",
      "6: Each chunk must stay under a certain character limit.\n",
      "\n",
      "🔹 Chunks:\n",
      "1: Hello world! How are you doing today?\n",
      "2: I'm working on RAPTOR.\n",
      "3: It helps with document chunking.\n",
      "4: Sometimes the text can be very long, so we need to split it into chunks.\n",
      "5: Each chunk must stay under a certain character limit.\n"
     ]
    }
   ],
   "source": [
    "# === Example check ===\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = (\n",
    "        \"Hello world! How are you doing today? \"\n",
    "        \"I'm working on RAPTOR. It helps with document chunking. \"\n",
    "        \"Sometimes the text can be very long, so we need to split it into chunks. \"\n",
    "        \"Each chunk must stay under a certain character limit.\"\n",
    "    )\n",
    "    \n",
    "    # Step 1: Sentence splitting\n",
    "    sentences = split_sentences(sample_text)\n",
    "    print(\"🔹 Sentences:\")\n",
    "    for i, s in enumerate(sentences, 1):\n",
    "        print(f\"{i}: {s}\")\n",
    "    \n",
    "    # Step 2: Chunking\n",
    "    chunks = chunk_by_sentences(sentences, max_chars=50)\n",
    "    print(\"\\n🔹 Chunks:\")\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        print(f\"{i}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "013ecc35-428c-4fc7-bf68-bbd719ffb0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Step 1: 문서 로드 → 청크 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e34b4a-f367-4125-81ef-c5c03e7dbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Specify document path (relative to current working directory) ===\n",
    "# The document is located inside ../../11_data/ relative to the current script location\n",
    "DOC_NAME = \"01 Harry Potter and the Sorcerers Stone.txt\"   # Target document name\n",
    "doc_path = Path(\"../../11_data\") / DOC_NAME                # Full relative path to the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9fa8d56-dff3-4ef3-992b-3e3c49cc5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text File  →  Raw String  →  List of Sentences  →  List of Chunks\n",
    "text   = doc_path.read_text(encoding=\"utf-8\")\n",
    "sents  = split_sentences(text)\n",
    "chunks = chunk_by_sentences(sents, max_chars=2000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4fcbb47-9ec1-4511-a064-56a310e7daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sentence Splitting & Chunk Creation ===\n",
    "# Save chunks to chunks.jsonl\n",
    "\n",
    "chunk_path = OUT / \"chunks.jsonl\"   # Output file path\n",
    "\n",
    "# Open the output file in write mode\n",
    "with chunk_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for i, ch in enumerate(chunks, 1):\n",
    "        # Write each chunk as a JSON object in JSONL format (one line per chunk)\n",
    "        f.write(json.dumps({\n",
    "            \"chunk_id\": f\"C{i:04d}\",   # Unique chunk ID, zero-padded (e.g., C0001, C0002, ...)\n",
    "            \"text\": ch,                # The actual chunk text\n",
    "            \"tokens\": len(ch.split())  # Token count (approx. word count using split on whitespace)\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ✅ Each line in chunks.jsonl now represents one chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db159c77-759e-44fe-9a73-9c6fbff2a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ chunks.jsonl saved at: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs/chunks.jsonl\n",
      "Total number of sentences: 5003\n",
      "Total number of chunks: 227\n",
      "First chunk preview:\n",
      " M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense. Mr. Dursley was the director of a fi ...\n"
     ]
    }
   ],
   "source": [
    "# ✅ Print confirmation after saving chunks\n",
    "print(\"✅ chunks.jsonl saved at:\", chunk_path)   # Confirm the output file path\n",
    "print(\"Total number of sentences:\", len(sents)) # Show how many sentences were split\n",
    "print(\"Total number of chunks:\", len(chunks))   # Show how many chunks were created\n",
    "\n",
    "# Preview the first chunk (first 300 characters)\n",
    "print(\"First chunk preview:\\n\", chunks[0][:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dd5c5-c310-466e-a8a1-ad6cbcca5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Transformers to ignore TensorFlow/Flax and stick to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef2c18e3-aba2-4e8c-9e5c-05938904cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PEGASUS loaded on mps\n"
     ]
    }
   ],
   "source": [
    "# --- Run this at the very top of a fresh cell, before importing transformers ---\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"     # tell transformers to ignore TensorFlow\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"   # and Flax/JAX\n",
    "\n",
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "#Prepare PEGASUS Model\n",
    "MODEL_NAME = \"google/pegasus-xsum\"\n",
    "# Pick device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cuda\" if torch.cuda.is_available()\n",
    "                      else \"cpu\")\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "print(f\"✅ PEGASUS loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04d4c1ef-a7d6-4add-94c0-b7cecc314da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSummarize a single text chunk using PEGASUS.\\n    \\n    Args:\\n        text (str): The input text chunk.\\n        max_in (int): Maximum number of input tokens (truncate if longer).\\n        max_out (int): Maximum number of output tokens in the summary.\\n        num_beams (int): Beam search width (higher = better quality but slower).\\n    \\n    Returns:\\n        str: The generated summary string.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summarize a single text chunk using PEGASUS.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text chunk.\n",
    "        max_in (int): Maximum number of input tokens (truncate if longer).\n",
    "        max_out (int): Maximum number of output tokens in the summary.\n",
    "        num_beams (int): Beam search width (higher = better quality but slower).\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated summary string.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "238ff5db-45cf-4478-ad6a-a84292f2c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 — Chunk Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63ece75b-3e14-4f61-81f2-e60c1861212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PEGASUS ready (device=mps)\n"
     ]
    }
   ],
   "source": [
    "# === Step 2 - Chunk Summarization ===\n",
    "def summarize_pegasus(text, max_in=512, max_out=64, num_beams=4):\n",
    "    \"\"\"\n",
    "    Summarize a single text chunk using PEGASUS.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(                 # <- was: tok(...)\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_in\n",
    "    ).to(device)                        # <- was: .to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_out,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)  # <- was: tok.decode(...)\n",
    "\n",
    "print(f\"✅ PEGASUS ready (device={device})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60d02794-5adf-42db-9001-cc7ce0a38c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PEGASUS model is already cached locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers.utils import cached_file\n",
    "import torch\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "\n",
    "# === Check if model is already cached locally ===\n",
    "def have_local_model(model_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the given model is already cached locally,\n",
    "    otherwise False (will need to download).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to locate a config file for the model in the local cache\n",
    "        _ = cached_file(model_name, \"config.json\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "if have_local_model(model_name):\n",
    "    print(\"✅ PEGASUS model is already cached locally.\")\n",
    "    # You can load directly with local_files_only=True if you want:\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name, local_files_only=True)\n",
    "else:\n",
    "    print(\"⬇️ Downloading PEGASUS… (internet required)\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    print(\"✅ Download complete.\")\n",
    "\n",
    "# === Device selection (MPS > CUDA > CPU) ===\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f167e5ef-4943-4cc3-9787-2d9e03a6810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3 - Summarization Functions (PEGASUS) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59eb1ae8-9334-4a67-9681-ba81ed861585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3 - Summarization (PEGASUS) ===\n",
    "def summarize_pegasus(text, tokenizer, model, device=\"cpu\",\n",
    "                      max_in=512, max_out=64, num_beams=4):\n",
    "    \"\"\"Summarize one text chunk with PEGASUS.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=max_in).to(device)\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(**inputs, max_length=max_out,\n",
    "                             num_beams=num_beams,\n",
    "                             no_repeat_ngram_size=3,\n",
    "                             early_stopping=True)\n",
    "    return tokenizer.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0799db46-481b-46f3-8da1-2b8ed11ea343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks, tokenizer, model, device=\"cpu\",\n",
    "                     max_in=512, max_out=64, num_beams=4):\n",
    "    \"\"\"Summarize a list of chunks → list of {chunk_id, summary} dicts.\"\"\"\n",
    "    return [\n",
    "        {\"chunk_id\": f\"C{i+1:04d}\",\n",
    "         \"summary\": summarize_pegasus(ch, tokenizer, model, device,\n",
    "                                      max_in, max_out, num_beams)}\n",
    "        for i, ch in enumerate(chunks)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "488650cd-8b34-4523-8de2-6e3ac32b209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Smoke test ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd34834d-334d-450f-a4bf-d5bc47ecfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12db5ffd-cc32-4f16-85c1-c42ab9db03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4-1) Load only the first 5 chunks for smoke testing ---\n",
    "chunks_path = OUT / \"chunks.jsonl\"              # Input file with all chunks\n",
    "summ_smoke = OUT / \"chunk_summaries_smoke.jsonl\"  # Output file for smoke test results\n",
    "\n",
    "sample = []\n",
    "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        sample.append(json.loads(line))   # Parse each line into a Python dict\n",
    "        if i >= 5:                        # Stop after 5 lines\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27a5cd1c-01f0-4e08-af8c-a47eee15ceb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264fbb7cdaf24c89b502b9cd4da2ddde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Smoke summarizing (5):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Smoke summarization saved to: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs/chunk_summaries_smoke.jsonl\n"
     ]
    }
   ],
   "source": [
    "# --- 4-2) Run PEGASUS summarization on the sample chunks ---\n",
    "with open(summ_smoke, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for obj in tqdm(sample, desc=\"Smoke summarizing (5)\"):\n",
    "        cid, text = obj[\"chunk_id\"], obj[\"text\"]\n",
    "\n",
    "        # Summarize using PEGASUS (slightly longer summaries: max_out=96)\n",
    "        summ = summarize_pegasus(\n",
    "            text, tokenizer, model,\n",
    "            device=device,\n",
    "            max_in=512, max_out=96, num_beams=4\n",
    "        )\n",
    "\n",
    "        # Build result object with summary + key points\n",
    "        item = {\n",
    "            \"chunk_id\": cid,                                     # Original chunk ID\n",
    "            \"summary\": summ,                                     # PEGASUS summary text\n",
    "            \"key_points\": [s.strip() for s in summ.split(\". \")   # Naive split into sentences\n",
    "                           if s.strip()][:4]                     # Keep up to 4 key points\n",
    "        }\n",
    "\n",
    "        # Save one JSON object per line\n",
    "        fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"✅ Smoke summarization saved to:\", summ_smoke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c60908f-86d3-447c-907e-99f5e75ac819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>key_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0001</td>\n",
       "      <td>This is the story of the Dursleys and the Potters.</td>\n",
       "      <td>[This is the story of the Dursleys and the Potters.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C0002</td>\n",
       "      <td>The Dursleys left the house for the day, with Mr. Dursley couldn’t bear people who dressed in funny clothes — the getups you saw on young people!</td>\n",
       "      <td>[The Dursleys left the house for the day, with Mr, Dursley couldn’t bear people who dressed in funny clothes — the getups you saw on young people!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0003</td>\n",
       "      <td>On the morning of the first day of school, Mr.</td>\n",
       "      <td>[On the morning of the first day of school, Mr.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0004</td>\n",
       "      <td>The first thing Mr.</td>\n",
       "      <td>[The first thing Mr.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0005</td>\n",
       "      <td>Dudley and Petunia Dursley had a strange day.</td>\n",
       "      <td>[Dudley and Petunia Dursley had a strange day.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_id  \\\n",
       "0    C0001   \n",
       "1    C0002   \n",
       "2    C0003   \n",
       "3    C0004   \n",
       "4    C0005   \n",
       "\n",
       "                                                                                                                                             summary  \\\n",
       "0                                                                                                 This is the story of the Dursleys and the Potters.   \n",
       "1  The Dursleys left the house for the day, with Mr. Dursley couldn’t bear people who dressed in funny clothes — the getups you saw on young people!   \n",
       "2                                                                                                     On the morning of the first day of school, Mr.   \n",
       "3                                                                                                                                The first thing Mr.   \n",
       "4                                                                                                      Dudley and Petunia Dursley had a strange day.   \n",
       "\n",
       "                                                                                                                                            key_points  \n",
       "0                                                                                                 [This is the story of the Dursleys and the Potters.]  \n",
       "1  [The Dursleys left the house for the day, with Mr, Dursley couldn’t bear people who dressed in funny clothes — the getups you saw on young people!]  \n",
       "2                                                                                                     [On the morning of the first day of school, Mr.]  \n",
       "3                                                                                                                                [The first thing Mr.]  \n",
       "4                                                                                                      [Dudley and Petunia Dursley had a strange day.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === 4-3) Preview smoke test summaries in a DataFrame ===\n",
    "# Show full text in DataFrame cells (no truncation)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Load JSONL file (one JSON object per line)\n",
    "df_smoke = pd.read_json(summ_smoke, lines=True)\n",
    "\n",
    "# Display the DataFrame (works nicely in Jupyter)\n",
    "display(df_smoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27e5ec-5c41-4561-88a3-b82cce733cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29e4626a-e7f7-43c2-92f3-56c38fde4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Meta summarization builders ===\n",
    "def _build_meta_with_xsum():\n",
    "    \"\"\"\n",
    "    Build a meta summarization function using PEGASUS-XSum.\n",
    "    This wraps summarize_pegasus with fixed parameters.\n",
    "    \"\"\"\n",
    "    def meta_func(text):\n",
    "        return summarize_pegasus(\n",
    "            text, tokenizer, model,\n",
    "            device=device,\n",
    "            max_in=512,   # input token limit\n",
    "            max_out=96,   # output length (longer summaries)\n",
    "            num_beams=4\n",
    "        )\n",
    "    return meta_func\n",
    "\n",
    "\n",
    "def _build_meta_with_multinews():\n",
    "    \"\"\"\n",
    "    Build a meta summarization function using PEGASUS-MultiNews.\n",
    "    You can swap the model_name here if you want MultiNews instead of XSum.\n",
    "    \"\"\"\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "    model_name = \"google/pegasus-multi_news\"\n",
    "    tok_mn = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl_mn = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def meta_func(text):\n",
    "        return summarize_pegasus(\n",
    "            text, tok_mn, mdl_mn,\n",
    "            device=device,\n",
    "            max_in=1024,   # MultiNews handles longer input\n",
    "            max_out=128,   # longer summaries\n",
    "            num_beams=5\n",
    "        )\n",
    "    return meta_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2bfe6e0-16dc-4c10-9b37-6d377c0c223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare leaves for tree building ---\n",
    "# Convert sample (list of dicts) into list of (chunk_id, text) tuples\n",
    "leaves = [(obj[\"chunk_id\"], obj[\"text\"]) for obj in sample]   # only the first 5 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ebcd2d33-9439-48bc-a803-c10a4e10bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ meta: using pegasus-xsum (prompt-enhanced)\n",
      "fanout = 2, leaves = 5\n"
     ]
    }
   ],
   "source": [
    "# 3. Choose meta_summarize function\n",
    "TRY_MULTINEWS = False\n",
    "if TRY_MULTINEWS:\n",
    "    try:\n",
    "        meta_summarize = _build_meta_with_multinews()\n",
    "        print(\"✅ meta: using pegasus-multi_news\")\n",
    "    except Exception as e:\n",
    "        print(f\"ℹ️ multi_news load failed → fallback to XSum: {e}\")\n",
    "        meta_summarize = _build_meta_with_xsum()\n",
    "        print(\"✅ meta: using pegasus-xsum (prompt-enhanced)\")\n",
    "else:\n",
    "    meta_summarize = _build_meta_with_xsum()\n",
    "    print(\"✅ meta: using pegasus-xsum (prompt-enhanced)\")\n",
    "\n",
    "# 4. Fanout setting (2 for smoke test, 6 for full run)\n",
    "fanout = 2 if len(leaves) <= 10 else 6\n",
    "print(f\"fanout = {fanout}, leaves = {len(leaves)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2875c2b3-42ad-4f45-a256-764e722de774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#three build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34ae9ae2-f196-42db-b2e0-75f927e2c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dicts into list of (id, text) tuples\n",
    "leaves = [(obj[\"chunk_id\"], obj[\"text\"]) for obj in sample]   # smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74a5e6d4-35d0-446e-9b04-ad78795c4222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tree built. Root: L3_N0001\n",
      "🧾 Root summary preview:\n",
      " All images are copyrighted. ...\n"
     ]
    }
   ],
   "source": [
    "# --- 5) Build the hierarchical summary tree ---\n",
    "level, nodes, current = 0, [], leaves  # current: list of (node_id, text_or_summary)\n",
    "\n",
    "while len(current) > 1:\n",
    "    level += 1\n",
    "    grouped = [current[i:i + fanout] for i in range(0, len(current), fanout)]\n",
    "    next_level = []\n",
    "    for gi, group in enumerate(grouped, 1):\n",
    "        children = [cid for cid, _ in group]\n",
    "        texts    = [t   for _,   t in group]\n",
    "        joined = \"\\n\\n\".join(texts)           # ← 간단/명확\n",
    "        summ   = meta_summarize(joined)       # ← meta는 문자열 받음\n",
    "\n",
    "        node_id = f\"L{level}_N{gi:04d}\"\n",
    "        nodes.append({\"node_id\": node_id, \"level\": level, \"children\": children, \"summary\": summ})\n",
    "        next_level.append((node_id, summ))\n",
    "    current = next_level\n",
    "\n",
    "root_id, root_summary = current[0]\n",
    "print(\"✅ Tree built. Root:\", root_id)\n",
    "print(\"🧾 Root summary preview:\\n\", root_summary[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a57b4-fc1f-44bc-b69e-629ce19ab872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "500a6187-a217-4666-9bc1-d213f30863e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ tree_nodes.jsonl: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs/tree_nodes.jsonl\n",
      "✅ tree_root.json : /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs/tree_root.json\n",
      "\n",
      "📌 Root Summary:\n",
      " {\n",
      "  \"root_id\": \"L3_N0001\",\n",
      "  \"summary\": \"All images are copyrighted.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## 6. Save results + Preview root summary\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output file paths\n",
    "nodes_path = OUT / \"tree_nodes.jsonl\"   # full hierarchy (all nodes)\n",
    "root_path  = OUT / \"tree_root.json\"     # only the root summary\n",
    "\n",
    "## 6. Save results + Preview root summary\n",
    "\n",
    "# Unpack root node (the last remaining node after tree building)\n",
    "root_id, root_text = current[0]\n",
    "\n",
    "# Save all nodes (entire tree) as JSONL: one JSON object per line\n",
    "nodes_path.write_text(\n",
    "    \"\\n\".join(json.dumps(n, ensure_ascii=False) for n in nodes),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# Save only the root summary as a JSON file (pretty-printed)\n",
    "root_path.write_text(\n",
    "    json.dumps({\"root_id\": root_id, \"summary\": root_text}, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# Print confirmation and preview the root summary\n",
    "print(\"✅ tree_nodes.jsonl:\", nodes_path)\n",
    "print(\"✅ tree_root.json :\", root_path)\n",
    "print(\"\\n📌 Root Summary:\\n\", root_path.read_text(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4bc4da1-581b-44c6-a2cf-9d64b64131f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_meta_with_xsum():\n",
    "    \"\"\"\n",
    "    Build a meta summarization function using PEGASUS-XSum.\n",
    "    This function returns another function (_fn) that takes a list of texts\n",
    "    (e.g., child summaries) and produces one higher-level summary.\n",
    "    \"\"\"\n",
    "    def _fn(texts, max_in=512, max_out=220, num_beams=8):\n",
    "        # Create a prompt from child summaries (bullet point style)\n",
    "        prompt = (\n",
    "            \"Summarize the following bullet points into a cohesive 4–6 sentence paragraph. \"\n",
    "            \"Write declarative sentences only (no questions, no instructions). \"\n",
    "            \"Include main characters, setting, key events/conflict, and why it matters.\\n\\n\"\n",
    "            + \"\\n\".join(f\"- {t}\" for t in texts)\n",
    "        )\n",
    "\n",
    "        # Run PEGASUS summarization on the prompt\n",
    "        out = summarize_pegasus(prompt, max_in=max_in, max_out=max_out, num_beams=num_beams)\n",
    "\n",
    "        # Quality check: retry if output looks bad (too short, question form, etc.)\n",
    "        bad = (\n",
    "            out.strip().endswith(\"?\")\n",
    "            or out.strip().lower().startswith((\"how \", \"do you \", \"what \"))\n",
    "            or len(out.split()) < 35\n",
    "        )\n",
    "        if bad:\n",
    "            # Retry once with longer output and wider beam search\n",
    "            out = summarize_pegasus(prompt, max_in=max_in, max_out=max_out+40, num_beams=num_beams+2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    return _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f25d4f59-8f89-4108-8c23-e62bf941d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-multi_news and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ meta: using pegasus-multi_news\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare meta_summarize (try MultiNews, fallback to XSum) ---\n",
    "try:\n",
    "    # Try to build meta summarizer with PEGASUS-MultiNews\n",
    "    meta_summarize = _build_meta_with_multinews()\n",
    "    print(\"✅ meta: using pegasus-multi_news\")\n",
    "except Exception as e:\n",
    "    # If MultiNews fails, fall back to PEGASUS-XSum\n",
    "    print(f\"ℹ️ multi_news load failed, falling back to XSum: {e}\")\n",
    "    meta_summarize = _build_meta_with_xsum()\n",
    "    print(\"✅ meta: using pegasus-xsum (prompt-enhanced)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977493d-b1cd-4b73-bab8-c34cdfcfd32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
