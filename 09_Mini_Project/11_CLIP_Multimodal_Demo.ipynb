{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f099e360-939b-4132-8a16-b48cbe897791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open-clip-torch in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: torch in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: pillow in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (11.1.0)\n",
      "Requirement already satisfied: regex in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from open-clip-torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from open-clip-torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from open-clip-torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from open-clip-torch) (0.33.1)\n",
      "Requirement already satisfied: safetensors in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from open-clip-torch) (0.5.3)\n",
      "Requirement already satisfied: timm>=1.0.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from open-clip-torch) (1.0.19)\n",
      "Requirement already satisfied: filelock in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: numpy in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from timm>=1.0.17->open-clip-torch) (6.0.2)\n",
      "Requirement already satisfied: wcwidth in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from ftfy->open-clip-torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch) (24.2)\n",
      "Requirement already satisfied: requests in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch) (1.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install open-clip-torch torch torchvision pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b1055d15-7205-4639-b509-80730e49fcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCLIP mainly consists of two encoders:\\nAn image encoder (based on a Vision Transformer or CNN)\\nA text encoder (based on a Transformer)\\nEach of these encoders is responsible for converting images and text into vector embeddings, respectively.\\nCLIP is not an encoder-decoder model;\\ninstead, it can be seen as a Siamese encoder model that embeds both images and text into the same vector space.\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CLIP mainly consists of two encoders:\n",
    "An image encoder (based on a Vision Transformer or CNN)\n",
    "A text encoder (based on a Transformer)\n",
    "Each of these encoders is responsible for converting images and text into vector embeddings, respectively.\n",
    "CLIP is not an encoder-decoder model;\n",
    "instead, it can be seen as a Siamese encoder model that embeds both images and text into the same vector space.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "be88ff2d-c60f-4617-a994-dbcd2dce54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8d45c191-db15-41a8-90b0-6b5f18ffdc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. model loading\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a009bfe5-692e-4fde-83de-42c8645df4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device setting (MPS > CUDA > CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9154dbbf-73b3-4709-b4c3-d05d5653c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0c5c1230-f56c-45a9-8acc-b06480c97eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "90fda0f6-2912-4971-bffd-302d32324a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f6f1f7fc-0394-4e02-98d5-1e2fbcaa2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03bc0613-38b5-49c0-9b88-abec15088b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c4d1e7d6-213c-44ab-984e-2d0f514fea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"11_data/fubao.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "908039a2-5609-4ac4-8cdc-c44d330918f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to RGB to make it compatible with the CLIP model\n",
    "image = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "40238824-4e20-49f8-b545-8908153142c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image, add batch dimension, and move to device\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4825c12c-2de4-4e3a-8697-e2ac447bd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text tokenizing using CLIP tokenizer => text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b6ee61fb-c0ab-4f59-9b81-adfd313aeb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"A cute baby panda\",\n",
    "    \"A sleeping cat\",\n",
    "    \"A playful puppy\",\n",
    "    \"A panda eating bamboo\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b54733c8-8b4b-48ae-9e22-9d156901b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = tokenizer(texts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "af23d3c3-d4c8-4573-8ba2-a529453ae29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8a27770b-e4ea-48fd-a08f-8f2e9ed6a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese encoder 모델 임베딩 추출 (gradient 계산 OFF)\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4d20e8af-a5aa-4964-ad8d-748cc2c48104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the length(norm) of each vector along the last dimention\n",
    "#with keepdim=true , the dimention is retained = shape the bactch size =1\n",
    "#the original tenspr is then divided by these length vaues for each vector\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7386fb74-ade0-4a7c-b291-e864274a6ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Image features shape:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "73bc805b-0031-4a08-afd7-2ee41d39a591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features shape: torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Text features shape:\", text_features.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "48a1fdab-c7ca-44e3-a271-edc8257cc6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding sample: tensor([ 0.0027, -0.0375, -0.1141, -0.0503, -0.0033], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Image embedding sample:\", image_features[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "baf96423-ca78-4024-8566-db36a171e3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding sample (1st): tensor([-0.0042,  0.0061,  0.0118, -0.0102,  0.0170], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Text embedding sample (1st):\", text_features[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2704b8f7-1bff-4106-82ec-64d8966c3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot product (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "30c36413-631f-4658-b176-f82ab5d58a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "00e7d71e-6a53-41d1-9153-15ddb20f68ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"A cute baby panda\" → similarity score: 0.9997\n",
      "2. \"A sleeping cat\" → similarity score: 0.0000\n",
      "3. \"A playful puppy\" → similarity score: 0.0000\n",
      "4. \"A panda eating bamboo\" → similarity score: 0.0003\n"
     ]
    }
   ],
   "source": [
    "for i, (text, score) in enumerate(zip(texts, similarity[0])):\n",
    "    print(f\"{i+1}. \\\"{text}\\\" → similarity score: {score.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f9ffb-8c68-4d10-9250-04d44e4da93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pyenv)",
   "language": "python",
   "name": "pyenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
