{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15f9e7d0-b458-441f-9861-ea545adf2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f555c88-0ee5-4251-b1c3-419f003e7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84d6ee4d-6e65-4899-87ef-565268f47547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device selection\n",
    "# Pick device (Apple Silicon â†’ mps, else cuda if available, else cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53d4f680-c62b-4606-bc74-11bd1856d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"âœ… device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b439a44-e2b7-4446-80fb-1f73819206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer Draft Vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5a84d3a-4f64-4e69-af5c-1954e4ecf1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_id  = \"distilgpt2\"  # draft model (smaller/faster)\n",
    "target_id = \"gpt2\"        # target model (larger/higher quality)\n",
    "\n",
    "# Use the target model's tokenizer so both models share the same tokenization/vocab\n",
    "tok = AutoTokenizer.from_pretrained(target_id)\n",
    "tok.pad_token = tok.eos_token  # set pad token to eos to silence padding warnings\n",
    "\n",
    "# Load models and move to the selected device; switch to eval() for inference\n",
    "draft  = AutoModelForCausalLM.from_pretrained(draft_id).to(device).eval()\n",
    "target = AutoModelForCausalLM.from_pretrained(target_id).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aaeeb696-8158-4df5-ad05-10451dd1120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: reproducibility\n",
    "_ = torch.manual_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a71b7356-c3a7-417d-a3b7-6eefec4ff6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draft : Token proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c009467-3f14-4b8b-b960-baaa5ceae01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def draft_next_token(input_ids: torch.Tensor) -> Tuple[torch.Tensor, int, torch.Tensor]:\n",
    "    # 1) Forward pass: logits shape is [batch=1, seq_len=T, vocab_size=V]\n",
    "    out = draft(input_ids=input_ids)\n",
    "\n",
    "    # 2) Take logits at the last position (distribution for the next token): [1, V]\n",
    "    logits = out.logits[:, -1, :]\n",
    "\n",
    "    # 3) Greedy top-1 over the vocabulary: resulting shape [1, 1]\n",
    "    next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "    # 4) Append the chosen token to the sequence along the time dimension: [1, T+1]\n",
    "    new_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "\n",
    "    # Return the extended sequence, the scalar token id, and the last-step logits\n",
    "    return new_ids, int(next_id.item()), logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62f9920a-135c-470a-83f4-bf9e4c121c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In a quiet village by the sea,\n",
      "proposed token id: 262\n",
      "proposed token   : ' the'\n",
      "length: 8 -> 9\n",
      "appended OK? True\n"
     ]
    }
   ],
   "source": [
    "# Minimal test: print what the draft just proposed\n",
    "\n",
    "prompt = \"In a quiet village by the sea,\"\n",
    "enc = tok(prompt, return_tensors=\"pt\")\n",
    "ids0 = enc.input_ids.to(device)              # [1, T]\n",
    "\n",
    "new_ids, token_id, logits = draft_next_token(ids0)\n",
    "\n",
    "print(\"prompt:\", prompt)\n",
    "print(\"proposed token id:\", token_id)\n",
    "print(\"proposed token   :\", repr(tok.decode([token_id])))\n",
    "print(\"length:\", ids0.shape[1], \"->\", new_ids.shape[1])     # T -> T+1\n",
    "print(\"appended OK?\", new_ids[0, -1].item() == token_id)    # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "264d2ea2-df0e-4203-89d0-30d21d0e5f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef draft_propose_k: Propose K next tokens sequentially with the draft model (greedy).\\n Notes:\\n        - This is compatible with `draft_next_token` that returns (new_ids, token_id, logits).\\n        - The `temperature` argument is kept for API compatibility but not used here\\n          because `draft_next_token` is greedy in this setup.\\nArgs:\\n     input_ids: Current context token IDs. Shape: [1, T]\\n        k : Number of tokens to propose in a row.\\nReturns:\\n        A Python list of length K with the proposed token IDs (ints).\\n    '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def draft_propose_k: Propose K next tokens sequentially with the draft model (greedy).\n",
    " Notes:\n",
    "        - This is compatible with `draft_next_token` that returns (new_ids, token_id, logits).\n",
    "        - The `temperature` argument is kept for API compatibility but not used here\n",
    "          because `draft_next_token` is greedy in this setup.\n",
    "Args:\n",
    "     input_ids: Current context token IDs. Shape: [1, T]\n",
    "        k : Number of tokens to propose in a row.\n",
    "Returns:\n",
    "        A Python list of length K with the proposed token IDs (ints).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5bf4d76-7650-4c99-831e-739b28f25462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft â†’ propose K tokens (compatible with draft_next_token returning (new_ids, int, logits))\n",
    "@torch.no_grad()\n",
    "def draft_propose_k(input_ids: torch.Tensor, k: int = 4, temperature: float = 0.8) -> List[int]:\n",
    "    ids = input_ids.clone()        # work on a copy; do not mutate the caller's tensor\n",
    "    proposals: List[int] = []\n",
    "    for _ in range(k):\n",
    "        ids, nid, _ = draft_next_token(ids)  # (new_ids, proposed_token_id, logits)\n",
    "        proposals.append(nid)\n",
    "    return proposals  # e.g., [1234, 42, 50256, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6656b67f-7084-4d4c-9de0-1902801d675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed ids : [262, 7404, 318, 257, 1402]\n",
      "proposed toks: [\"' the'\", \"' village'\", \"' is'\", \"' a'\", \"' small'\"]\n",
      "original len : 8\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a quiet village by the sea,\"\n",
    "enc = tok(prompt, return_tensors=\"pt\")\n",
    "ids0 = enc.input_ids.to(device)\n",
    "\n",
    "props = draft_propose_k(ids0, k=5)  # propose 5 tokens\n",
    "print(\"proposed ids :\", props)\n",
    "print(\"proposed toks:\", [repr(tok.decode([t])) for t in props])\n",
    "print(\"original len :\", ids0.shape[1])  # ids0 is unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63c76172-e44e-41a5-9e4b-48af87a9629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target â†’ top-1 token (Jupyter cell)\n",
    "#Returns  int: The ID of the most likely next token according to the target model.\n",
    "@torch.no_grad()\n",
    "def target_top1(input_ids: torch.Tensor) -> int:\n",
    "    out = target(input_ids=input_ids)      # logits shape: [1, T, V]\n",
    "    logits = out.logits[:, -1, :]          # take last-step logits: [1, V]\n",
    "    return int(torch.argmax(logits, dim=-1).item())  # greedy argmax â†’ scalar token id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80f66a4f-ddc8-47b2-b7e6-d516b7ed4448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1 id: 262\n",
      "top-1 tok: ' the'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a quiet village by the sea,\"\n",
    "enc = tok(prompt, return_tensors=\"pt\")\n",
    "ids = enc.input_ids.to(device)\n",
    "\n",
    "tid = target_top1(ids)\n",
    "print(\"top-1 id:\", tid)\n",
    "print(\"top-1 tok:\", repr(tok.decode([tid])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a69736b7-7a13-4d80-89c2-f8c74433650f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef target_sample_one : Sample ONE next token from the target model (stochastic decoding).\\nArgs:\\ninput_ids : Current context token IDs. Shape: [1, T]\\ntemperature: Softens/sharpens the distribution (>1 = more random, <1 = more greedy)\\nReturns=> int: Sampled next-token ID.\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def target_sample_one : Sample ONE next token from the target model (stochastic decoding).\n",
    "Args:\n",
    "input_ids : Current context token IDs. Shape: [1, T]\n",
    "temperature: Softens/sharpens the distribution (>1 = more random, <1 = more greedy)\n",
    "Returns=> int: Sampled next-token ID.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8bd882ad-1747-4fc9-918d-a48f4a8bbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def target_sample_one(input_ids: torch.Tensor, temperature: float = 0.7) -> int:\n",
    "    # Forward pass â†’ logits over the vocabulary at each step: [1, T, V]\n",
    "    out = target(input_ids=input_ids)\n",
    "\n",
    "    # Take only the last-step logits (distribution for the next token): [1, V]\n",
    "    logits = out.logits[:, -1, :]\n",
    "\n",
    "    # Temperature scaling, then convert logits â†’ probabilities\n",
    "    probs = F.softmax(logits / max(1e-6, temperature), dim=-1)\n",
    "\n",
    "    # Multinomial sampling: draw exactly one token id from the categorical distribution\n",
    "    next_id = torch.multinomial(probs, num_samples=1)  # shape: [1, 1]\n",
    "\n",
    "    # Return as a Python int (batch=1 assumed)\n",
    "    return int(next_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "182639bf-5324-4c0a-b45b-bf4f0fac6647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled id: 262\n",
      "sampled tok: ' the'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a quiet village by the sea,\"\n",
    "enc = tok(prompt, return_tensors=\"pt\")\n",
    "ids = enc.input_ids.to(device)\n",
    "\n",
    "tid = target_sample_one(ids, temperature=0.8)\n",
    "print(\"sampled id:\", tid)\n",
    "print(\"sampled tok:\", repr(tok.decode([tid])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac811f1e-2357-40e9-9124-31b6b4a5164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the draft's proposed tokens in order using the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d534f759-b29a-4a21-82ef-96fc6f9d3e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process:\n",
    "#If target top-1 == proposed token â†’ accept (append) and continue.\n",
    "#Otherwise â†’ sample ONE token from the target, append it, and STOP this cycle.\n",
    "#Args:\n",
    "#-input_ids: Current context IDs. Shape: [1, T]\n",
    "#- proposed  : List of K proposed token IDs from the draft (ints)\n",
    "#-temperature: Used only for the rejection path sampling\n",
    "#Returns:new_ids : Updated context after this cycle. Shape: [1, T + accepted] or [1, T + accepted + 1] if rejected\n",
    "#accepted: Number of proposals accepted in this cycle (0..K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee3bb781-8f19-40b6-b088-1828c2612eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def verify_one_cycle(\n",
    "    input_ids: torch.Tensor,\n",
    "    proposed: List[int],\n",
    "    temperature: float = 0.7\n",
    ") -> Tuple[torch.Tensor, int]:\n",
    "    ids = input_ids.clone()\n",
    "    accepted = 0\n",
    "    for t in proposed:\n",
    "        top1 = target_top1(ids)\n",
    "        if top1 == t:  # accept\n",
    "            ids = torch.cat([ids, torch.tensor([[t]], device=ids.device)], dim=1)\n",
    "            accepted += 1\n",
    "        else:          # reject â†’ target samples one token, then stop\n",
    "            samp = target_sample_one(ids, temperature=temperature)\n",
    "            ids = torch.cat([ids, torch.tensor([[samp]], device=ids.device)], dim=1)\n",
    "            break\n",
    "    return ids, accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72dadde8-6460-4b05-949d-be747918cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed: [262, 7404, 318]\n",
      "accepted: 2\n",
      "len: 8 -> 11\n",
      "partial: In a quiet village by the sea, the village did\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a quiet village by the sea,\"\n",
    "enc = tok(prompt, return_tensors=\"pt\")\n",
    "ids0 = enc.input_ids.to(device)\n",
    "\n",
    "props = draft_propose_k(ids0, k=3)\n",
    "ids1, acc = verify_one_cycle(ids0, props, temperature=0.7)\n",
    "\n",
    "print(\"proposed:\", props)\n",
    "print(\"accepted:\", acc)\n",
    "print(\"len:\", ids0.shape[1], \"->\", ids1.shape[1])\n",
    "print(\"partial:\", tok.decode(ids1[0, :], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2297414f-4689-42af-b60c-53cbb9c82002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Minimal speculative decoding loop.\\n\\n    Steps per cycle:\\n      1) Draft proposes K tokens (greedy in this setup).\\n      2) Target verifies them in order:\\n         - if target top-1 == proposed â†’ accept (append) and continue\\n         - else â†’ target samples ONE token, append it, stop the cycle\\n      3) Repeat cycles until `max_new_tokens` are generated.\\n\\n    Args:\\n        prompt         : Seed text.\\n        max_new_tokens : Generation budget (number of new tokens to add).\\n        k              : How many tokens the draft proposes per cycle.\\n        draft_temp     : Present for API symmetry; not used because draft is greedy here.\\n        target_temp    : Temperature for the target's sampling on reject.\\n\\n    Returns:\\n        generated_text : Decoded string.\\n        total_accepted : Total number of draft proposals accepted by the target.\\n\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Minimal speculative decoding loop.\n",
    "\n",
    "    Steps per cycle:\n",
    "      1) Draft proposes K tokens (greedy in this setup).\n",
    "      2) Target verifies them in order:\n",
    "         - if target top-1 == proposed â†’ accept (append) and continue\n",
    "         - else â†’ target samples ONE token, append it, stop the cycle\n",
    "      3) Repeat cycles until `max_new_tokens` are generated.\n",
    "\n",
    "    Args:\n",
    "        prompt         : Seed text.\n",
    "        max_new_tokens : Generation budget (number of new tokens to add).\n",
    "        k              : How many tokens the draft proposes per cycle.\n",
    "        draft_temp     : Present for API symmetry; not used because draft is greedy here.\n",
    "        target_temp    : Temperature for the target's sampling on reject.\n",
    "\n",
    "    Returns:\n",
    "        generated_text : Decoded string.\n",
    "        total_accepted : Total number of draft proposals accepted by the target.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3969b7df-b13f-4bea-b94f-c1ee9363034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def speculative_generate_minimal(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 60,\n",
    "    k: int = 4,\n",
    "    draft_temp: float = 0.8,   # kept for API compatibility; ignored by greedy draft_propose_k\n",
    "    target_temp: float = 0.7,  # used inside verify_one_cycle (sampling on reject)\n",
    ") -> Tuple[str, int]:\n",
    "\n",
    "    # Encode prompt and move to device\n",
    "    enc = tok(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "\n",
    "    base_len = input_ids.shape[1]\n",
    "    total_accepted = 0\n",
    "\n",
    "    # Keep generating until we reach the budget\n",
    "    while (input_ids.shape[1] - base_len) < max_new_tokens:\n",
    "        # 1) Draft proposes K tokens (greedy; `draft_temp` ignored here)\n",
    "        proposed = draft_propose_k(input_ids, k=k, temperature=draft_temp)\n",
    "\n",
    "        # 2) Target verifies proposals and merges accepted ones\n",
    "        input_ids, acc = verify_one_cycle(input_ids, proposed, temperature=target_temp)\n",
    "        total_accepted += acc\n",
    "\n",
    "        # 3) If nothing was accepted this cycle, advance one step with target top-1\n",
    "        if acc == 0 and (input_ids.shape[1] - base_len) < max_new_tokens:\n",
    "            nid = target_top1(input_ids)\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[nid]], device=device)], dim=1)\n",
    "\n",
    "    # Decode the final sequence\n",
    "    text = tok.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return text, total_accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42bf5bd9-0aee-4183-b232-48d9b6ebee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Prompt: In a quiet village by the sea,\n",
      "âœ… Accepted tokens (by target): 26\n",
      "\n",
      "=== Output ===\n",
      " In a quiet village by the sea, the village of Kshira, Flora, with its beautiful trees, is a town of ordinary people. It is a place of quiet, and lived in a time when the people had no power to control the weather. It is a place of beauty, and is a place of fair play.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a quiet village by the sea,\"\n",
    "text, accepted = speculative_generate_minimal(\n",
    "    prompt, max_new_tokens=60, k=3, draft_temp=0.7, target_temp=0.7\n",
    ")\n",
    "print(\"ðŸ“ Prompt:\", prompt)\n",
    "print(\"âœ… Accepted tokens (by target):\", accepted)\n",
    "print(\"\\n=== Output ===\\n\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157ffbc-3cb6-4c37-8936-69a33c9b2b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
