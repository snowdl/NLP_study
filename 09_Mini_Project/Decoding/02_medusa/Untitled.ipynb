{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dee688-7dce-41f2-9c8a-0238d17fcadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medusa-lite flow : drafter â†’ verifier â†’ multi-branch prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "574acafc-251b-41b4-839b-2b830cd01e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f76b73-22f9-43f3-8147-e347011afde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Device ì„ íƒ\n",
    "\n",
    "def pick_device():\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"     # ë§¥ë¶ì´ë©´ mps\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"    # GPU ìˆìœ¼ë©´ cuda\n",
    "    return \"cpu\"         # ë‚˜ë¨¸ì§€ëŠ” cpu\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"âœ… DEVICE =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac97c899-d7e5-4382-a66b-733297bd2fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert DEVICE in {\"cpu\", \"cuda\", \"mps\"}\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7638c442-b9bf-40e2-a5ac-ba623689249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed ê³ ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2eee6f1-cbe5-4039-a818-0531387899a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… device: mps\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch, random\n",
    "\n",
    "# Device ìë™ ì„ íƒ\n",
    "DEVICE = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"âœ… device:\", DEVICE)\n",
    "\n",
    "# ì¬í˜„ì„±(ìƒ˜í”Œë§ ì•ˆ ì“°ë‹ˆê¹Œ í° ì˜í–¥ì€ ì—†ì§€ë§Œ ê³ ì •)\n",
    "random.seed(42); torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc287d39-609a-46fe-8ff0-22ee92ebb0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cffe8892-3fc5-4a6c-b431-a13648e10fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    DRAFTER_ID: str = \"distilgpt2\"\n",
    "    VERIFIER_ID: str = \"gpt2-medium\"\n",
    "\n",
    "    MAX_NEW_TOKENS: int = 30\n",
    "\n",
    "    TEMPERATURE: float = 0.8   # ğŸ”¹ ì¶”ê°€\n",
    "    TOP_P: float = 0.9         # ğŸ”¹ ì¶”ê°€\n",
    "\n",
    "    REPETITION_PENALTY: float = 1.3\n",
    "    NO_REPEAT_NGRAM: int = 5\n",
    "\n",
    "    TOPK_BRANCH: int = 4\n",
    "    DRAFT_SPAN: int = 3\n",
    "\n",
    "    DEVICE: str = DEVICE\n",
    "    DEBUG: bool = False\n",
    "\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34a66f10-e69f-467d-9e62-0430b012ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Draft model (Tokenizer-> Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a29b3e32-fa22-4e42-8066-ad0c551abad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… models ready: distilgpt2 / gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "drafter_tok  = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "\n",
    "# gpt2 ê³„ì—´ì€ eos/padê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° å¤š â†’ ë³´ì •\n",
    "if verifier_tok.eos_token_id is None:\n",
    "    verifier_tok.eos_token = \"\"\n",
    "if verifier_tok.pad_token_id is None:\n",
    "    verifier_tok.pad_token = verifier_tok.eos_token\n",
    "\n",
    "EOS_ID = verifier_tok.eos_token_id\n",
    "\n",
    "drafter  = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE).eval()\n",
    "verifier = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE).eval()\n",
    "\n",
    "# ìºì‹œ ì‚¬ìš© í™œì„±í™”(ê¸°ë³¸ True ì´ì§€ë§Œ ëª…ì‹œ)\n",
    "drafter.config.use_cache  = True\n",
    "verifier.config.use_cache = True\n",
    "\n",
    "print(\"âœ… models ready:\", cfg.DRAFTER_ID, \"/\", cfg.VERIFIER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35c9c8ed-930a-4c08-8ab1-54f9c624fa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ok? True | shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "#Prompt & Context preparation# Prompt & Context\n",
    "prompt = \"In a distant future, a small crew of explorers discovers \"\n",
    "\n",
    "# drafter í† í¬ë‚˜ì´ì €ë¡œ ì¸ì½”ë”© + DEVICE ì˜¬ë¦¬ê¸°\n",
    "ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "\n",
    "# input_idsë§Œ ë”°ë¡œ êº¼ë‚´ê¸°\n",
    "input_ids = ctx[\"input_ids\"]\n",
    "\n",
    "print(\"context ok?\", ctx is not None, \"| shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f468de4-2962-45f9-9aa7-27a6260de082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draft í•œ í† í° ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfc10263-c989-4bd5-8830-8072961ca328",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens_basic(model, ids, k: int, temperature: float = 0.8):\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "    k = min(k, probs.numel())\n",
    "    picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "    return [int(i) for i in picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "355c1084-6206-4e50-a143-f119d1628798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token str (repr): '\\xa0'\n",
      "gpt2 piece: Ã‚Å‚\n",
      "is space? True\n"
     ]
    }
   ],
   "source": [
    "tid = 1849\n",
    "print(\"token str (repr):\", repr(drafter_tok.decode([tid])))\n",
    "print(\"gpt2 piece:\", drafter_tok.convert_ids_to_tokens([tid])[0])\n",
    "print(\"is space?\", drafter_tok.decode([tid]).isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b46f125f-884c-433c-863a-6b94220ee084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë©€í‹°-ë¸Œëœì¹˜ Draft í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bb14a27-6f04-4c2f-ab6a-5daa4bf73249",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids, k: int, temperature=0.8, top_p=0.9) -> list[int]:\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "\n",
    "    # nucleus(top-p) ìƒ˜í”Œë§\n",
    "    if top_p is not None:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "        keep = cumsum <= top_p\n",
    "        keep[0] = True\n",
    "        pool_ix = sorted_ix[keep]\n",
    "        pool_p  = probs[pool_ix] / probs[pool_ix].sum()\n",
    "        picks   = torch.multinomial(pool_p, num_samples=min(k, pool_ix.numel()), replacement=False)\n",
    "        return [int(pool_ix[i]) for i in picks]\n",
    "    else:\n",
    "        picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "        return [int(i) for i in picks]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout(ids, first_tok: int, span: int) -> list[int]:\n",
    "    cur = torch.cat([ids, torch.tensor([[first_tok]], device=ids.device)], dim=1)\n",
    "    seq = [first_tok]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        seq.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return seq\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, k: int, span: int, temperature=0.8, top_p=0.9) -> list[list[int]]:\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    branches = [drafter_rollout(ids, f, span) for f in firsts]\n",
    "    return branches\n",
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids, k: int, temperature=0.8, top_p=0.9) -> list[int]:\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "\n",
    "    # nucleus(top-p) ìƒ˜í”Œë§\n",
    "    if top_p is not None:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "        keep = cumsum <= top_p\n",
    "        keep[0] = True\n",
    "        pool_ix = sorted_ix[keep]\n",
    "        pool_p  = probs[pool_ix] / probs[pool_ix].sum()\n",
    "        picks   = torch.multinomial(pool_p, num_samples=min(k, pool_ix.numel()), replacement=False)\n",
    "        return [int(pool_ix[i]) for i in picks]\n",
    "    else:\n",
    "        picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "        return [int(i) for i in picks]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout(ids, first_tok: int, span: int) -> list[int]:\n",
    "    cur = torch.cat([ids, torch.tensor([[first_tok]], device=ids.device)], dim=1)\n",
    "    seq = [first_tok]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        seq.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return seq\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, k: int, span: int, temperature=0.8, top_p=0.9) -> list[list[int]]:\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    branches = [drafter_rollout(ids, f, span) for f in firsts]\n",
    "    return branches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3620faf-5ab6-4f05-a816-4e0cf24bb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifier: í•œ í† í° ì˜ˆì¸¡(greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec078a23-6075-4d6c-8cc5-c333abee6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def verifier_next_token(ids) -> int:\n",
    "    \"\"\"verifierë¡œ ë‹¤ìŒ í† í° id í•˜ë‚˜ ì˜ˆì¸¡ (greedy)\"\"\"\n",
    "    logits = verifier(ids).logits[:, -1, :]\n",
    "    return int(torch.argmax(logits, dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6048916a-51ff-4cdc-aa7c-f8135bb663b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_token(tokenizer, tid: int):\n",
    "    \"\"\"í† í° idë¥¼ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ í‘œí˜„\"\"\"\n",
    "    s_decode = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    s_token  = tokenizer.convert_ids_to_tokens([tid])[0]\n",
    "    try:\n",
    "        s_fixed = s_token.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        s_fixed = s_token\n",
    "    return {\n",
    "        \"id\": tid,\n",
    "        \"decode_repr\": repr(s_decode),   # ì‚¬ëŒì´ ì•ˆ ë³´ì´ëŠ” ê³µë°±ë„ í™•ì¸\n",
    "        \"token_repr\": repr(s_token),     # BPE í† í° ìŠ¤íŠ¸ë§\n",
    "        \"token_fixed\": repr(s_fixed),    # ëª¨ì§€ë°”ì¼€ ë³´ì • ì‹œë„\n",
    "        \"codepoints\": [hex(ord(c)) for c in s_decode],\n",
    "        \"bytes\": list(s_decode.encode(\"utf-8\")),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d3b19ef-a078-4ceb-889d-4967becceffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def next_human_token(ids, tokenizer, tries=10):\n",
    "    \"\"\"ë³´ì´ëŠ” í† í°ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ìµœëŒ€ triesë²ˆ ì˜ˆì¸¡\"\"\"\n",
    "    cur = ids.clone()\n",
    "    for _ in range(tries):\n",
    "        tid = verifier_next_token(cur)\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        if any(ch.isprintable() and not ch.isspace() for ch in s):\n",
    "            return tid, s\n",
    "        cur = torch.cat([cur, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "    return tid, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90e03e16-7db8-4112-b02d-1dc3f960992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ˆì¸¡ í† í° ì •ë³´: {'id': 488, 'decode_repr': \"'ich'\", 'token_repr': \"'ich'\", 'token_fixed': \"'ich'\", 'codepoints': ['0x69', '0x63', '0x68'], 'bytes': [105, 99, 104]}\n",
      "ë‹¤ìŒ ë³´ì´ëŠ” í† í°: 488 'ich'\n"
     ]
    }
   ],
   "source": [
    "vid = verifier_next_token(input_ids)\n",
    "info = pretty_token(verifier_tok, vid)\n",
    "\n",
    "print(\"ì˜ˆì¸¡ í† í° ì •ë³´:\", info)\n",
    "\n",
    "t2, s2 = next_human_token(input_ids, verifier_tok)\n",
    "print(\"ë‹¤ìŒ ë³´ì´ëŠ” í† í°:\", t2, repr(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee5b543e-128a-474c-a00c-d1df2c0cad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix-Accept (mismatchê¹Œì§€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "480cf63a-537b-4739-8e80-52db616ca7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch(context_ids, branch_tokens:List[int]) -> Tuple[torch.Tensor, List[int], bool]:\n",
    "    ids = context_ids.clone()\n",
    "    accepted = []\n",
    "    mismatched = False\n",
    "    for tid in branch_tokens:\n",
    "        pred = verifier_next_token(ids)\n",
    "        if pred == tid:\n",
    "            ids = torch.cat([ids, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "            accepted.append(tid)\n",
    "        else:\n",
    "            ids = torch.cat([ids, torch.tensor([[pred]], device=ids.device)], dim=1)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return ids, accepted, mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1f9d322-655c-4a85-89ec-eb71ee18f4aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ë°©ê¸ˆ ë§Œë“  ë¸Œëœì¹˜ë“¤ ì¤‘ ì²« ë²ˆì§¸ë¥¼ ê²€ì‚¬í•´ë³´ê¸°\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m new_ids, accepted, mism \u001b[38;5;241m=\u001b[39m accept_until_mismatch(input_ids, \u001b[43mb\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccepted len:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(accepted), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| mismatched?\u001b[39m\u001b[38;5;124m'\u001b[39m, mism)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìƒˆ ê¸¸ì´:\u001b[39m\u001b[38;5;124m'\u001b[39m, new_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| ì¶”ê°€ëœ í† í° ìˆ˜:\u001b[39m\u001b[38;5;124m'\u001b[39m, new_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "# ë°©ê¸ˆ ë§Œë“  ë¸Œëœì¹˜ë“¤ ì¤‘ ì²« ë²ˆì§¸ë¥¼ ê²€ì‚¬í•´ë³´ê¸°\n",
    "new_ids, accepted, mism = accept_until_mismatch(input_ids, b[0])\n",
    "print('accepted len:', len(accepted), '| mismatched?', mism)\n",
    "print('ìƒˆ ê¸¸ì´:', new_ids.shape[1], '| ì¶”ê°€ëœ í† í° ìˆ˜:', new_ids.shape[1] - input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686c820-f497-4ba6-bc79-1de30f815e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
