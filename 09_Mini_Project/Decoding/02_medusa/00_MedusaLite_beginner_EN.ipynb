{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e46353b7-7078-415f-9040-3f20f26b8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Medusa-lite flow (beginner)\n",
    "# drafter → verifier → multi-branch prefix-accept\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ab43540-a679-4dfc-bea9-767a75607adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 1) Imports ----------\n",
    "from dataclasses import dataclass\n",
    "import time, math, random\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b722306-3387-4100-a24c-ac353774f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 2) Pick device ----------\n",
    "def pick_device():\n",
    "    \"\"\"Pick the best available device (Apple Silicon -> mps, CUDA GPU -> cuda, else cpu).\"\"\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"✅ DEVICE =\", DEVICE)\n",
    "assert DEVICE in {\"cpu\", \"cuda\", \"mps\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ca4d20be-51af-47dc-b8ed-c2d34cf39cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ seeding...\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 3) Fix seeds (for reproducibility) ----------\n",
    "print(\"✅ seeding...\")\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf964e18-4166-4dff-8104-a69553eb6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 4) Config ----------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Models\n",
    "    DRAFTER_ID: str = \"distilgpt2\"     # small, fast drafter\n",
    "    VERIFIER_ID: str = \"gpt2-medium\"   # stronger verifier\n",
    "\n",
    "    # Generation limits\n",
    "    MAX_NEW_TOKENS: int = 30\n",
    "\n",
    "    # Drafter sampling (diversity)\n",
    "    TEMPERATURE: float = 0.8           # mild temperature\n",
    "    TOP_P: float = 0.9                 # nucleus sampling\n",
    "\n",
    "    # Repetition control for verifier\n",
    "    REPETITION_PENALTY: float = 1.3    # penalize repeated tokens\n",
    "    NO_REPEAT_NGRAM: int = 5           # block repeated n-grams\n",
    "\n",
    "    # Medusa-lite branching\n",
    "    TOPK_BRANCH: int = 3               # number of branches (K)\n",
    "    DRAFT_SPAN: int = 3                # tokens per branch (M)\n",
    "\n",
    "    # Misc\n",
    "    DEVICE: str = DEVICE\n",
    "    DEBUG: bool = False\n",
    "\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dd100859-26a7-428d-b08c-8f99c54e7049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ models ready: distilgpt2 / gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 5) Load models & tokenizers ----------\n",
    "drafter_tok  = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "\n",
    "# GPT-2 family often lacks eos/pad by default -> set them\n",
    "if verifier_tok.eos_token_id is None:\n",
    "    verifier_tok.eos_token = \"\"\n",
    "if verifier_tok.pad_token_id is None:\n",
    "    verifier_tok.pad_token = verifier_tok.eos_token\n",
    "EOS_ID = verifier_tok.eos_token_id\n",
    "\n",
    "drafter  = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE).eval()\n",
    "verifier = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE).eval()\n",
    "drafter.config.use_cache  = True\n",
    "verifier.config.use_cache = True\n",
    "\n",
    "print(\"✅ models ready:\", cfg.DRAFTER_ID, \"/\", cfg.VERIFIER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7f0a711-b21e-4678-93c3-14d51d3c550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ok? True | shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 6) Prepare prompt & context ----------\n",
    "prompt = \"In a distant future, a small crew of explorers discovers \"\n",
    "ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "input_ids = ctx[\"input_ids\"]\n",
    "print(\"context ok?\", ctx is not None, \"| shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d7b0024-611a-4f72-ac17-ba8b68b9b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 7) Drafter: sample K first tokens (basic) ----------\n",
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids, k: int, temperature=0.8, top_p=0.9) -> list[int]:\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "\n",
    "    if top_p is not None:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "        keep = cumsum <= top_p; keep[0] = True\n",
    "        pool_ix = sorted_ix[keep]\n",
    "        pool_p  = probs[pool_ix] / probs[pool_ix].sum()\n",
    "        num = min(k, pool_ix.numel())\n",
    "        picks = torch.multinomial(pool_p, num_samples=num, replacement=False)\n",
    "        firsts = [int(pool_ix[i]) for i in picks]\n",
    "    else:\n",
    "        num = min(k, probs.numel())\n",
    "        picks = torch.multinomial(probs, num_samples=num, replacement=False)\n",
    "        firsts = [int(i) for i in picks]\n",
    "\n",
    "    # ✅ filter out pure whitespace tokens\n",
    "    firsts = [t for t in firsts if not drafter_tok.decode([t]).isspace()]\n",
    "    if not firsts:\n",
    "        firsts = [int(torch.argmax(probs).item())]  # fallback\n",
    "    return firsts[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87d19e12-3cde-40b6-9910-5a95ecaa3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 8) Drafter: greedy rollout of a branch ----------\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout(ids, first_tok: int, span: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Starting from 'first_tok', greedily extend with the drafter for (span-1) tokens.\n",
    "    Returns a list[int] of length = span (including first_tok).\n",
    "    \"\"\"\n",
    "    cur = torch.cat([ids, torch.tensor([[first_tok]], device=ids.device)], dim=1)\n",
    "    seq = [first_tok]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        seq.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c043eb52-3954-4c4c-9f2a-7e5a53e8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 9) Drafter: propose K branches (each length = span) ----------\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, k: int, span: int, temperature=0.8, top_p=0.9) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Propose K branches:\n",
    "      1) sample K first tokens\n",
    "      2) for each, greedy-rollout to length 'span'\n",
    "    Returns: list of branches (list[list[int]]).\n",
    "    \"\"\"\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    branches = [drafter_rollout(ids, f, span) for f in firsts]\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e16d4bd6-e6eb-4989-b3fe-f3f6c77a1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token str (repr): '\\xa0'\n",
      "gpt2 piece: Âł\n",
      "is space? True\n"
     ]
    }
   ],
   "source": [
    "# ---------- (Optional) Inspect a whitespace token example ----------\n",
    "tid = 1849\n",
    "print(\"token str (repr):\", repr(drafter_tok.decode([tid])))\n",
    "print(\"gpt2 piece:\", drafter_tok.convert_ids_to_tokens([tid])[0])\n",
    "print(\"is space?\", drafter_tok.decode([tid]).isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aeb72db2-cd74-48aa-a0a3-70d618b1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 10) Verifier: predict next token (greedy) ----------\n",
    "@torch.inference_mode()\n",
    "def verifier_next_token(ids, repetition_penalty: float = 1.35, no_repeat_ngram: int = 5) -> int:\n",
    "    \"\"\"\n",
    "    Greedy next token with simple repetition controls:\n",
    "    - repetition_penalty: down-weight tokens that already appeared\n",
    "    - no_repeat_ngram: block the n-th token if last (n-1) pattern already occurred\n",
    "    \"\"\"\n",
    "    logits = verifier(ids).logits[:, -1, :].clone()\n",
    "    V = logits.size(-1)\n",
    "\n",
    "    # 1) repetition penalty\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        seen = torch.bincount(ids[0].to(torch.int64), minlength=V).bool()\n",
    "        logits[:, seen] = logits[:, seen] / repetition_penalty\n",
    "\n",
    "    # 2) no-repeat n-gram\n",
    "    n = int(no_repeat_ngram or 0)\n",
    "    if n > 1 and ids.size(1) >= n - 1:\n",
    "        tail = ids[0].tolist()\n",
    "        prefix = tail[-(n-1):]\n",
    "        blocked = set()\n",
    "        for i in range(len(tail) - n + 1):\n",
    "            if tail[i:i+n-1] == prefix:\n",
    "                blocked.add(tail[i+n-1])\n",
    "        if blocked:\n",
    "            logits[:, list(blocked)] = -1e9\n",
    "\n",
    "    return int(torch.argmax(logits, dim=-1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8516d202-1746-4430-9347-17506532cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted token info: {'id': 220, 'decode_repr': \"' '\", 'token_repr': \"'Ġ'\", 'token_fixed': \"'Ġ'\", 'codepoints': ['0x20'], 'bytes': [32]}\n",
      "next visible token: 257 ' a'\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def next_human_token(ids, tokenizer, tries=10):\n",
    "    \"\"\"\n",
    "    Run greedy next-token repeatedly until we see a printable, non-whitespace char,\n",
    "    or we hit 'tries' attempts. Useful when the immediate next token is whitespace.\n",
    "    \"\"\"\n",
    "    cur = ids.clone()\n",
    "    for _ in range(tries):\n",
    "        tid = verifier_next_token(cur)\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        if any(ch.isprintable() and not ch.isspace() for ch in s):\n",
    "            return tid, s\n",
    "        cur = torch.cat([cur, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "    return tid, s\n",
    "\n",
    "vid = verifier_next_token(input_ids)\n",
    "info = pretty_token(verifier_tok, vid)\n",
    "print(\"predicted token info:\", info)\n",
    "t2, s2 = next_human_token(input_ids, verifier_tok)\n",
    "print(\"next visible token:\", t2, repr(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a23bc6a6-8263-4f83-8858-1c9d93865d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted len: 0 | mismatched? True\n",
      "new length: 13 | appended tokens: 1\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 11) Prefix-accept until first mismatch ----------\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch(context_ids, branch_tokens: List[int]) -> Tuple[torch.Tensor, List[int], bool]:\n",
    "    \"\"\"\n",
    "    Simulate prefix-accept with the verifier:\n",
    "      - If verifier's next == branch token -> accept (append branch token)\n",
    "      - On first mismatch -> append verifier token instead and stop\n",
    "    Returns: (new_ids, accepted_tokens_list, mismatched_flag)\n",
    "    \"\"\"\n",
    "    ids = context_ids.clone()\n",
    "    accepted = []\n",
    "    mismatched = False\n",
    "    for tid in branch_tokens:\n",
    "        pred = verifier_next_token(ids)\n",
    "        if pred == tid:\n",
    "            ids = torch.cat([ids, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "            accepted.append(tid)\n",
    "        else:\n",
    "            ids = torch.cat([ids, torch.tensor([[pred]], device=ids.device)], dim=1)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return ids, accepted, mismatched\n",
    "\n",
    "# Example: build branches then test the first one\n",
    "b = drafter_propose(input_ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE, cfg.TOP_P)\n",
    "new_ids, accepted, mism = accept_until_mismatch(input_ids, b[0])\n",
    "print('accepted len:', len(accepted), '| mismatched?', mism)\n",
    "print('new length:', new_ids.shape[1], '| appended tokens:', new_ids.shape[1] - input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1ca643d6-c40b-42a0-abc4-4d98157578d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score sanity: 3 1\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 12) Branch scoring ----------\n",
    "def score_branch(accepted, mismatched):\n",
    "    \"\"\"\n",
    "    Simple score: number of accepted tokens minus a small penalty if mismatch happened.\n",
    "    Larger is better.\n",
    "    \"\"\"\n",
    "    return len(accepted) - (1 if mismatched else 0)\n",
    "\n",
    "print(\"score sanity:\", score_branch([1,2,3], False), score_branch([1,2], True))  # 3, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2392dfbb-de90-4c85-93cc-324c928f2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 13) Utilities for encoding ----------\n",
    "@torch.inference_mode()\n",
    "def encode_prompt(prompt: str):\n",
    "    # use verifier tokenizer for consistency\n",
    "    return verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)[\"input_ids\"]\n",
    "\n",
    "import re\n",
    "_NBSP = \"\\u00A0\"\n",
    "_RE_PUNCT = re.compile(r\"\\s+([,.;:!?])\")\n",
    "_RE_SPACES = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.replace(_NBSP, \" \")     # NBSP → normal space\n",
    "    s = _RE_PUNCT.sub(r\"\\1\", s)   # trim space before punctuation\n",
    "    s = _RE_SPACES.sub(\" \", s)    # collapse multiple spaces\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ee46ff8-cdde-43ee-9b0e-a7536ac3213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 6 → after: 7\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 14) One Medusa step ----------\n",
    "@torch.inference_mode()\n",
    "def medusa_step(ids, topk_branch: int, draft_span: int, temperature: float):\n",
    "    \"\"\"\n",
    "    One iteration:\n",
    "      - Draft K branches of length M\n",
    "      - For each, run prefix-accept until mismatch\n",
    "      - Pick the best by 'score_branch'\n",
    "      - Return the updated token ids\n",
    "    \"\"\"\n",
    "    branches = drafter_propose(ids, topk_branch, draft_span, temperature, cfg.TOP_P)\n",
    "    best_score = -10**9\n",
    "    best_ids = None\n",
    "    for br in branches:\n",
    "        new_ids, accepted, mism = accept_until_mismatch(ids, br)\n",
    "        s = score_branch(accepted, mism)\n",
    "        if s > best_score:\n",
    "            best_score, best_ids = s, new_ids\n",
    "    return best_ids\n",
    "\n",
    "ids2 = medusa_step(ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(\"before:\", ids.shape[1], \"→ after:\", ids2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "79d94d53-2d3f-4fa9-8883-c0577e0ad973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 15) Orchestrator: Medusa-lite generator (with sentence stop) ----------\n",
    "import re\n",
    "\n",
    "@torch.inference_mode()\n",
    "def medusa_generate(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int | None = None,\n",
    "    topk_branch: int | None = None,\n",
    "    draft_span: int | None = None,\n",
    "    temperature: float | None = None,\n",
    "    *,\n",
    "    stop_at_sentence: bool = True,   # stop when sentence ends\n",
    "    tail_chars: int = 80             # how many chars to inspect for sentence end\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run multiple Medusa steps until we reach 'max_new_tokens'.\n",
    "    Adds:\n",
    "      - sentence-level early stop (., !, ? with optional closing quotes/brackets)\n",
    "      - safety: break if a step makes no progress\n",
    "    \"\"\"\n",
    "    if max_new_tokens is None: max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "    if topk_branch   is None: topk_branch   = cfg.TOPK_BRANCH\n",
    "    if draft_span    is None: draft_span    = cfg.DRAFT_SPAN\n",
    "    if temperature   is None: temperature   = cfg.TEMPERATURE\n",
    "\n",
    "    # compile once (local to keep function drop-in)\n",
    "    SENT_END = re.compile(r'[\\.!\\?][\"\\')\\]]?\\s*$')\n",
    "\n",
    "    ids = encode_prompt(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "\n",
    "    steps = math.ceil(max_new_tokens / draft_span)\n",
    "    for _ in range(steps):\n",
    "        before = ids.shape[1]\n",
    "        ids = medusa_step(ids, topk_branch, draft_span, temperature)\n",
    "\n",
    "        # safety: no progress → break\n",
    "        if ids.shape[1] == before:\n",
    "            break\n",
    "\n",
    "        # reached target token budget → break\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "        # sentence-level early stop\n",
    "        if stop_at_sentence:\n",
    "            tail = verifier_tok.decode(ids[0][-min(tail_chars, ids.shape[1]):], skip_special_tokens=True)\n",
    "            if SENT_END.search(tail):\n",
    "                break\n",
    "\n",
    "    # decode final text (GPT-2 family tokenizers are compatible)\n",
    "    return drafter_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7fec024a-5f24-45ad-bce9-a1deeb3cd05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future,      is a   ?\n"
     ]
    }
   ],
   "source": [
    "# Try once\n",
    "out = medusa_generate(\"In a distant future, \", 40)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "370724b0-074e-4403-a604-83bb34e0b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants to control the world's resources so that he can rule the world. \n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 16) Greedy baseline (verifier-only) ----------\n",
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int = None) -> str:\n",
    "    \"\"\"\n",
    "    Plain greedy decoding with the verifier (for comparison).\n",
    "    \"\"\"\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = verifier(ids).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "txt = greedy_generate(\"In a distant future, \", 40)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3f1184f5-a362-4bd8-9808-440d664fd0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ greedy: 3.478 s\n",
      "⏱ medusa: 0.786 s\n",
      "\n",
      "--- greedy ---\n",
      " In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants to control the world's resources so that he can rule the world.  He wants to control the world's resources so that he can rule the world.  He wants to control the world's resources so that he can rule the world.  He wants to\n",
      "\n",
      "--- medusa ---\n",
      " In a distant future,      is a   ?\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 17) A/B timing: greedy vs medusa ----------\n",
    "def time_it(fn, *args, **kwargs):\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    return out, time.perf_counter() - t0\n",
    "\n",
    "g_txt, g_t = time_it(greedy_generate, \"In a distant future, \", 80)\n",
    "m_txt, m_t = time_it(medusa_generate, \"In a distant future, \", 80)\n",
    "\n",
    "print(\"⏱ greedy:\", round(g_t, 3), \"s\")\n",
    "print(\"⏱ medusa:\", round(m_t, 3), \"s\")\n",
    "print(\"\\n--- greedy ---\\n\", g_txt[:400])\n",
    "print(\"\\n--- medusa ---\\n\", m_txt[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66a17-0016-4606-a4dc-03c8dae9450f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc0d46-c787-4048-b06c-a0ee7057924b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
