{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34aed94a-face-4fff-ade1-e7b82fd55787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# =============================\n",
    "# RUN SWITCHES (원하는 단계만 on)\n",
    "# =============================\n",
    "흐름을 다시 정리하면 이렇게 돼요:\n",
    "\n",
    "STEP0 — 디바이스 선택\n",
    "(DEVICE 정하고, seed 설정)\n",
    "\n",
    "STEP1 — 토크나이저 / 모델 준비\n",
    "(tok, model, EOS_ID 준비)\n",
    "\n",
    "STEP2 — 유틸 함수\n",
    "(encode, decode, append_token, last_logits, greedy_next)\n",
    "\n",
    "➡️ STEP3 — 샘플링 함수\n",
    "(softmax_temp, top_p_indices, sample_next) ← 지금 말씀하신 부분\n",
    "\n",
    "STEP4 — 드래프터 (propose_branch)\n",
    "\n",
    "STEP5 — 프리픽스-어셉트 (prefix_accept_once)\n",
    "\n",
    "STEP6 — 루프 실행 (medusa_tiny / run_greedy)\n",
    "\n",
    "STEP7 — 데모 (Greedy vs Medusa 결과 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d321dd9-5565-4e52-b8ad-bbbabaeea32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 0) Config\n",
    "# =============================\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    MODEL_ID: str = \"distilgpt2\"\n",
    "    TEMPERATURE: float = 0.9\n",
    "    TOP_P: float = 0.95\n",
    "    SPAN: int = 3\n",
    "    MAX_NEW_TOKENS: int = 30\n",
    "    BAN_EOS_FIRST_N: int = 0\n",
    "    REP_BAN_N: int = 0\n",
    "    SEED: int | None = 7\n",
    "    DEBUG: bool = False\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a81014a-e195-459a-b269-f5d989ae942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP0] DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "if RUN_STEP_0:\n",
    "    DEVICE = (\n",
    "        \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "        else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    "\n",
    "    def set_seed(seed: int | None) -> None:\n",
    "        if seed is None:\n",
    "            return\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    set_seed(cfg.SEED)\n",
    "    print(f\"[STEP0] DEVICE = {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "079af4e7-4a88-425d-a710-443b348aaa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP1] MODEL = distilgpt2, EOS_ID=50256\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 1 — 토크나이저/모델 준비 (eos/pad 보정)\n",
    "# =============================\n",
    "if RUN_STEP_1:\n",
    "    def load_tokenizer(model_id: str):\n",
    "        tok = AutoTokenizer.from_pretrained(model_id)\n",
    "        if tok.eos_token_id is None:\n",
    "            tok.eos_token = \"\"\n",
    "        if tok.pad_token_id is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        return tok\n",
    "\n",
    "    def load_model(model_id: str, device: str):\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id).to(device).eval()\n",
    "        model.config.use_cache = True\n",
    "        return model\n",
    "\n",
    "    tok = load_tokenizer(cfg.MODEL_ID)\n",
    "    model = load_model(cfg.MODEL_ID, DEVICE)\n",
    "    EOS_ID = tok.eos_token_id\n",
    "\n",
    "    print(f\"[STEP1] MODEL = {cfg.MODEL_ID}, EOS_ID={EOS_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bee3f608-e586-4a30-9d64-c7d8424a4576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP2] utils ready: encode/decode/append/last_logits/greedy_next\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 2 — 유틸 함수 (encode/decode/append/last_logits/greedy_next)\n",
    "# =============================\n",
    "if RUN_STEP_2:\n",
    "    def encode(text: str) -> torch.Tensor:\n",
    "        \"\"\"문자열 -> 토큰 IDs [1, T] (DEVICE로 이동)\"\"\"\n",
    "        return tok(text, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "\n",
    "    def decode(ids: torch.Tensor) -> str:\n",
    "        \"\"\"토큰 IDs -> 문자열 (스페셜 토큰 스킵)\"\"\"\n",
    "        return tok.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def append_token(ids: torch.Tensor, token_id: int) -> torch.Tensor:\n",
    "        \"\"\"마지막에 단일 토큰 붙이기\"\"\"\n",
    "        t = torch.tensor([[token_id]], device=ids.device)\n",
    "        return torch.cat([ids, t], dim=1)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def last_logits(ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"마지막 위치의 로짓 벡터 [V]\"\"\"\n",
    "        return model(ids).logits[0, -1, :]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def greedy_next(ids: torch.Tensor) -> int:\n",
    "        \"\"\"그리디 argmax 토큰 ID\"\"\"\n",
    "        return int(torch.argmax(last_logits(ids)).item())\n",
    "\n",
    "    print(\"[STEP2] utils ready: encode/decode/append/last_logits/greedy_next\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8996215f-d5ca-4c3a-b2b6-d6dd8601bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# STEP 3 — Sampling Function (softmax_temp/top_p_indices/sample_next)\n",
    "# ============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8186ae64-5524-40e6-ab01-f5199c9939c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP3a] ready: softmax_temp\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 3a — softmax_temp\n",
    "# =============================\n",
    "if RUN_STEP_3:\n",
    "    @torch.inference_mode()\n",
    "    def softmax_temp(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "        t = max(float(temperature), 1e-6)\n",
    "        return torch.softmax(logits / t, dim=-1)\n",
    "\n",
    "    print(\"[STEP3a] ready: softmax_temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "024d635a-bc5b-44eb-b9a4-1a808bc725a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP3b] ready: top_p_indices\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 3b — top_p_indices\n",
    "# =============================\n",
    "if RUN_STEP_3:\n",
    "    @torch.inference_mode()\n",
    "    def top_p_indices(probs: torch.Tensor, top_p: float) -> torch.Tensor:\n",
    "        V = probs.numel()\n",
    "        if top_p is None or top_p >= 1:\n",
    "            return torch.arange(V, device=probs.device)\n",
    "        sp, sx = torch.sort(probs, descending=True)\n",
    "        csum = torch.cumsum(sp, dim=0)\n",
    "        keep = csum <= top_p\n",
    "        keep[0] = True\n",
    "        return sx[keep]\n",
    "\n",
    "    print(\"[STEP3b] ready: top_p_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f88bdbc9-f544-46fb-a0ec-1e6dbffd878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP3c] ready: sample_next\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 3c — sample_next\n",
    "# =============================\n",
    "if RUN_STEP_3:\n",
    "    @torch.inference_mode()\n",
    "    def sample_next(ids: torch.Tensor, temperature: float, top_p: float,\n",
    "                    ban_eos: bool = False, rep_ban_n: int = 0) -> int:\n",
    "        logits = last_logits(ids)\n",
    "        probs = softmax_temp(logits, temperature)\n",
    "\n",
    "        # 반복 억제 (최근 N)\n",
    "        if rep_ban_n > 0:\n",
    "            tail = ids[0, -rep_ban_n:].tolist()\n",
    "            probs[tail] = 0\n",
    "            s = probs.sum()\n",
    "            probs = probs if s <= 0 else probs / s\n",
    "            if s <= 0:\n",
    "                probs = softmax_temp(logits, temperature)  # fallback\n",
    "\n",
    "        pool_ix = top_p_indices(probs, top_p)\n",
    "        pool = probs[pool_ix]\n",
    "        pool = pool / pool.sum()\n",
    "        pick_local = int(torch.multinomial(pool, 1)[0].item())\n",
    "        picked = int(pool_ix[pick_local].item())\n",
    "\n",
    "        if ban_eos and EOS_ID is not None and picked == EOS_ID:\n",
    "            # EOS 제외 재샘플 (1회)\n",
    "            mask = pool_ix != EOS_ID\n",
    "            if mask.any():\n",
    "                pool_ix2 = pool_ix[mask]\n",
    "                pool2 = pool[mask]\n",
    "                pool2 = pool2 / pool2.sum()\n",
    "                pick_local = int(torch.multinomial(pool2, 1)[0].item())\n",
    "                picked = int(pool_ix2[pick_local].item())\n",
    "        return picked\n",
    "\n",
    "    print(\"[STEP3c] ready: sample_next\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5293a22d-02c6-4641-9253-e4a6902c2fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP4] drafter ready: propose_one/propose_branch\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 4 — 드래프터 (초초 쪼갬)\n",
    "# =============================\n",
    "if RUN_STEP_4:\n",
    "    @torch.inference_mode()\n",
    "    def propose_one(cur_ids: torch.Tensor, temperature: float, top_p: float,\n",
    "                    accepted_so_far: int) -> int:\n",
    "        \"\"\"드래프팅 1스텝: ban_eos 여부 결정 → sample_next 한 번 호출\"\"\"\n",
    "        ban_eos = accepted_so_far < cfg.BAN_EOS_FIRST_N\n",
    "        return sample_next(cur_ids, temperature, top_p,\n",
    "                           ban_eos=ban_eos, rep_ban_n=cfg.REP_BAN_N)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def propose_branch(ids: torch.Tensor, span: int, temperature: float, top_p: float,\n",
    "                       accepted_so_far: int = 0) -> List[int]:\n",
    "        \"\"\"span 회 만큼 propose_one 반복\"\"\"\n",
    "        cur = ids.clone()\n",
    "        out: List[int] = []\n",
    "        for i in range(span):\n",
    "            t = propose_one(cur, temperature, top_p, accepted_so_far)\n",
    "            out.append(t)\n",
    "            cur = append_token(cur, t)\n",
    "            if cfg.DEBUG:\n",
    "                print(f\"  [draft {i}] pick={t} ({tok.decode([t])!r})\")\n",
    "        return out\n",
    "\n",
    "    print(\"[STEP4] drafter ready: propose_one/propose_branch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c973927-da9c-42ce-b0ce-53a6959789a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e34e5ed8-f83e-48d0-8ede-67188de91849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP5] prefix-accept ready: accept_one/prefix_accept_once\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 5 — 프리픽스-어셉트 (초초 쪼갬)\n",
    "# =============================\n",
    "if RUN_STEP_5:\n",
    "    @torch.inference_mode()\n",
    "    def accept_one(cur: torch.Tensor, token: int) -> Tuple[torch.Tensor, bool]:\n",
    "        \"\"\"그리디 vs 브랜치 토큰 1개 비교 후 (일치=accept, 불일치=stop)\"\"\"\n",
    "        g = greedy_next(cur)\n",
    "        if cfg.DEBUG:\n",
    "            print(f\"    compare {token}({tok.decode([token])!r}) vs greedy={g}({tok.decode([g])!r})\")\n",
    "        if g == token:\n",
    "            return append_token(cur, token), True   # 계속\n",
    "        else:\n",
    "            return append_token(cur, g), False      # 불일치 → 종료\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def prefix_accept_once(ids: torch.Tensor, branch: List[int]) -> Tuple[torch.Tensor, int]:\n",
    "        cur = ids.clone()\n",
    "        accepted = 0\n",
    "        for t in branch:\n",
    "            cur, ok = accept_one(cur, t)\n",
    "            if ok:\n",
    "                accepted += 1\n",
    "            else:\n",
    "                break\n",
    "        return cur, accepted\n",
    "\n",
    "    print(\"[STEP5] prefix-accept ready: accept_one/prefix_accept_once\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61196b25-f992-4e4b-8f95-9cc32a95ab6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP6] loops ready: medusa_loop_step/medusa_tiny/run_greedy\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 6 — 루프 실행 (초초 쪼갬)\n",
    "# =============================\n",
    "if RUN_STEP_6:\n",
    "    @torch.inference_mode()\n",
    "    def medusa_loop_step(ids: torch.Tensor, span: int, temperature: float,\n",
    "                         top_p: float, accepted_total: int, loop_idx: int):\n",
    "        \"\"\"한 번의 루프: 브랜치 제안 -> prefix-accept 적용\"\"\"\n",
    "        if cfg.DEBUG:\n",
    "            print(f\"[loop {loop_idx}] cur_len={ids.shape[1]}\")\n",
    "        branch = propose_branch(ids, span, temperature, top_p, accepted_total)\n",
    "        ids, acc = prefix_accept_once(ids, branch)\n",
    "        return ids, acc\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def medusa_tiny(prompt: str, max_new_tokens: int, span: int,\n",
    "                    temperature: float, top_p: float) -> str:\n",
    "        ids = encode(prompt)\n",
    "        start = ids.shape[1]\n",
    "        accepted_total = 0\n",
    "        loop_idx = 0\n",
    "        while ids.shape[1] - start < max_new_tokens:\n",
    "            ids, acc = medusa_loop_step(ids, span, temperature, top_p,\n",
    "                                        accepted_total, loop_idx)\n",
    "            accepted_total += acc\n",
    "            loop_idx += 1\n",
    "        return decode(ids)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def run_greedy(prompt: str, max_new_tokens: int) -> str:\n",
    "        ids = encode(prompt)\n",
    "        start = ids.shape[1]\n",
    "        while ids.shape[1] - start < max_new_tokens:\n",
    "            nxt = greedy_next(ids)\n",
    "            if EOS_ID is not None and nxt == EOS_ID:\n",
    "                break\n",
    "            ids = append_token(ids, nxt)\n",
    "        return decode(ids)\n",
    "\n",
    "    print(\"[STEP6] loops ready: medusa_loop_step/medusa_tiny/run_greedy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd42e98f-b474-4faa-a90f-9088aa9bb884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Greedy ===\n",
      "In a distant future,   the world is a place where the world is a place where the world is a place where the world is a place where the world is a place \n",
      "\n",
      "=== Medusa-tiny ===\n",
      "In a distant future,   the world is a place where the world is a place where the world is a place where the world is a place where the world is a place where the\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 7 — 데모 (Greedy vs Medusa 결과)\n",
    "# =============================\n",
    "if RUN_STEP_7:\n",
    "    prompt = \"In a distant future, \"\n",
    "    print(\"\\n=== Greedy ===\")\n",
    "    print(run_greedy(prompt, cfg.MAX_NEW_TOKENS), \"\\n\")\n",
    "\n",
    "    print(\"=== Medusa-tiny ===\")\n",
    "    print(medusa_tiny(prompt, cfg.MAX_NEW_TOKENS,\n",
    "                      cfg.SPAN, cfg.TEMPERATURE, cfg.TOP_P))\n",
    "\n",
    "    # 디버그 보고 싶으면 아래 두 줄로 토글 후 재실행\n",
    "    # cfg.DEBUG = True\n",
    "    # print(medusa_tiny(prompt, cfg.MAX_NEW_TOKENS,\n",
    "    #                   cfg.SPAN, cfg.TEMPERATURE, cfg.TOP_P))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a1d15-6d02-4975-b8ad-6f9c683b3727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2175f90-7a45-43a7-ba5c-50038086be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
