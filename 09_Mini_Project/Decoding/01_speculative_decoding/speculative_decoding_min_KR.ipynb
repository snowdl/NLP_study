{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9142b1f5-3ce0-48a3-bb82-769079447061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install & Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "86805c62-3986-4ace-a88f-422bb2eeb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4fd98278-dc91-4b37-b9c6-eecb5531d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ device: mps\n"
     ]
    }
   ],
   "source": [
    "# Pick device (Apple Silicon → mps, else cuda if available, else cpu)\n",
    "device = (\n",
    "    \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"✅ device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a81aefb-a833-4c45-8d85-aea72bb9152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft (fast, tiny) and Target (a bit larger)\n",
    "draft_id  = \"distilgpt2\"\n",
    "target_id = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5de1993b-12be-4569-89dd-3999fd299985",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(target_id)\n",
    "tok.pad_token = tok.eos_token  # silence warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d819d900-1441-419a-82ba-4b95e8305937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft  = AutoModelForCausalLM.from_pretrained(draft_id)\n",
    "draft  = draft.to(device)\n",
    "draft.eval()\n",
    "\n",
    "target = AutoModelForCausalLM.from_pretrained(target_id)\n",
    "target = target.to(device)\n",
    "target.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b025984-a8cd-4e3e-b836-48b61ecb6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: reproducibility\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a5cd3d98-69de-4d78-a542-7390b3d3b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 Draft → propose ONE token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bcdaf90b-9cae-42c9-b8cd-b0241346baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d7831be6-0c36-4aa2-8887-820d69b7142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Draft → propose ONE token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0876a9b2-53fe-417e-b91a-ee640d19aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def draft_next_token(input_ids: torch.Tensor) -> Tuple[torch.Tensor, int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    draft 모델로 다음 토큰 1개를 greedy로 제안하고,\n",
    "    그 토큰을 input_ids에 바로 이어붙여 반환.\n",
    "\n",
    "    Args:\n",
    "        input_ids: [1, T] 토큰 IDs (draft와 같은 device)\n",
    "\n",
    "    Returns:\n",
    "        new_ids : [1, T+1]  이어붙인 시퀀스\n",
    "        token_id: int       제안된 토큰 ID (스칼라)\n",
    "        logits  : [1, V]    마지막 스텝 로짓(softmax 전)\n",
    "    \"\"\"\n",
    "    # 1) forward\n",
    "    out = draft(input_ids=input_ids)          # out.logits: [1, T, V]\n",
    "\n",
    "    # 2) 마지막 스텝 로짓만 추출\n",
    "    logits = out.logits[:, -1, :]             # [1, V]\n",
    "\n",
    "    # 3) greedy로 다음 토큰 선택\n",
    "    next_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "    # 4) 선택 토큰을 시퀀스에 이어붙이기\n",
    "    new_ids = torch.cat([input_ids, next_id], dim=1)      # [1, T+1]\n",
    "\n",
    "    return new_ids, int(next_id.item()), logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14cab615-4a6d-4ab6-94dd-e9923a475fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added token: 262 ' the'\n",
      "new shape: torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a quiet village by the sea,\"\n",
    "enc = tok(prompt, return_tensors=\"pt\")\n",
    "ids = enc.input_ids.to(device)                # [1, T]\n",
    "\n",
    "ids, token_id, logits = draft_next_token(ids)\n",
    "print(\"added token:\", token_id, repr(tok.decode([token_id])))\n",
    "print(\"new shape:\", ids.shape)                # [1, T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1804b605-bd21-41d3-a6a8-8bb254654c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Draft → propose K tokens  (draft_next_token이 (new_ids, int, logits) 를 반환하는 버전과 호환)\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def draft_propose_k(input_ids: torch.Tensor, k: int = 4, temperature: float = 0.8) -> List[int]:\n",
    "    \"\"\"\n",
    "    Greedy로 K 토큰을 연속 제안.\n",
    "    NOTE: draft_next_token이 (new_ids, token_id, logits)를 반환하므로\n",
    "          여기서는 이어붙임을 직접 하지 않고, draft_next_token이 돌려준 new_ids를 사용합니다.\n",
    "    \"\"\"\n",
    "    ids = input_ids.clone()\n",
    "    proposals: List[int] = []\n",
    "    for _ in range(k):\n",
    "        ids, nid, _ = draft_next_token(ids)     # ← 3개 언패킹 (이어붙인 ids가 돌아옴)\n",
    "        proposals.append(nid)\n",
    "    return proposals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d957f9ea-ac00-491f-aaec-33d695228642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Target → top-1 token (Jupyter cell)\n",
    "\n",
    "@torch.no_grad()\n",
    "def target_top1(input_ids: torch.Tensor) -> int:\n",
    "    out = target(input_ids=input_ids)      # [1, T, V]\n",
    "    logits = out.logits[:, -1, :]          # [1, V]\n",
    "    return int(torch.argmax(logits, dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4a927d7-cf7a-4479-9b00-90d5ed984841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Target → sample ONE token on reject (Jupyter cell)\n",
    "\n",
    "@torch.no_grad()\n",
    "def target_sample_one(input_ids: torch.Tensor, temperature: float = 0.7) -> int:\n",
    "    out = target(input_ids=input_ids)\n",
    "    logits = out.logits[:, -1, :]\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    next_id = torch.multinomial(probs, num_samples=1)\n",
    "    return int(next_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8a9b391c-12a1-4c3d-83b4-3ffe35ce94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Verify one cycle \n",
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "@torch.no_grad()\n",
    "def verify_one_cycle(input_ids: torch.Tensor, proposed: List[int], temperature: float = 0.7) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Iterate over proposed tokens:\n",
    "      - If target top-1 == proposed token → accept (append).\n",
    "      - Else → sample ONE token from target, append it, and STOP this cycle.\n",
    "    Returns: (new_input_ids, num_accepted_in_this_cycle)\n",
    "    \"\"\"\n",
    "    ids = input_ids.clone()\n",
    "    accepted = 0\n",
    "    for t in proposed:\n",
    "        top1 = target_top1(ids)\n",
    "        if top1 == t:  # accept\n",
    "            ids = torch.cat([ids, torch.tensor([[t]], device=ids.device)], dim=1)\n",
    "            accepted += 1\n",
    "        else:          # reject → sample 1 and stop cycle\n",
    "            samp = target_sample_one(ids, temperature=temperature)\n",
    "            ids = torch.cat([ids, torch.tensor([[samp]], device=ids.device)], dim=1)\n",
    "            break\n",
    "    return ids, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e9d97f9-8d2c-4263-af0f-5bd4137587c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Speculative loop (Jupyter cell)\n",
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def speculative_generate_minimal(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 60,\n",
    "    k: int = 4,\n",
    "    draft_temp: float = 0.8,   # 전달은 하지만 draft_propose_k에서 실제로는 무시(greedy 버전)\n",
    "    target_temp: float = 0.7,  # verify_one_cycle에서 사용\n",
    ") -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Minimal speculative decoding 루프:\n",
    "      1) draft가 K개 토큰을 제안(현재 draft_propose_k는 greedy이므로 temperature를 무시)\n",
    "      2) target이 순차 검증(일치하면 채택, 불일치 시 1토큰 생성 후 사이클 종료)\n",
    "      3) 예산(max_new_tokens)에 도달할 때까지 반복\n",
    "\n",
    "    Returns:\n",
    "        (생성 텍스트, 총 수락된 토큰 수)\n",
    "    \"\"\"\n",
    "    enc = tok(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    base_len = input_ids.shape[1]\n",
    "    total_accepted = 0\n",
    "\n",
    "    while (input_ids.shape[1] - base_len) < max_new_tokens:\n",
    "        # 1) 제안: draft_propose_k는 시그니처상 temperature를 받지만 현재 구현은 greedy라 내부에서 사용하지 않음\n",
    "        proposed = draft_propose_k(input_ids, k=k, temperature=draft_temp)\n",
    "\n",
    "        # 2) 검증/병합: target이 제안 토큰을 확인 (여기서 target_temp는 실제 사용)\n",
    "        input_ids, acc = verify_one_cycle(input_ids, proposed, temperature=target_temp)\n",
    "        total_accepted += acc\n",
    "\n",
    "        # 3) 안전장치: 이번 사이클에서 아무것도 수락되지 않았다면 target top-1로 한 스텝 전진\n",
    "        if acc == 0 and (input_ids.shape[1] - base_len) < max_new_tokens:\n",
    "            nid = target_top1(input_ids)\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[nid]], device=device)], dim=1)\n",
    "\n",
    "    text = tok.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return text, total_accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d3961525-e914-4530-b9c4-085cc452b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Prompt: In a quiet village by the sea,\n",
      "✅ Accepted tokens (by target): 32\n",
      "\n",
      "=== Output ===\n",
      " In a quiet village by the sea, the village is a mess. A few soldiers are stationed there, but none of them are there to help.\n",
      "\n",
      "\"I'm not going to say anything! I'm just going to say what I want to say!\"\n",
      "\n",
      "\"I'm going to say what I want to say!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.7 Run test\n",
    "\n",
    "prompt = \"In a quiet village by the sea,\"\n",
    "text, accepted = speculative_generate_minimal(\n",
    "    prompt, max_new_tokens=60, k=3, draft_temp=0.7, target_temp=0.7\n",
    ")\n",
    "\n",
    "print(\"📝 Prompt:\", prompt)\n",
    "print(\"✅ Accepted tokens (by target):\", accepted)\n",
    "print(\"\\n=== Output ===\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f0146-3ffb-41b2-be7b-6bfd3bacc176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
