{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "e46353b7-7078-415f-9040-3f20f26b8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medusa-lite flow : drafter ‚Üí verifier ‚Üí multi-branch prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab43540-a679-4dfc-bea9-767a75607adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b722306-3387-4100-a24c-ac353774f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Device selection\n",
    "\n",
    "def pick_device():\n",
    "    # Check if Apple Silicon (MPS) is available\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"     # Use MPS on Mac\n",
    "    # Otherwise, check if CUDA GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"    # Use CUDA if available\n",
    "    # Fallback to CPU if no GPU/MPS is found\n",
    "    return \"cpu\"         \n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"‚úÖ DEVICE =\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4d20be-51af-47dc-b8ed-c2d34cf39cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert DEVICE in {\"cpu\", \"cuda\", \"mps\"}\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd100859-26a7-428d-b08c-8f99c54e7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed Í≥†Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f0a711-b21e-4678-93c3-14d51d3c550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch, random\n",
    "\n",
    "# Set seeds for reproducibility \n",
    "# (not critical if sampling is not used, but still good practice)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "7d7b0024-611a-4f72-ac17-ba8b68b9b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d19e12-3cde-40b6-9910-5a95ecaa3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # IDs of the models used\n",
    "    DRAFTER_ID: str = \"distilgpt2\"      # Small, fast draft model\n",
    "    VERIFIER_ID: str = \"gpt2-medium\"    # Larger, more accurate verifier model\n",
    "\n",
    "    # Generation length\n",
    "    MAX_NEW_TOKENS: int = 30            # Maximum number of tokens to generate\n",
    "\n",
    "    # Sampling parameters\n",
    "    TEMPERATURE: float = 0.8            # Controls randomness (lower ‚Üí more deterministic)\n",
    "    TOP_P: float = 0.9                  # Nucleus sampling (probability mass cutoff)\n",
    "\n",
    "    # Repetition control\n",
    "    REPETITION_PENALTY: float = 1.3     # Penalize repeating tokens\n",
    "    NO_REPEAT_NGRAM: int = 5            # Prevent repeating n-grams of size 5\n",
    "\n",
    "    # Speculative decoding settings\n",
    "    TOPK_BRANCH: int = 4                # How many draft tokens to branch for verification\n",
    "    DRAFT_SPAN: int = 3                 # Number of tokens the drafter proposes at once\n",
    "\n",
    "    # Runtime settings\n",
    "    DEVICE: str = DEVICE                # Device to run on (mps / cuda / cpu)\n",
    "    DEBUG: bool = False                 # Debug mode toggle\n",
    "\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c043eb52-3954-4c4c-9f2a-7e5a53e8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Draft and Verifier Models (Tokenizer ‚Üí Model) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16d4bd6-e6eb-4989-b3fe-f3f6c77a1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ models ready: distilgpt2 / gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizers for drafter and verifier models\n",
    "drafter_tok  = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "\n",
    "# üîß Fix for GPT-2 family: often eos_token / pad_token are missing\n",
    "if verifier_tok.eos_token_id is None:\n",
    "    verifier_tok.eos_token = \"\"   # Add EOS token if missing\n",
    "if verifier_tok.pad_token_id is None:\n",
    "    verifier_tok.pad_token = verifier_tok.eos_token  # Use EOS as padding if missing\n",
    "\n",
    "# Save EOS token ID for reference\n",
    "EOS_ID = verifier_tok.eos_token_id\n",
    "\n",
    "# Load models and move them to the chosen device\n",
    "drafter  = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE).eval()\n",
    "verifier = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE).eval()\n",
    "\n",
    "# Ensure caching is enabled (default is True, but set explicitly)\n",
    "drafter.config.use_cache  = True\n",
    "verifier.config.use_cache = True\n",
    "\n",
    "print(\"‚úÖ models ready:\", cfg.DRAFTER_ID, \"/\", cfg.VERIFIER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8be34b4-8927-4182-bbe4-99e2275c1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4) Prompt & Context Preparation ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeb72db2-cd74-48aa-a0a3-70d618b1c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ok? True | shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# Define the initial prompt text\n",
    "prompt = \"In a distant future, a small crew of explorers discovers \"\n",
    "\n",
    "# Encode the prompt with the drafter tokenizer\n",
    "# and move the tensor to the selected DEVICE (mps / cuda / cpu)\n",
    "ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "\n",
    "# Extract only the input_ids (token IDs for the prompt)\n",
    "input_ids = ctx[\"input_ids\"]\n",
    "\n",
    "# Debug print: confirm context preparation and tensor shape\n",
    "print(\"context ok?\", ctx is not None, \"| shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8516d202-1746-4430-9347-17506532cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function: Draft k candidate tokens ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a23bc6a6-8263-4f83-8858-1c9d93865d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()  # Disable gradient calculation for efficiency\n",
    "def drafter_sample_first_tokens_basic(model, ids, k: int, temperature: float = 0.8):\n",
    "    # Forward pass ‚Üí get logits for the last token position\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    \n",
    "    # Apply temperature scaling + softmax to convert logits into probabilities\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "    \n",
    "    # Ensure k does not exceed vocabulary size\n",
    "    k = min(k, probs.numel())\n",
    "    \n",
    "    # Sample k distinct token IDs (multinomial sampling without replacement)\n",
    "    picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "    \n",
    "    # Return as a Python list of integers\n",
    "    return [int(i) for i in picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca643d6-c40b-42a0-abc4-4d98157578d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token str (repr): '\\xa0'\n",
      "gpt2 piece: √Ç≈Ç\n",
      "is space? True\n"
     ]
    }
   ],
   "source": [
    "# === Debug: Inspect a single token ID ===\n",
    "tid = 1849\n",
    "\n",
    "# Decode the token ID back into a string (repr shows invisible characters)\n",
    "print(\"token str (repr):\", repr(drafter_tok.decode([tid])))\n",
    "\n",
    "# Show the raw GPT-2 subword token (BPE piece)\n",
    "print(\"gpt2 piece:\", drafter_tok.convert_ids_to_tokens([tid])[0])\n",
    "\n",
    "# Check if the decoded token is only whitespace\n",
    "print(\"is space?\", drafter_tok.decode([tid]).isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "2392dfbb-de90-4c85-93cc-324c928f2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Î©ÄÌã∞-Î∏åÎûúÏπò Draft Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "052dc313-d5f5-44be-9148-b5472f7684fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Optional\n",
    "\n",
    "# =========================\n",
    "# Small, focused utilities\n",
    "# =========================\n",
    "\n",
    "@torch.inference_mode()\n",
    "def last_token_logits(model, ids: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run a forward pass and return logits for the last time step.\n",
    "    ids: [B, T] LongTensor on the same device as the model.\n",
    "    returns: [V] 1D logits for the last position (batch assumed 1).\n",
    "    \"\"\"\n",
    "    out = model(ids)\n",
    "    # shape: [B, T, V] ‚Üí take last step, squeeze batch\n",
    "    return out.logits[:, -1, :][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b187f5a-7e69-4d11-a14c-58eb2f6c5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply temperature scaling and softmax.\n",
    "    Clamps temperature to a minimum to avoid division by zero.\n",
    "    returns: probability vector over vocabulary [V].\n",
    "    \"\"\"\n",
    "    t = max(float(temperature), 1e-6)\n",
    "    return torch.softmax(logits / t, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cd301e3-b7dd-4a1a-9457-957584c8bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_top_p_filter(probs: torch.Tensor, top_p: Optional[float]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return the indices of tokens inside the nucleus (top-p) set.\n",
    "    - Sort by prob desc, take smallest prefix whose cumulative prob ‚â§ top_p.\n",
    "    - Always keep at least the top-1 token.\n",
    "    If top_p is None, returns all indices (torch.arange(V)).\n",
    "    \"\"\"\n",
    "    V = probs.numel()\n",
    "    if top_p is None:\n",
    "        return torch.arange(V, device=probs.device)\n",
    "\n",
    "    # Bound top_p into (0, 1]; treat <=0 as keep only top-1, >1 as keep all.\n",
    "    if top_p <= 0:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        return sorted_ix[:1]\n",
    "    if top_p >= 1:\n",
    "        return torch.arange(V, device=probs.device)\n",
    "\n",
    "    sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "    cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "    keep_mask = cumsum <= top_p\n",
    "    # Ensure at least one token remains\n",
    "    keep_mask[0] = True\n",
    "    return sorted_ix[keep_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e530cc0-28f1-4ec0-9016-1031e05926c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample_without_replacement(probs: torch.Tensor, k: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Multinomial sampling WITHOUT replacement from probs.\n",
    "    Caps k by available items and returns Python ints.\n",
    "    \"\"\"\n",
    "    k = max(0, min(int(k), probs.numel()))\n",
    "    if k == 0:\n",
    "        return []\n",
    "    picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "    return [int(i) for i in picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80f1d851-ce61-4d61-a69e-60a483c28f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_token(ids: torch.Tensor, tok_id: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Append a single token id to a [1, T] LongTensor on same device/dtype.\n",
    "    \"\"\"\n",
    "    tok = torch.tensor([[tok_id]], dtype=ids.dtype, device=ids.device)\n",
    "    return torch.cat([ids, tok], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f32a565-d369-4e15-af92-bf8d39b0d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Drafting: first-token + greedy rollout\n",
    "# ======================================\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens_basic(model, ids: torch.Tensor, k: int,\n",
    "                                      temperature: float = 0.8) -> List[int]:\n",
    "    \"\"\"\n",
    "    Simple temperature sampling over the full vocab (no top-p).\n",
    "    \"\"\"\n",
    "    logits = last_token_logits(model, ids)\n",
    "    probs = softmax_with_temperature(logits, temperature)\n",
    "    return sample_without_replacement(probs, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc396657-2aff-4597-8ef5-972d76123d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids: torch.Tensor, k: int,\n",
    "                                temperature: float = 0.8,\n",
    "                                top_p: Optional[float] = 0.9) -> List[int]:\n",
    "    \"\"\"\n",
    "    Nucleus (top-p) sampling for the FIRST next-token proposals.\n",
    "    \"\"\"\n",
    "    logits = last_token_logits(model, ids)\n",
    "    probs = softmax_with_temperature(logits, temperature)\n",
    "    pool_ix = safe_top_p_filter(probs, top_p)\n",
    "    pool_probs = probs[pool_ix]\n",
    "    pool_probs = pool_probs / pool_probs.sum()  # renormalize\n",
    "    k = min(k, pool_ix.numel())\n",
    "    if k == 0:\n",
    "        return []\n",
    "    picks_local = torch.multinomial(pool_probs, num_samples=k, replacement=False)\n",
    "    return [int(pool_ix[i]) for i in picks_local]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "362227c1-99bc-4f8b-8974-6d503a0da86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_rollout_greedy(model, ids: torch.Tensor,\n",
    "                           first_tok: int, span: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Greedy rollout for 'span' tokens starting with 'first_tok'.\n",
    "    The first token is fixed; subsequent tokens use argmax.\n",
    "    \"\"\"\n",
    "    span = max(1, int(span))\n",
    "    cur = append_token(ids, first_tok)\n",
    "    seq = [first_tok]\n",
    "\n",
    "    for _ in range(span - 1):\n",
    "        logits = last_token_logits(model, cur)\n",
    "        nxt = int(torch.argmax(logits).item())\n",
    "        seq.append(nxt)\n",
    "        cur = append_token(cur, nxt)\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ede50e6-aa32-4705-a354-c7fad89effbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_propose(ids: torch.Tensor, k: int, span: int,\n",
    "                    temperature: float = 0.8, top_p: Optional[float] = 0.9) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Propose K branches:\n",
    "      1) sample K first tokens (nucleus + temperature)\n",
    "      2) greedy rollout for remaining (span-1) steps per branch\n",
    "    Uses the global 'drafter' model and current cfg.* settings.\n",
    "    \"\"\"\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    return [drafter_rollout_greedy(drafter, ids, f, span) for f in firsts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "93eaab97-53d5-4d35-9772-a90b97043a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verifier: predict one token (greedy) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a602aa44-d7b8-4cdf-a48d-9c8a37fd274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def verifier_next_token(ids) -> int:\n",
    "    \"\"\"\n",
    "    Use the verifier model to predict the next token ID greedily.\n",
    "    - Runs a forward pass on the current ids\n",
    "    - Takes the last-step logits\n",
    "    - Returns the argmax token ID as int\n",
    "    \"\"\"\n",
    "    logits = verifier(ids).logits[:, -1, :]\n",
    "    return int(torch.argmax(logits, dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df8e4703-5107-4707-80f6-2dca543b9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pretty-print token information ===\n",
    "def pretty_token(tokenizer, tid: int):\n",
    "    \"\"\"\n",
    "    Convert a token ID into multiple human-readable formats for debugging.\n",
    "    \n",
    "    Returns a dict with:\n",
    "      - \"id\": the token ID (int)\n",
    "      - \"decode_repr\": decoded string (repr to reveal hidden chars, e.g., '\\xa0')\n",
    "      - \"token_repr\": raw BPE token string (repr form)\n",
    "      - \"token_fixed\": attempt to fix mojibake via latin1 ‚Üí utf-8 roundtrip\n",
    "      - \"codepoints\": list of Unicode codepoints in hex\n",
    "      - \"bytes\": list of raw UTF-8 bytes\n",
    "    \"\"\"\n",
    "    # Decode the token ID into text (keep special tokens and spaces)\n",
    "    s_decode = tokenizer.decode([tid],\n",
    "                                skip_special_tokens=False,\n",
    "                                clean_up_tokenization_spaces=False)\n",
    "\n",
    "    # Get the raw BPE subword token string\n",
    "    s_token = tokenizer.convert_ids_to_tokens([tid])[0]\n",
    "\n",
    "    # Try to re-encode/decode to fix potential mojibake (encoding artifacts)\n",
    "    try:\n",
    "        s_fixed = s_token.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        s_fixed = s_token\n",
    "\n",
    "    return {\n",
    "        \"id\": tid,\n",
    "        \"decode_repr\": repr(s_decode),   # decoded string, repr shows hidden chars\n",
    "        \"token_repr\": repr(s_token),     # raw token string as stored by tokenizer\n",
    "        \"token_fixed\": repr(s_fixed),    # mojibake-fixed token string\n",
    "        \"codepoints\": [hex(ord(c)) for c in s_decode],  # Unicode codepoints\n",
    "        \"bytes\": list(s_decode.encode(\"utf-8\")),        # raw UTF-8 bytes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6abed687-dd54-4c6f-b1dc-895d46f33f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def next_human_token(ids, tokenizer, tries=10):\n",
    "    \"\"\"\n",
    "    Predict tokens with the verifier until a *human-visible* token appears.\n",
    "    \n",
    "    A token is considered \"human-visible\" if:\n",
    "      - It contains at least one printable character\n",
    "      - That character is not just whitespace\n",
    "    \n",
    "    Args:\n",
    "        ids: current input sequence (tensor [1, T])\n",
    "        tokenizer: the tokenizer used for decoding\n",
    "        tries: maximum number of attempts before giving up\n",
    "    \n",
    "    Returns:\n",
    "        (tid, s) ‚Üí the token ID and its decoded string\n",
    "    \"\"\"\n",
    "    cur = ids.clone()\n",
    "    for _ in range(tries):\n",
    "        # Predict next token (greedy)\n",
    "        tid = verifier_next_token(cur)\n",
    "\n",
    "        # Decode into a string without skipping special tokens\n",
    "        s = tokenizer.decode(\n",
    "            [tid],\n",
    "            skip_special_tokens=False,\n",
    "            clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        # If the decoded string contains a visible (printable, non-space) character, return\n",
    "        if any(ch.isprintable() and not ch.isspace() for ch in s):\n",
    "            return tid, s\n",
    "\n",
    "        # Otherwise, append token and continue searching\n",
    "        cur = torch.cat([cur, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "\n",
    "    # If no human-visible token found after max tries, return the last one\n",
    "    return tid, s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f30ef52a-82c6-4466-90e6-a8ad6d4c1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token info: {'id': 488, 'decode_repr': \"'ich'\", 'token_repr': \"'ich'\", 'token_fixed': \"'ich'\", 'codepoints': ['0x69', '0x63', '0x68'], 'bytes': [105, 99, 104]}\n",
      "Next human-visible token: 488 'ich'\n"
     ]
    }
   ],
   "source": [
    "# === Example usage ===\n",
    "vid = verifier_next_token(input_ids)\n",
    "info = pretty_token(verifier_tok, vid)\n",
    "\n",
    "print(\"Predicted token info:\", info)\n",
    "\n",
    "t2, s2 = next_human_token(input_ids, verifier_tok)\n",
    "print(\"Next human-visible token:\", t2, repr(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "42acd454-c409-4673-8d3b-c63d24b3c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix-Accept (mismatchÍπåÏßÄ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02cdbb53-4055-490f-80b6-b70f3d967954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch(context_ids, branch_tokens:List[int]) -> Tuple[torch.Tensor, List[int], bool]:\n",
    "    ids = context_ids.clone()\n",
    "    accepted = []\n",
    "    mismatched = False\n",
    "    for tid in branch_tokens:\n",
    "        pred = verifier_next_token(ids)\n",
    "        if pred == tid:\n",
    "            ids = torch.cat([ids, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "            accepted.append(tid)\n",
    "        else:\n",
    "            ids = torch.cat([ids, torch.tensor([[pred]], device=ids.device)], dim=1)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return ids, accepted, mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34fd27c9-b087-4f7f-b4a6-fe7def8c1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted len: 0 | mismatched? True\n",
      "new length: 13 | tokens added: 1\n"
     ]
    }
   ],
   "source": [
    "# Generate proposed branches from drafter\n",
    "branches = drafter_propose(\n",
    "    input_ids,\n",
    "    k=cfg.TOPK_BRANCH,\n",
    "    span=cfg.DRAFT_SPAN,\n",
    "    temperature=cfg.TEMPERATURE,\n",
    "    top_p=cfg.TOP_P\n",
    ")\n",
    "\n",
    "# Test prefix-accept on the first branch\n",
    "new_ids, accepted, mism = accept_until_mismatch(input_ids, branches[0])\n",
    "\n",
    "print(\"accepted len:\", len(accepted), \"| mismatched?\", mism)\n",
    "print(\"new length:\", new_ids.shape[1],\n",
    "      \"| tokens added:\", new_ids.shape[1] - input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b938a293-6969-4edb-8784-0e1b34b473a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Branch Scoring Utility ===\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "510d8429-152b-42b2-ab3d-46c2994a0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "ee1be8b8-5047-4ee2-adde-26409e63722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch Ï†êÏàò Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8cea79e-4e6c-474b-895c-e745b078be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_branch(accepted, mismatched):\n",
    "    \"\"\"\n",
    "    Score a drafter branch based on prefix-accept results.\n",
    "\n",
    "    Args:\n",
    "        accepted: list of tokens accepted before mismatch\n",
    "        mismatched: bool, True if a mismatch occurred\n",
    "\n",
    "    Returns:\n",
    "        int ‚Üí simple score = (#accepted tokens) - (1 if mismatch happened else 0)\n",
    "\n",
    "    Intuition:\n",
    "        - Longer accepted prefix ‚Üí higher score\n",
    "        - If mismatch occurred ‚Üí apply small penalty (-1)\n",
    "    \"\"\"\n",
    "    return len(accepted) - (1 if mismatched else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7227c78d-ee2e-454c-b9a4-baa43a3d7aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Quick checks\n",
    "print(score_branch([1,2,3], False))  # 3 (3 accepted, no penalty)\n",
    "print(score_branch([1,2], True))     # 1 (2 accepted, -1 penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "f6414314-abf1-484f-9a9a-d782b3773b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Encode Prompt into Token IDs ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "757a2021-07af-452b-b469-d550e46e48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def encode_prompt(prompt: str):\n",
    "    \"\"\"\n",
    "    Tokenize a text prompt using the drafter tokenizer and\n",
    "    move it to the configured device (mps/cuda/cpu).\n",
    "\n",
    "    Args:\n",
    "        prompt: input string\n",
    "\n",
    "    Returns:\n",
    "        input_ids: tensor of shape [1, T] with token IDs\n",
    "    \"\"\"\n",
    "    ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    return ctx[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "512fc442-7b76-4fb3-9ffd-c618d859642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids.shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "ids = encode_prompt(\"In a distant future, \")\n",
    "print(\"ids.shape:\", ids.shape)   # e.g. torch.Size([1, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "9c5a09ad-485a-4887-9fd8-c623132712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ìïú Ïä§ÌÖù ÏàòÌñâ(multi-branch‚ÜíÍ≤ÄÏ¶ù‚ÜíÏµúÍ≥† Ï†êÏàò Ï±ÑÌÉù)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9036549-1494-49c6-b2cb-0c55ea2ce21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === One Medusa Step ===\n",
    "@torch.inference_mode()\n",
    "def medusa_step(ids, topk_branch: int, draft_span: int, temperature: float):\n",
    "    \"\"\"\n",
    "    Perform one Medusa decoding step:\n",
    "      1) Drafter proposes multiple candidate branches\n",
    "      2) Each branch is verified with prefix-accept\n",
    "      3) Branches are scored (longer accepted prefix is better; mismatch penalized)\n",
    "      4) Best-scoring branch is chosen and returned\n",
    "\n",
    "    Args:\n",
    "        ids: tensor [1, T] ‚Üí current context sequence\n",
    "        topk_branch: number of branches to propose\n",
    "        draft_span: number of tokens per branch\n",
    "        temperature: sampling temperature (‚â•0.9 enforced for diversity)\n",
    "\n",
    "    Returns:\n",
    "        best_ids: updated context tensor after accepting one branch\n",
    "    \"\"\"\n",
    "    # Drafter proposes branches with stronger sampling (top-p = 0.95)\n",
    "    branches = drafter_propose(\n",
    "        ids,\n",
    "        topk_branch,\n",
    "        draft_span,\n",
    "        temperature=max(0.9, float(temperature)),  # force ‚â• 0.9 for diversity\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    # Select the branch with the highest score\n",
    "    best_score = -10**9\n",
    "    best_ids = None\n",
    "    for br in branches:\n",
    "        new_ids, accepted, mism = accept_until_mismatch(ids, br)\n",
    "        s = score_branch(accepted, mism)\n",
    "        if s > best_score:\n",
    "            best_score, best_ids = s, new_ids\n",
    "\n",
    "    return best_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "689ee0dd-6059-4bcd-9556-fdf4a4b5d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 6 ‚Üí after: 8\n"
     ]
    }
   ],
   "source": [
    "ids2 = medusa_step(ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(\"before:\", ids.shape[1], \"‚Üí after:\", ids2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "21a7e22b-51dd-4218-82f7-20555827dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21accee7-8bde-4019-a992-5255ca3a5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Full Orchestrator: Medusa Generate ===\n",
    "@torch.inference_mode()\n",
    "def medusa_generate(prompt: str,\n",
    "                    max_new_tokens: int = None,\n",
    "                    topk_branch: int = None,\n",
    "                    draft_span: int = None,\n",
    "                    temperature: float = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Medusa decoding (drafter + verifier + prefix-accept).\n",
    "\n",
    "    Args:\n",
    "        prompt: starting string\n",
    "        max_new_tokens: max number of tokens to add\n",
    "        topk_branch: number of branches to propose per step\n",
    "        draft_span: number of tokens per branch\n",
    "        temperature: sampling temperature for drafter\n",
    "\n",
    "    Returns:\n",
    "        Decoded string (str) including the prompt and generated text\n",
    "    \"\"\"\n",
    "    # Fill with defaults from cfg if not specified\n",
    "    if max_new_tokens is None: max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "    if topk_branch   is None: topk_branch   = cfg.TOPK_BRANCH\n",
    "    if draft_span    is None: draft_span    = cfg.DRAFT_SPAN\n",
    "    if temperature   is None: temperature   = cfg.TEMPERATURE\n",
    "\n",
    "    # Encode the prompt into token IDs\n",
    "    ids = encode_prompt(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "\n",
    "    # Number of Medusa steps (ceil to cover full length)\n",
    "    steps = math.ceil(max_new_tokens / draft_span)\n",
    "\n",
    "    # Iteratively expand with Medusa steps\n",
    "    for _ in range(steps):\n",
    "        ids = medusa_step(ids, topk_branch, draft_span, temperature)\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "    # Decode back into human-readable text\n",
    "    return drafter_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78601636-7a26-4a4b-a2be-0f1cbd210e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, ¬†the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. ¬†He wants\n"
     ]
    }
   ],
   "source": [
    "# ‚úîÔ∏è Example run\n",
    "out = medusa_generate(\"In a distant future, \", 40)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "9a00ab2b-8e08-4ae9-8277-13002c95529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13) Greedy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3ed7155-d9e8-47b4-a337-4e64f12a4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Baseline: Greedy Decoding with Verifier ===\n",
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using plain greedy decoding with the verifier model.\n",
    "    Acts as a baseline for comparison against Medusa decoding.\n",
    "\n",
    "    Args:\n",
    "        prompt: starting string\n",
    "        max_new_tokens: number of tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Decoded string (prompt + generated text)\n",
    "    \"\"\"\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    # Encode the prompt with verifier tokenizer\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "\n",
    "    # Iteratively add one token at a time (greedy argmax)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = verifier(ids).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])  # pick most likely token\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "    # Decode back to human-readable text\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "200e4cc7-141e-4a4f-b3be-7925962cff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, ¬†the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. ¬†He wants to control the world's resources so that he can rule the world. \n"
     ]
    }
   ],
   "source": [
    "txt = greedy_generate(\"In a distant future, \", 40)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "7b6f4292-a915-4184-8f0b-b279ccffaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) A/B Speed & Text Comparison ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f924eb0c-f5c1-4ac3-a144-e3904c3ffc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è± greedy: 7.495 s\n",
      "‚è± medusa: 9.798 s\n",
      "\n",
      "--- greedy ---\n",
      " In a distant future, ¬†the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. ¬†He wants to control the world's resources so that he can rule the world. ¬†He wants to control the world's resources so that he can rule the world. ¬†He wants to control the world's resources so that he can rule the world. ¬†He wants to\n",
      "\n",
      "--- medusa ---\n",
      " In a distant future, ¬†the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. ¬†He wants to control the world's resources so that he can rule the world. ¬†He wants to control the world's resources so that he can rule the world. ¬†He\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def time_it(fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a function and measure its execution time.\n",
    "    Returns:\n",
    "        (output, elapsed_seconds)\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    return out, time.perf_counter() - t0\n",
    "\n",
    "\n",
    "# Run both generators on the same prompt\n",
    "g_txt, g_t = time_it(greedy_generate, \"In a distant future, \", 80)\n",
    "m_txt, m_t = time_it(medusa_generate, \"In a distant future, \", 80)\n",
    "\n",
    "# Compare runtime\n",
    "print(\"‚è± greedy:\", round(g_t, 3), \"s\")\n",
    "print(\"‚è± medusa:\", round(m_t, 3), \"s\")\n",
    "\n",
    "# Show a preview of outputs (first 400 chars)\n",
    "print(\"\\n--- greedy ---\\n\", g_txt[:400])\n",
    "print(\"\\n--- medusa ---\\n\", m_txt[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66a17-0016-4606-a4dc-03c8dae9450f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc0d46-c787-4048-b06c-a0ee7057924b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
