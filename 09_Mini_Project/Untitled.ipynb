{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68902547-3e53-4233-8d45-aefa61f5fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Minimal setup: imports, device, and seed (modular)\n",
    "# ============================================================\n",
    "import math\n",
    "import random\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f55224c4-8404-4b7f-88f3-9ad156a8645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ device: mps\n"
     ]
    }
   ],
   "source": [
    "# ---------- 0.1 Device ----------\n",
    "def pick_device() -> str:\n",
    "    \"\"\"Pick best available device: MPS (Apple) → CUDA → CPU.\"\"\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"✅ device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fb05545-d05d-439c-b8f7-a57fab08b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 0.2 Seed (optional, for reproducibility) ----------\n",
    "def set_seed(seed: Optional[int] = 42) -> None:\n",
    "    \"\"\"Set random seeds; pass None to skip.\"\"\"\n",
    "    if seed is None:\n",
    "        return\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)  # change to None if you want non-deterministic sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a2333d1-f804-406d-9396-32141bd2d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Tokenizer & Model loaders (GPT-2 family friendly)\n",
    "# ============================================================\n",
    "def load_tokenizer(model_id: str) -> AutoTokenizer:\n",
    "    \"\"\"\n",
    "    Load tokenizer and patch EOS/PAD for GPT-2-like models\n",
    "    (they often lack explicit eos/pad tokens).\n",
    "    \"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tok.eos_token_id is None:\n",
    "        tok.eos_token = \"\"         # set an EOS token text\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "\n",
    "def load_model(model_id: str, device: str) -> AutoModelForCausalLM:\n",
    "    \"\"\"Load causal LM on device with KV-cache enabled.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device).eval()\n",
    "    model.config.use_cache = True\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_ID = \"distilgpt2\"  # small & fast for demos\n",
    "tok = load_tokenizer(MODEL_ID)\n",
    "model = load_model(MODEL_ID, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "41f7ae7f-d214-4a84-b4d8-cd36c52a23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1.1 Special IDs ----------\n",
    "def get_special_ids(tokenizer: AutoTokenizer) -> Tuple[Optional[int], Optional[int]]:\n",
    "    \"\"\"Return (EOS_ID, PAD_ID) from tokenizer.\"\"\"\n",
    "    return tokenizer.eos_token_id, tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "EOS_ID, PAD_ID = get_special_ids(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e118073-22da-4b2f-b850-6a537776c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS_ID=50256, PAD_ID=50256, #BAN_IDS=22\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1-Extra) Ban-list builders (fight NBSP/whitespace loops)\n",
    "# ============================================================\n",
    "def get_nbsp_token_ids(tokenizer: AutoTokenizer) -> List[int]:\n",
    "    \"\"\"\n",
    "    Return ALL token IDs that represent NBSP (U+00A0).\n",
    "    In byte-level BPE it can be 1+ IDs.\n",
    "    \"\"\"\n",
    "    return tokenizer.encode(\"\\u00A0\", add_special_tokens=False)\n",
    "\n",
    "\n",
    "def build_whitespace_banlist(tokenizer: AutoTokenizer) -> List[int]:\n",
    "    \"\"\"\n",
    "    Build a list of token IDs that decode to whitespace-only strings,\n",
    "    EXCLUDING a normal space ' ' (keep regular spaces).\n",
    "    \"\"\"\n",
    "    ban = []\n",
    "    for tid in range(tokenizer.vocab_size):\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        if s and s.isspace() and s != \" \":\n",
    "            ban.append(tid)\n",
    "    return ban\n",
    "\n",
    "\n",
    "# Choose your strategy:\n",
    "#   A) Only NBSP            → safer/targeted\n",
    "#   B) All whitespace-only  → stronger (recommended)\n",
    "BAN_IDS = torch.tensor(build_whitespace_banlist(tok), device=DEVICE, dtype=torch.long)\n",
    "print(f\"EOS_ID={EOS_ID}, PAD_ID={PAD_ID}, #BAN_IDS={BAN_IDS.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21157dc0-831f-47e4-8573-03078e94b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Text I/O helpers (encode/decode/append + normalization)\n",
    "# ============================================================\n",
    "def normalize_nbsp_to_space(text: str) -> str:\n",
    "    \"\"\"Replace NBSP (U+00A0) with a regular space for cleaner display.\"\"\"\n",
    "    return text.replace(\"\\u00A0\", \" \")\n",
    "\n",
    "\n",
    "def encode(prompt: str) -> torch.Tensor:\n",
    "    \"\"\"Text → token IDs on DEVICE. Shape: [1, T].\"\"\"\n",
    "    return tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "\n",
    "\n",
    "def decode(ids: torch.Tensor) -> str:\n",
    "    \"\"\"Token IDs → text (with NBSP normalized).\"\"\"\n",
    "    txt = tok.decode(ids[0], skip_special_tokens=True)\n",
    "    return normalize_nbsp_to_space(txt)\n",
    "\n",
    "\n",
    "def append_token(ids: torch.Tensor, tid: int) -> torch.Tensor:\n",
    "    \"\"\"Append single token ID to [1, T] sequence (preserve dtype/device).\"\"\"\n",
    "    t = torch.tensor([[tid]], dtype=ids.dtype, device=ids.device)\n",
    "    return torch.cat([ids, t], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1180d3bf-76b3-43c8-8ddb-0c4b3390bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Logits utilities (last-step + masking)\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def last_step_logits(ids: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return logits for the last position. Shape: [V].\"\"\"\n",
    "    return model(ids).logits[:, -1, :][0]\n",
    "\n",
    "\n",
    "def mask_eos_(logits: torch.Tensor, ban: bool) -> None:\n",
    "    \"\"\"In-place: forbid EOS by setting its logit to -inf.\"\"\"\n",
    "    if ban and EOS_ID is not None:\n",
    "        logits[EOS_ID] = float(\"-inf\")\n",
    "\n",
    "\n",
    "def mask_bad_tokens_(logits: torch.Tensor, ban_ids: Optional[torch.Tensor]) -> None:\n",
    "    \"\"\"In-place: forbid any token in ban_ids by setting logits to -inf.\"\"\"\n",
    "    if ban_ids is not None and ban_ids.numel() > 0:\n",
    "        logits[ban_ids] = float(\"-inf\")\n",
    "\n",
    "\n",
    "def mask_all_(logits: torch.Tensor, *, ban_eos: bool = False, ban_ids: Optional[torch.Tensor] = None) -> None:\n",
    "    \"\"\"In-place: apply both EOS mask and ban-list mask.\"\"\"\n",
    "    mask_eos_(logits, ban=ban_eos)\n",
    "    mask_bad_tokens_(logits, ban_ids=ban_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9101b925-27ce-4614-90dc-94f188ca0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) Greedy decoding (baseline, EOS-safe + ban-list)\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def greedy_next(ids: torch.Tensor,\n",
    "                *,\n",
    "                ban_eos: bool = False,\n",
    "                ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                repetition_penalty: float = 1.0,\n",
    "                no_repeat_ngram_size: int = 0) -> int:\n",
    "    \"\"\"\n",
    "    Argmax with masks + repetition controls.\n",
    "    \"\"\"\n",
    "    logits = last_step_logits(ids).clone()\n",
    "    # 1) 기본 마스크(EOS/공백 전용 토큰 등)\n",
    "    mask_all_(logits, ban_eos=ban_eos, ban_ids=ban_ids)\n",
    "    # 2) 반복 억제\n",
    "    apply_repetition_penalty_(logits, ids, penalty=repetition_penalty)\n",
    "    # 3) n-그램 반복 금지\n",
    "    apply_no_repeat_ngram_(logits, ids, n=no_repeat_ngram_size)\n",
    "    # 4) 최종 선택\n",
    "    return int(torch.argmax(logits).item())\n",
    "\n",
    "    \n",
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int = 40, *, ban_eos_steps: int = 8,\n",
    "                    ban_ids: Optional[torch.Tensor] = BAN_IDS) -> str:\n",
    "    \"\"\"\n",
    "    Plain greedy decoding with early EOS ban + ban-list.\n",
    "    - For the first `ban_eos_steps`, EOS is forbidden to avoid immediate stop.\n",
    "    \"\"\"\n",
    "    ids = encode(prompt)\n",
    "    for t in range(max_new_tokens):\n",
    "        nxt = greedy_next(ids, ban_eos=(t < ban_eos_steps), ban_ids=ban_ids)\n",
    "        # If EOS appears after ban window, stop\n",
    "        if EOS_ID is not None and nxt == EOS_ID and t >= ban_eos_steps:\n",
    "            break\n",
    "        # If EOS still slips in during the ban window, skip it (rare)\n",
    "        if EOS_ID is not None and nxt == EOS_ID:\n",
    "            continue\n",
    "        ids = append_token(ids, nxt)\n",
    "    return decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "edccf6fe-90bc-4112-b7b0-3cd5fffd7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Sampling building blocks (temperature + top-p nucleus)\n",
    "# ============================================================\n",
    "def softmax_with_temperature(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "    \"\"\"Apply temperature scaling + softmax.\"\"\"\n",
    "    t = max(float(temperature), 1e-6)\n",
    "    return torch.softmax(logits / t, dim=-1)\n",
    "\n",
    "\n",
    "def top_p_filter_indices(probs: torch.Tensor, top_p: Optional[float]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return indices inside the nucleus (top-p) set.\n",
    "    Always keep at least the top-1 token.\n",
    "    \"\"\"\n",
    "    V = probs.numel()\n",
    "    if top_p is None or top_p >= 1:\n",
    "        return torch.arange(V, device=probs.device)\n",
    "    if top_p <= 0:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        return sorted_ix[:1]\n",
    "\n",
    "    sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "    csum = torch.cumsum(sorted_p, dim=0)\n",
    "    keep = csum <= top_p\n",
    "    keep[0] = True\n",
    "    return sorted_ix[keep]\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def sample_next_token(ids: torch.Tensor,\n",
    "                      *,\n",
    "                      temperature: float = 0.9,\n",
    "                      top_p: float = 0.95,\n",
    "                      ban_eos: bool = False,\n",
    "                      ban_ids: Optional[torch.Tensor] = BAN_IDS) -> int:\n",
    "    \"\"\"\n",
    "    Sample ONE token via temperature + top-p with optional masks.\n",
    "    \"\"\"\n",
    "    logits = last_step_logits(ids).clone()\n",
    "    mask_all_(logits, ban_eos=ban_eos, ban_ids=ban_ids)\n",
    "\n",
    "    probs = softmax_with_temperature(logits, temperature)\n",
    "    pool_ix = top_p_filter_indices(probs, top_p)\n",
    "    pool_p = probs[pool_ix] / pool_p.sum() if (pool_p := probs[pool_ix]).sum() > 0 else probs[pool_ix]\n",
    "\n",
    "    pick_local = torch.multinomial(pool_p, num_samples=1, replacement=False)[0]\n",
    "    return int(pool_ix[pick_local].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d23a619-cee0-416f-a84c-2212b404f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5.1) Repetition controls (logits post-processing)\n",
    "# ============================================================\n",
    "\n",
    "def apply_repetition_penalty_(logits: torch.Tensor,\n",
    "                              ids: torch.Tensor,\n",
    "                              penalty: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    In-place repetition penalty (HuggingFace 방식):\n",
    "      - If a token appeared in the context:\n",
    "          logit > 0 → logit /= penalty\n",
    "          logit < 0 → logit *= penalty\n",
    "    \"\"\"\n",
    "    if penalty is None or float(penalty) == 1.0:\n",
    "        return\n",
    "    seen = torch.unique(ids)  # context에 등장한 토큰들\n",
    "    for tid in seen:\n",
    "        t = logits[int(tid)]\n",
    "        if t > 0:\n",
    "            logits[int(tid)] = t / penalty\n",
    "        else:\n",
    "            logits[int(tid)] = t * penalty\n",
    "\n",
    "\n",
    "def _get_banned_tokens_no_repeat_ngram(ids: torch.Tensor,\n",
    "                                       n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Return tokens that would create a repeated n-gram if chosen next.\n",
    "    (batch size 1 가정)\n",
    "    \"\"\"\n",
    "    if n is None or n <= 0:\n",
    "        return []\n",
    "    seq = ids[0].tolist()\n",
    "    if len(seq) < n:\n",
    "        return []\n",
    "    # build { (n-1)-gram prefix → set(next_token) } from history\n",
    "    prefix2next = {}\n",
    "    for i in range(len(seq) - n + 1):\n",
    "        prefix = tuple(seq[i:i + n - 1])\n",
    "        nxt = seq[i + n - 1]\n",
    "        prefix2next.setdefault(prefix, set()).add(nxt)\n",
    "    # current (n-1)-gram\n",
    "    cur_prefix = tuple(seq[-(n - 1):])\n",
    "    return list(prefix2next.get(cur_prefix, set()))\n",
    "\n",
    "\n",
    "def apply_no_repeat_ngram_(logits: torch.Tensor,\n",
    "                           ids: torch.Tensor,\n",
    "                           n: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    In-place: ban tokens that would form a repeated n-gram.\n",
    "    \"\"\"\n",
    "    if n is None or n <= 0:\n",
    "        return\n",
    "    banned = _get_banned_tokens_no_repeat_ngram(ids, n)\n",
    "    if banned:\n",
    "        idx = torch.tensor(banned, device=logits.device, dtype=torch.long)\n",
    "        logits[idx] = float(\"-inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a8325b9-cc44-4bc4-9f59-a2f0f0d4fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Propose a short branch (span tokens) via sampling\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def propose_branch(ids: torch.Tensor,\n",
    "                   *,\n",
    "                   span: int = 3,\n",
    "                   temperature: float = 0.9,\n",
    "                   top_p: float = 0.95,\n",
    "                   ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                   ban_eos_first_n: int = 2) -> List[int]:\n",
    "    \"\"\"\n",
    "    Propose a short branch of length `span` by iterative sampling.\n",
    "    - Ban EOS for the first `ban_eos_first_n` samples to avoid immediate stop.\n",
    "    \"\"\"\n",
    "    cur = ids.clone()\n",
    "    branch: List[int] = []\n",
    "    for i in range(span):\n",
    "        ban = (i < ban_eos_first_n)\n",
    "        t = sample_next_token(cur, temperature=temperature, top_p=top_p, ban_eos=ban, ban_ids=ban_ids)\n",
    "        branch.append(t)\n",
    "        cur = append_token(cur, t)\n",
    "    return branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41a94bec-dd3c-47ab-8a7a-c61f1c72e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7) Prefix-accept-once (compare sampled vs greedy)\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def prefix_accept_once(ids: torch.Tensor,\n",
    "                       branch: List[int],\n",
    "                       *,\n",
    "                       ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                       ban_eos_during_accept: bool = False) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Accept sampled tokens while they match the greedy prediction.\n",
    "    On first mismatch, accept greedy token and STOP.\n",
    "    Returns: (updated_ids, accepted_count)\n",
    "    \"\"\"\n",
    "    cur = ids.clone()\n",
    "    accepted = 0\n",
    "    for i, t in enumerate(branch):\n",
    "        g = greedy_next(cur, ban_eos=(ban_eos_during_accept and i == 0), ban_ids=ban_ids)\n",
    "        if g == t:                         # match → accept sampled token\n",
    "            cur = append_token(cur, t)\n",
    "            accepted += 1\n",
    "        else:                               # mismatch → accept greedy token and stop\n",
    "            cur = append_token(cur, g)\n",
    "            break\n",
    "    return cur, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c12173e-afe4-4211-9b0b-bd2388e5a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) Ultra-simple Medusa-like loop (one branch per step)\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def medusa_tiny(prompt: str,\n",
    "                *,\n",
    "                max_new_tokens: int = 30,\n",
    "                span: int = 3,\n",
    "                temperature: float = 0.9,\n",
    "                top_p: float = 0.95,\n",
    "                ban_eos_steps: int = 8,\n",
    "                ban_ids: Optional[torch.Tensor] = BAN_IDS) -> str:\n",
    "    \"\"\"\n",
    "    Minimal Medusa-like generation:\n",
    "      loop:\n",
    "        (a) propose a small branch of `span` tokens (sampling)\n",
    "        (b) prefix-accept against greedy (1st mismatch → greedy, stop)\n",
    "      stop when new tokens reach `max_new_tokens`\n",
    "    \"\"\"\n",
    "    ids = encode(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "    steps = math.ceil(max_new_tokens / max(1, span))\n",
    "    remaining_ban = max(0, ban_eos_steps)\n",
    "\n",
    "    for step in range(steps):\n",
    "        branch = propose_branch(ids, span=span, temperature=temperature, top_p=top_p,\n",
    "                                ban_ids=ban_ids, ban_eos_first_n=min(span, remaining_ban))\n",
    "        ids, _ = prefix_accept_once(ids, branch, ban_ids=ban_ids,\n",
    "                                    ban_eos_during_accept=(step == 0 and remaining_ban > 0))\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "        remaining_ban = max(0, remaining_ban - span)\n",
    "\n",
    "    return decode(ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8ab0a2c2-a308-4764-ac81-7f6c03a45184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Greedy (baseline) ===\n",
      "PROMPT: 'In a distant future, '\n",
      "'In a distant future, vernacular is a form of the word “vernacular”. It is a form of the word “vernacular”. It'\n",
      "------------------------------------------------------------\n",
      "PROMPT: 'Long ago, '\n",
      "'Long ago, iced tea was a popular drink in the United States. It was also popular in the United States. It was also popular in the United States. It'\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Medusa-tiny (span=3) ===\n",
      "PROMPT: 'In a distant future, '\n",
      "'In a distant future, vernacular is a form of the word “vernacular”. It is'\n",
      "------------------------------------------------------------\n",
      "PROMPT: 'Long ago, '\n",
      "'Long ago, iced tea was a popular drink in the United States. It was also popular'\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9) Quick smoke tests (repr shows spaces clearly)\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    prompts = [\n",
    "        \"In a distant future, \",\n",
    "        \"Long ago, \",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n=== Greedy (baseline) ===\")\n",
    "    for p in prompts:\n",
    "        print(\"PROMPT:\", repr(p))\n",
    "        print(repr(greedy_generate(p, max_new_tokens=30, ban_eos_steps=8)))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(\"\\n=== Medusa-tiny (span=3) ===\")\n",
    "    for p in prompts:\n",
    "        print(\"PROMPT:\", repr(p))\n",
    "        print(repr(medusa_tiny(p, max_new_tokens=30, span=3, temperature=0.9, top_p=0.95)))\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8aecf-b966-4564-a1de-364e8d6fa178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
