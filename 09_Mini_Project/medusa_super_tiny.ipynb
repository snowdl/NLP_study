{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90caf9c7-2c68-451b-bef9-c701c3e313ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "import random, torch\n",
    "DEVICE = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"DEVICE =\", DEVICE)\n",
    "\n",
    "def set_seed(seed=7):\n",
    "    random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b86e6251-9cc1-4d40-8a09-3e3798e7bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1 : Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c76c09c3-c8ec-4040-9e6e-d3507dc8382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "MODEL_ID = \"distilgpt2\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tok.eos_token_id is None: tok.eos_token = \"\"     # EOS 없으면 빈 문자열로라도 지정\n",
    "if tok.pad_token_id is None: tok.pad_token = tok.eos_token  # PAD 없으면 EOS 재사용\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "model.config.use_cache = True\n",
    "EOS_ID = tok.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07ba24e6-2d3f-44b4-89bc-42a10f0769c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP2] utils ready\n"
     ]
    }
   ],
   "source": [
    "# STEP2 — Utilities: encode/decode/greedy_next\n",
    "\n",
    "def encode(text: str):\n",
    "    return tok(text, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "\n",
    "def decode(ids):\n",
    "    return tok.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def greedy_next(ids):\n",
    "    \"\"\"가장 점수 높은 토큰 하나 고르기\"\"\"\n",
    "    logits = model(ids).logits[0, -1, :]      # 마지막 위치 로짓\n",
    "    return int(torch.argmax(logits).item())   # argmax\n",
    "\n",
    "print(\"[STEP2] utils ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bba75597-3134-4d43-bd28-1f64df8d7b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Greedy ===\n",
      "Artificial intelligence is changing the way humans , and it’s changing the way we interact with other people.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Artificial intelligence is changing the way humans ,\"\n",
    "ids = encode(prompt)\n",
    "start = ids.shape[1]\n",
    "\n",
    "for _ in range(30):\n",
    "    nxt = greedy_next(ids)\n",
    "    if EOS_ID is not None and nxt == EOS_ID:\n",
    "        break\n",
    "    ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "print(\"=== Greedy ===\")\n",
    "print(decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb69b8b2-05bc-44ba-a616-6ecca78e5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling(only temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d021dbef-ceaf-4bc4-a0ec-22e9e7106a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def softmax_temp(logits, temperature=1.0):\n",
    "    \"\"\"온도 낮을수록 결정적(<1), 높을수록 랜덤(>1)\"\"\"\n",
    "    t = max(float(temperature), 1e-6)\n",
    "    return torch.softmax(logits / t, dim=-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def sample_next_temp_only(ids, temperature=0.9):\n",
    "    logits = model(ids).logits[0, -1, :]\n",
    "    probs  = softmax_temp(logits, temperature)\n",
    "    pick = torch.multinomial(probs, 1)[0].item()  # 확률대로 1개 뽑기\n",
    "    return int(pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "023c6ab1-c2c4-4904-a934-4323c360683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aea26f2-2e10-4e3c-ba32-1ef06262966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample (temp only) ===\n",
      "Artificial intelligence is changing the way humans , and understanding human biology is changing the way humans are prepared for its challenges, a presentation published this week in the journal Current Biology Proceedings of the National Academy\n"
     ]
    }
   ],
   "source": [
    "ids2 = encode(prompt)\n",
    "for _ in range(30):\n",
    "    nxt = sample_next_temp_only(ids2, temperature=0.9)\n",
    "    if EOS_ID is not None and nxt == EOS_ID: break\n",
    "    ids2 = torch.cat([ids2, torch.tensor([[nxt]], device=ids2.device)], dim=1)\n",
    "\n",
    "print(\"=== Sample (temp only) ===\")\n",
    "print(decode(ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e91741f-c1dd-42da-8647-6b15a72c63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 5 — nucleus(top-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b5f40c6-4e7a-4beb-ab1e-2f1ede0ce0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def top_p_indices(probs, top_p=0.95):\n",
    "    if top_p is None or top_p >= 1:  # 무제한\n",
    "        return torch.arange(probs.numel(), device=probs.device)\n",
    "    sp, sx = torch.sort(probs, descending=True)\n",
    "    csum = torch.cumsum(sp, dim=0)\n",
    "    keep = csum <= top_p\n",
    "    keep[0] = True\n",
    "    return sx[keep]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def sample_next(ids, temperature=0.9, top_p=0.95):\n",
    "    logits = model(ids).logits[0, -1, :]\n",
    "    probs  = softmax_temp(logits, temperature)\n",
    "    pool_ix = top_p_indices(probs, top_p)\n",
    "    pool = probs[pool_ix]\n",
    "    pool = pool / pool.sum()                  # 후보 합=1로 정규화\n",
    "    pick_local = torch.multinomial(pool, 1)[0].item()\n",
    "    return int(pool_ix[pick_local].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25721bf7-a8fc-4e28-8a65-2597a9c4f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31e552e1-24e2-4fce-9174-440e419b0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    " @torch.inference_mode()\n",
    "def propose_branch(ids, span=3, temperature=0.9, top_p=0.95):\n",
    "    cur = ids.clone()\n",
    "    out = []\n",
    "    for _ in range(span):\n",
    "        t = sample_next(cur, temperature, top_p)\n",
    "        out.append(t)\n",
    "        cur = torch.cat([cur, torch.tensor([[t]], device=cur.device)], dim=1)\n",
    "    return out  # list[int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00d76d21-fe64-48fd-bfe2-45979f4dd35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed token IDs: [1849, 10996, 351, 257, 3644]\n",
      "Decoded tokens: ['\\xa0', ' communicate', ' with', ' a', ' computer']\n",
      "Joined as text:   communicate with a computer\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Artificial intelligence is changing the way humans \"\n",
    "ids = encode(prompt)\n",
    "\n",
    "branch = propose_branch(ids, span=5, temperature=0.9, top_p=0.95)\n",
    "\n",
    "print(\"Proposed token IDs:\", branch)                # 숫자 리스트\n",
    "print(\"Decoded tokens:\", [tok.decode([t]) for t in branch])  # 각각 글자로\n",
    "print(\"Joined as text:\", tok.decode(branch))        # 한 번에 이어붙인 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea27c80a-2d8a-4d85-b07b-874a873ddb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 7 — prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "749bd9a4-8495-427b-89cd-b6b9a03d32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def prefix_accept_once(ids, branch):\n",
    "    cur = ids.clone()\n",
    "    accepted = 0\n",
    "    for t in branch:\n",
    "        # 1) 현재까지의 시퀀스에서 그리디 다음 토큰\n",
    "        g = greedy_next(cur)\n",
    "        if g == t:\n",
    "            # 2) 같으면 제안 토큰 수락\n",
    "            cur = torch.cat([cur, torch.tensor([[t]], device=cur.device)], dim=1)\n",
    "            accepted += 1\n",
    "        else:\n",
    "            # 3) 다르면 그리디 토큰 붙이고 즉시 중단\n",
    "            cur = torch.cat([cur, torch.tensor([[g]], device=cur.device)], dim=1)\n",
    "            break\n",
    "    return cur, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca4c48b5-a128-4f55-b28f-8ab57e811aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 8 — Medusa-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "987ecb17-7203-485b-9825-a24ce8abc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_tiny(prompt, max_new_tokens=30, span=3, temperature=0.9, top_p=0.95):\n",
    "    ids = encode(prompt)\n",
    "    start = ids.shape[1]\n",
    "    steps = 0\n",
    "    max_steps = max_new_tokens * 3  # 간단 안전장치\n",
    "    while ids.shape[1] - start < max_new_tokens and steps < max_steps:\n",
    "        branch = propose_branch(ids, span=span, temperature=temperature, top_p=top_p)\n",
    "        ids, _ = prefix_accept_once(ids, branch)\n",
    "        steps += 1\n",
    "    return decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d7985a6-ad8c-4a21-9bab-2aa3d58acd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Medusa-tiny ===\n",
      "In a distant future,   the world is a place where the world is a place where the world is a place where the world is a place where the world is a place\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Medusa-tiny ===\")\n",
    "print(medusa_tiny(\"In a distant future, \", max_new_tokens=30, span=3, temperature=0.9, top_p=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c70db-56dd-420d-82b0-54cdb073e850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
