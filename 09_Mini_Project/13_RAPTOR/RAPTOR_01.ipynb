{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33751fc8-ce3c-472e-a488-c596e79897c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, hashlib, statistics as stats\n",
    "import re, unicodedata\n",
    "import nltk\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a5ca1d1-9a61-4588-a7d6-92ad1e4e0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 경로/설정 =========\n",
    "DOC_NAME = \"01 Harry Potter and the Sorcerers Stone.txt\"\n",
    "DOC_PATH = Path(\"../../11_data\") / DOC_NAME             # 필요시 경로만 변경\n",
    "OUT_JSONL = Path(\"hp_chunks_100tok.jsonl\")\n",
    "OUT_JSONL_DEDUP = Path(\"hp_chunks_100tok.dedup.jsonl\")\n",
    "MAX_TOKENS = 100\n",
    "OVERLAP_TOKENS = 0  # (옵션) 10~20 추천. 논문 필수 아님."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e7f708f-ea4b-4802-b33c-b5a5adab4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 전처리 =========\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"OCR/특수공백/축약어 깨짐 보정 (강화 + B 보강 포함)\"\"\"\n",
    "    t = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # 줄바꿈 통일\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # 대표적인 제로폭/nbsp 정리\n",
    "    t = re.sub(r\"[\\u00A0\\u200B\\u200C\\u200D]\", \" \", t)\n",
    "    t = re.sub(r\"[ \\t]{2,}\", \" \", t)\n",
    "\n",
    "    # 하이픈 줄바꿈 연결: some-\\nthing → something\n",
    "    t = re.sub(r\"-\\s*\\n\\s*\", \"\", t)\n",
    "\n",
    "    # 단일 개행 → 공백, 이중 이상 개행은 문단 유지\n",
    "    t = re.sub(r\"\\n{2,}\", \"\\n\\n\", t)\n",
    "    t = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", t)\n",
    "    t = re.sub(r\"[ \\t]{2,}\", \" \", t).strip()\n",
    "\n",
    "    # --- Mr. 패턴(일반 케이스) ---\n",
    "    t = re.sub(\n",
    "        r'(?i)([\"“‘\\'(\\[]?\\s*)m[\\s\\u00A0\\u200B\\u200C\\u200D]*r[\\s\\u00A0\\u200B\\u200C\\u200D]*\\.(\\s*[\"”’\\'\\])]?)+',\n",
    "        r'\\1Mr.\\2',\n",
    "        t\n",
    "    )\n",
    "\n",
    "    # --- B 보강 1) 진짜 'M r.'만 강제 치유 (M과 r 사이에 공백류 1개 이상일 때만) ---\n",
    "    t = re.sub(\n",
    "        r'(?i)([\"“‘\\'(\\[]?\\s*)m[\\s\\u00A0\\u1680\\u180E\\u2000-\\u200A\\u202F\\u205F\\u3000\\uFEFF\\u200B\\u200C\\u200D\\u2060]+r\\s*\\.(\\s*[\"”’\\'\\])]?)+',\n",
    "        r'\\1Mr.\\2',\n",
    "        t\n",
    "    )\n",
    "\n",
    "    # --- B 보강 2) Mr./Mrs./Ms./Dr./Prof. 뒤에 공백이 없으면 한 칸 추가 ---\n",
    "    t = re.sub(r'\\b(Mr|Mrs|Ms|Dr|Prof)\\.(?=[A-Za-z])', r'\\1. ', t)\n",
    "\n",
    "    # --- 다른 축약어(Mrs./Ms./Dr./Prof.) 일반 보정 ---\n",
    "    abbrev_patterns = {\n",
    "        r\"\\bm\\s*rs\\s*\\.\\b\": \"Mrs.\",\n",
    "        r\"\\bm\\s*s\\s*\\.\\b\":  \"Ms.\",\n",
    "        r\"\\bd\\s*r\\s*\\.\\b\":  \"Dr.\",\n",
    "        r\"\\bp\\s*rof\\s*\\.\\b\": \"Prof.\",\n",
    "    }\n",
    "    for pat, rep in abbrev_patterns.items():\n",
    "        t = re.sub(pat, rep, t, flags=re.IGNORECASE)\n",
    "\n",
    "    # --- 이니셜 H . → H. ---\n",
    "    t = re.sub(r\"(?i)(?<![A-Za-z])([A-Z])[\\s\\u00A0\\u200B\\u200C\\u200D]+\\.\", r\"\\1.\", t)\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "438b53dc-8c24-4528-ae1b-cd24638ab05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 토크나이저 =========\n",
    "try:\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "except Exception:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def tok_len(text: str) -> int:\n",
    "    return len(enc.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afacf2c4-df61-412f-8a3a-c7ee42765153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 청크 함수 (문장 중간 절대 자르지 않음) =========\n",
    "def chunk_by_tokens_sentence_safe(sents, max_tokens=100, overlap_tokens=0):\n",
    "    \"\"\"\n",
    "    - 문장을 하나씩 추가하다가 max_tokens 넘으면 새 청크로.\n",
    "    - 문장 자체가 max_tokens를 넘으면 '그 문장만 단독 청크'(초과 허용).\n",
    "    - overlap_tokens>0이면 이전 청크의 꼬리를 다음 청크 앞에 겹침(선택).\n",
    "    반환: [{\"text\":..., \"tokens\":...}, ...]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    cur_texts, cur_tokens = [], 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur_texts, cur_tokens\n",
    "        if not cur_texts:\n",
    "            return\n",
    "        text = \" \".join(cur_texts).strip()\n",
    "        chunks.append({\"text\": text, \"tokens\": cur_tokens})\n",
    "        cur_texts, cur_tokens = [], 0\n",
    "\n",
    "    for s in sents:\n",
    "        n = tok_len(s)\n",
    "\n",
    "        # 1) 한 문장이 너무 길면 그 문장만 단독 청크\n",
    "        if n > max_tokens:\n",
    "            flush()\n",
    "            chunks.append({\"text\": s.strip(), \"tokens\": n})\n",
    "            continue\n",
    "\n",
    "        # 2) 현재 청크에 넣으면 초과 → 비우고 새로 시작\n",
    "        if cur_tokens > 0 and (cur_tokens + n > max_tokens):\n",
    "            flush()\n",
    "            if overlap_tokens > 0 and len(chunks) > 0:\n",
    "                tail_text = chunks[-1][\"text\"]\n",
    "                tail_ids = enc.encode(tail_text)\n",
    "                ov_ids = tail_ids[max(0, len(tail_ids) - overlap_tokens):]\n",
    "                ov_text = enc.decode(ov_ids).strip()\n",
    "                cur_texts = [ov_text] if ov_text else []\n",
    "                cur_tokens = len(enc.encode(\" \".join(cur_texts))) if cur_texts else 0\n",
    "\n",
    "        # 3) 현재 청크에 문장 추가\n",
    "        cur_texts.append(s)\n",
    "        cur_tokens += n\n",
    "\n",
    "    flush()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa305f91-0428-40bb-8205-47a12060ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 청크 함수 (문장 중간 절대 자르지 않음) =========\n",
    "def chunk_by_tokens_sentence_safe(sents, max_tokens=100, overlap_tokens=0):\n",
    "    \"\"\"\n",
    "    - 문장을 하나씩 추가하다가 max_tokens 넘으면 새 청크로.\n",
    "    - 문장 자체가 max_tokens를 넘으면 '그 문장만 단독 청크'(초과 허용).\n",
    "    - overlap_tokens>0이면 이전 청크의 마지막 토큰 일부를 다음 청크 앞에 겹침(선택).\n",
    "    반환: [{\"text\":..., \"tokens\":...}, ...]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    cur_texts, cur_tokens = [], 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur_texts, cur_tokens\n",
    "        if not cur_texts:\n",
    "            return\n",
    "        text = \" \".join(cur_texts).strip()\n",
    "        chunks.append({\"text\": text, \"tokens\": cur_tokens})\n",
    "        cur_texts, cur_tokens = [], 0\n",
    "\n",
    "    for s in sents:\n",
    "        n = tok_len(s)\n",
    "\n",
    "        # 1) 한 문장이 너무 길면 그 문장만 단독 청크\n",
    "        if n > max_tokens:\n",
    "            flush()\n",
    "            chunks.append({\"text\": s.strip(), \"tokens\": n})\n",
    "            continue\n",
    "\n",
    "        # 2) 현재 청크에 넣으면 초과 → 비우고 새로 시작\n",
    "        if cur_tokens > 0 and (cur_tokens + n > max_tokens):\n",
    "            flush()\n",
    "            if overlap_tokens > 0 and len(chunks) > 0:\n",
    "                tail_text = chunks[-1][\"text\"]\n",
    "                tail_ids = enc.encode(tail_text)\n",
    "                ov_ids = tail_ids[max(0, len(tail_ids) - overlap_tokens):]\n",
    "                ov_text = enc.decode(ov_ids).strip()\n",
    "                cur_texts = [ov_text] if ov_text else []\n",
    "                cur_tokens = len(enc.encode(\" \".join(cur_texts))) if cur_texts else 0\n",
    "\n",
    "        # 3) 현재 청크에 문장 추가\n",
    "        cur_texts.append(s)\n",
    "        cur_tokens += n\n",
    "\n",
    "    flush()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32f0537b-366e-499b-94f0-19633695319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 해시(ID) =========\n",
    "def cid16(text: str) -> str:\n",
    "    \"\"\"내용 기반 SHA-256 해시 앞 16자리\"\"\"\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8213359-4893-4ad1-9050-865b2f203655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHUNKS SUMMARY ===\n",
      "총 청크 수: 1222\n",
      "토큰수(평균/중앙/최소/최대): 85.51 / 89.0 / 7 / 294\n",
      "100 토큰 근접(90~100) 비율: 46.56%\n",
      "\n",
      "가장 긴 청크 Top3 (id, tokens): [(739, 294), (263, 236), (72, 178)]\n",
      "\n",
      "--- 미리보기 (앞 3개) ---\n",
      "[1] cid=978646441d4a7b5a · 79 tokens\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or...\n",
      "\n",
      "[2] cid=c8797b0f0bde0730 · 87 tokens\n",
      "He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as s...\n",
      "\n",
      "[3] cid=ca7fd7fb387dd3d3 · 45 tokens\n",
      "The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Pot...\n",
      "\n",
      "정확중복 제거본 저장: hp_chunks_100tok.dedup.jsonl (원본 1222 → 1222)\n"
     ]
    }
   ],
   "source": [
    "# ========= 메인 =========\n",
    "def main():\n",
    "    # (A) NLTK 토큰라이저 준비\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    # (B) 파일 확인/로드\n",
    "    if not DOC_PATH.exists():\n",
    "        print(f\"[오류] 파일을 찾을 수 없습니다: {DOC_PATH.resolve()}\")\n",
    "        print(\"→ DOC_PATH 경로를 확인하거나 파일을 해당 위치로 옮겨주세요.\")\n",
    "        return\n",
    "    text = DOC_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # (C) 전처리 + 문장 분리\n",
    "    text = clean_text(text)\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "\n",
    "    # (D) 청크 생성\n",
    "    chunks = chunk_by_tokens_sentence_safe(\n",
    "        sents, max_tokens=MAX_TOKENS, overlap_tokens=OVERLAP_TOKENS\n",
    "    )\n",
    "\n",
    "    # (E) 통계 계산\n",
    "    lens = [c[\"tokens\"] for c in chunks]\n",
    "    print(\"=== CHUNKS SUMMARY ===\")\n",
    "    print(f\"총 청크 수: {len(chunks)}\")\n",
    "    print(\n",
    "        \"토큰수(평균/중앙/최소/최대): \"\n",
    "        f\"{round(stats.mean(lens),2)} / {stats.median(lens)} / {min(lens)} / {max(lens)}\"\n",
    "    )\n",
    "    pct_90_100 = round(sum(1 for x in lens if 90 <= x <= 100) / len(lens) * 100, 2)\n",
    "    print(f\"100 토큰 근접(90~100) 비율: {pct_90_100}%\")\n",
    "    topk = sorted(enumerate(lens, 1), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\n가장 긴 청크 Top3 (id, tokens):\", topk)\n",
    "\n",
    "    # (F) 저장(JSONL, cid 부여)\n",
    "    with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, ch in enumerate(chunks, 1):\n",
    "            obj = {\n",
    "                \"id\": i,\n",
    "                \"cid\": cid16(ch[\"text\"]),\n",
    "                \"tokens\": ch[\"tokens\"],\n",
    "                \"text\": ch[\"text\"],\n",
    "            }\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # (G) 미리보기 3개\n",
    "    print(\"\\n--- 미리보기 (앞 3개) ---\")\n",
    "    with OUT_JSONL.open(encoding=\"utf-8\") as f:\n",
    "        for _ in range(3):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            o = json.loads(line)\n",
    "            print(f\"[{o['id']}] cid={o['cid']} · {o['tokens']} tokens\")\n",
    "            print(o[\"text\"][:200].replace(\"\\n\", \" \") + \"...\\n\")\n",
    "\n",
    "    # (H) 정확중복 제거본도 저장(같은 cid는 1개만 유지)\n",
    "    seen, kept = set(), []\n",
    "    with OUT_JSONL.open(encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            o = json.loads(line)\n",
    "            if o[\"cid\"] in seen:\n",
    "                continue\n",
    "            seen.add(o[\"cid\"])\n",
    "            kept.append(o)\n",
    "    with OUT_JSONL_DEDUP.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for o in kept:\n",
    "            f.write(json.dumps(o, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"정확중복 제거본 저장: {OUT_JSONL_DEDUP} (원본 {len(chunks)} → {len(kept)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ddb1e3d-a326-4d42-95cc-b92a112a1fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHUNKS SUMMARY ===\n",
      "총 청크 수: 1219\n",
      "토큰수(평균/중앙/최소/최대): 85.69 / 89 / 7 / 294\n",
      "100 토큰 근접(90~100) 비율: 47.17%\n",
      "\n",
      "가장 긴 청크 Top3 (id, tokens): [(737, 294), (261, 236), (70, 178)]\n",
      "\n",
      "--- 미리보기 (앞 3개) ---\n",
      "[1] cid=70ea7b1bb3a13b4d · 100 tokens\n",
      "Mr.and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or ...\n",
      "\n",
      "[2] cid=eca2e02b6f307ca6 · 91 tokens\n",
      "Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Durs...\n",
      "\n",
      "[3] cid=a3c0d6f207742a58 · 79 tokens\n",
      "They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley pretended she didn’t hav...\n",
      "\n",
      "정확중복 제거본 저장: hp_chunks_100tok.dedup.jsonl (원본 1219 → 1219)\n"
     ]
    }
   ],
   "source": [
    "# ========= 메인 =========\n",
    "def main():\n",
    "    # (A) NLTK 토큰라이저 준비\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    # (B) 파일 확인/로드\n",
    "    if not DOC_PATH.exists():\n",
    "        print(f\"[오류] 파일을 찾을 수 없습니다: {DOC_PATH.resolve()}\")\n",
    "        print(\"→ DOC_PATH 경로를 확인하거나 파일을 해당 위치로 옮겨주세요.\")\n",
    "        return\n",
    "    text = DOC_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # (C) 전처리 + 문장 분리\n",
    "    text = clean_text(text)\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "\n",
    "    # (D) 청크 생성\n",
    "    chunks = chunk_by_tokens_sentence_safe(\n",
    "        sents, max_tokens=MAX_TOKENS, overlap_tokens=OVERLAP_TOKENS\n",
    "    )\n",
    "\n",
    "    # (E) 통계 계산\n",
    "    lens = [c[\"tokens\"] for c in chunks]\n",
    "    print(\"=== CHUNKS SUMMARY ===\")\n",
    "    print(f\"총 청크 수: {len(chunks)}\")\n",
    "    print(\n",
    "        \"토큰수(평균/중앙/최소/최대): \"\n",
    "        f\"{round(stats.mean(lens),2)} / {stats.median(lens)} / {min(lens)} / {max(lens)}\"\n",
    "    )\n",
    "    pct_90_100 = round(sum(1 for x in lens if 90 <= x <= 100) / len(lens) * 100, 2)\n",
    "    print(f\"100 토큰 근접(90~100) 비율: {pct_90_100}%\")\n",
    "    topk = sorted(enumerate(lens, 1), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\n가장 긴 청크 Top3 (id, tokens):\", topk)\n",
    "\n",
    "    # (F) 저장(JSONL, cid 부여)\n",
    "    with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, ch in enumerate(chunks, 1):\n",
    "            obj = {\n",
    "                \"id\": i,\n",
    "                \"cid\": cid16(ch[\"text\"]),\n",
    "                \"tokens\": ch[\"tokens\"],\n",
    "                \"text\": ch[\"text\"],\n",
    "            }\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # (G) 미리보기 3개\n",
    "    print(\"\\n--- 미리보기 (앞 3개) ---\")\n",
    "    with OUT_JSONL.open(encoding=\"utf-8\") as f:\n",
    "        for _ in range(3):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            o = json.loads(line)\n",
    "            print(f\"[{o['id']}] cid={o['cid']} · {o['tokens']} tokens\")\n",
    "            print(o[\"text\"][:200].replace(\"\\n\", \" \") + \"...\\n\")\n",
    "\n",
    "    # (H) 정확중복 제거본도 저장(같은 cid는 1개만 유지)\n",
    "    seen, kept = set(), []\n",
    "    with OUT_JSONL.open(encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            o = json.loads(line)\n",
    "            if o[\"cid\"] in seen:\n",
    "                continue\n",
    "            seen.add(o[\"cid\"])\n",
    "            kept.append(o)\n",
    "    with OUT_JSONL_DEDUP.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for o in kept:\n",
    "            f.write(json.dumps(o, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"정확중복 제거본 저장: {OUT_JSONL_DEDUP} (원본 {len(chunks)} → {len(kept)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c5e3ece-a2da-4d50-b2d9-df53be7fc3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M r. 유형: 0\n",
      "Mr.뒤 공백 없음: 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "raw = Path(\"../../11_data/01 Harry Potter and the Sorcerers Stone.txt\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "txt = clean_text(raw)\n",
    "\n",
    "# 진짜 'M r.' (M과 r 사이에 공백류 1개 이상)\n",
    "print(\"M r. 유형:\", len(re.findall(r\"(?i)m[\\s\\u00A0\\u1680\\u180E\\u2000-\\u200A\\u202F\\u205F\\u3000\\uFEFF\\u200B\\u200C\\u200D\\u2060]+r\\s*\\.\", txt)))\n",
    "\n",
    "# Mr./Mrs./Ms./Dr./Prof. 뒤 공백 없음\n",
    "print(\"Mr.뒤 공백 없음:\", len(re.findall(r\"\\b(Mr|Mrs|Ms|Dr|Prof)\\.(?=[A-Za-z])\", txt)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b0455-f5bb-46ca-90a2-cb6bc5141ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccfc6e-e993-4bb1-9f5a-a19f647b5b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7deae65-cec1-4e93-8748-88b38f169a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba0484-39ba-4900-9c89-0446440da1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e88e9-8ab5-4d0d-aaa3-af1379848189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
