{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39875ed7-d801-44c2-b49b-f13770885aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1201f35b-47d6-4a2e-bb59-0d2bb91af483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "chunks = []\n",
    "with open(\"outputs/chunks.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        chunks.append(json.loads(line))\n",
    "\n",
    "tree = []\n",
    "with open(\"outputs/tree_nodes.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        tree.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f21c40-13cb-4dad-bd0c-6ac809552997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. ê°„ë‹¨ ê²€ìƒ‰ í•¨ìˆ˜ ===\n",
    "def raptor_search(query, tree, chunks, topk=2):\n",
    "    node_ids = [n[\"id\"] for n in tree]\n",
    "    summaries = [n[\"summary\"] for n in tree]\n",
    "\n",
    "    # TF-IDF ê¸°ë°˜ ê²€ìƒ‰\n",
    "    vec = TfidfVectorizer().fit(summaries + [query])\n",
    "    q_vec = vec.transform([query])\n",
    "    sims = cosine_similarity(q_vec, vec.transform(summaries))[0]\n",
    "\n",
    "    # ìƒìœ„ ë…¸ë“œ ë½‘ê¸°\n",
    "    top_idx = sims.argsort()[-topk:][::-1]\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        node = tree[idx]\n",
    "        child_chunks = [c for c in node.get(\"children\", []) if c.startswith(\"C\")]\n",
    "        chunk_texts = [c[\"text\"] for c in chunks if c[\"id\"] in child_chunks]\n",
    "        results.append({\n",
    "            \"node\": node[\"id\"],\n",
    "            \"summary\": node[\"summary\"],\n",
    "            \"chunks\": chunk_texts\n",
    "        })\n",
    "    return results# === 3. ì‹¤í–‰ ===\n",
    "query = \"What strange events happened on Privet Drive?\"\n",
    "res = raptor_search(query, tree, chunks)\n",
    "\n",
    "for r in res:\n",
    "    print(\"ğŸ“Œ Node:\", r[\"node\"])\n",
    "    print(\"ğŸ“ Summary:\", r[\"summary\"])\n",
    "    print(\"ğŸ“‘ Chunks:\", r[\"chunks\"][:2])  # ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "026fc55f-2c9f-4ab8-be6e-756f83c88604",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === 3. ì‹¤í–‰ ===\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat strange events happened on Privet Drive?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mraptor_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“Œ Node:\u001b[39m\u001b[38;5;124m\"\u001b[39m, r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m, in \u001b[0;36mraptor_search\u001b[0;34m(query, tree, chunks, topk)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraptor_search\u001b[39m(query, tree, chunks, topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     node_ids \u001b[38;5;241m=\u001b[39m [n[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[1;32m      4\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m [n[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# TF-IDF ê¸°ë°˜ ê²€ìƒ‰\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraptor_search\u001b[39m(query, tree, chunks, topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     node_ids \u001b[38;5;241m=\u001b[39m [\u001b[43mn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[1;32m      4\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m [n[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# TF-IDF ê¸°ë°˜ ê²€ìƒ‰\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "# === 3. ì‹¤í–‰ ===\n",
    "query = \"What strange events happened on Privet Drive?\"\n",
    "res = raptor_search(query, tree, chunks)\n",
    "\n",
    "for r in res:\n",
    "    print(\"ğŸ“Œ Node:\", r[\"node\"])\n",
    "    print(\"ğŸ“ Summary:\", r[\"summary\"])\n",
    "    print(\"ğŸ“‘ Chunks:\", r[\"chunks\"][:2])  # ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1038734d-18f5-432b-8a3c-78f613aea1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dadd88c-8106-4256-8bbd-0b11b5c85dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_node2chunks_from_linkfile(path):\n",
    "    \"\"\"node_chunks.jsonl ë¥˜ (node_id -> [chunk_id,...]) í˜•íƒœ íƒì§€\"\"\"\n",
    "    node2chunks = defaultdict(list)\n",
    "    try:\n",
    "        for obj in load_jsonl(path):\n",
    "            nid = obj.get(\"node_id\")\n",
    "            cids = None\n",
    "            for k in NODE_CHUNK_KEYS:\n",
    "                if k in obj:\n",
    "                    cids = obj[k]\n",
    "                    break\n",
    "            if nid and cids:\n",
    "                if not isinstance(cids, list): cids = [cids]\n",
    "                for cid in cids:\n",
    "                    if cid in chunk_by_id:\n",
    "                        node2chunks[nid].append(cid)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return dict(node2chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e17ffc59-ff6f-4dbf-bb82-a887aa0a5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_node2chunks_from_reverse_linkfile(path):\n",
    "    \"\"\"chunk_node_links.jsonl ë¥˜ (chunk_id -> node_id/...) í˜•íƒœ íƒì§€ í›„ ì—­ì¸ë±ìŠ¤\"\"\"\n",
    "    node2chunks = defaultdict(list)\n",
    "    try:\n",
    "        for obj in load_jsonl(path):\n",
    "            cid = obj.get(\"chunk_id\")\n",
    "            if not cid or cid not in chunk_by_id: \n",
    "                continue\n",
    "            ref = None\n",
    "            for k in CHUNK_NODE_KEYS:\n",
    "                if k in obj:\n",
    "                    ref = obj[k]; break\n",
    "            if ref is None: \n",
    "                continue\n",
    "            if not isinstance(ref, list): ref = [ref]\n",
    "            for nid in ref:\n",
    "                if nid in node_by_id:\n",
    "                    node2chunks[nid].append(cid)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return dict(node2chunks)\n",
    "\n",
    "node2chunks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a1796d0-a0fb-405d-89bd-86d27993955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) ë§í¬ íŒŒì¼ì—ì„œ ë¨¼ì € ì°¾ì•„ë³´ê¸°\n",
    "for lf in link_files:\n",
    "    node2chunks = detect_node2chunks_from_linkfile(lf)\n",
    "    if node2chunks:\n",
    "        print(f\"âœ… ë§í¬ ê°ì§€ (node->chunks): {lf.name}\")\n",
    "        break\n",
    "    node2chunks = detect_node2chunks_from_reverse_linkfile(lf)\n",
    "    if node2chunks:\n",
    "        print(f\"âœ… ë§í¬ ê°ì§€ (chunk->nodes): {lf.name}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0feac7a2-3c8f-45a0-b000-7ff4e5a5e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) ë§í¬ íŒŒì¼ì´ ì—†ê±°ë‚˜ í…… ë¹„ë©´, children/ì²­í¬í•„ë“œì—ì„œ íœ´ë¦¬ìŠ¤í‹± ì‹œë„\n",
    "if not node2chunks:\n",
    "    # (A) children ì•ˆì— chunk_id íŒ¨í„´ì´ ì„ì¸ ê²½ìš°\n",
    "    chunk_like = re.compile(r\"^c[\\w-]+\", re.I)\n",
    "    for nid, nd in node_by_id.items():\n",
    "        kids = nd.get(\"children\") or []\n",
    "        cids = [x for x in kids if isinstance(x, str) and chunk_like.match(x)]\n",
    "        if cids:\n",
    "            node2chunks[nid] = [cid for cid in cids if cid in chunk_by_id]\n",
    "\n",
    "if not node2chunks:\n",
    "    # (B) chunks.jsonl ì•ˆì— ì†Œì† ë…¸ë“œ íŒíŠ¸ê°€ ë“¤ì–´ìˆëŠ” ê²½ìš°\n",
    "    for ch in chunks:\n",
    "        cid = ch.get(\"chunk_id\")\n",
    "        if not cid: \n",
    "            continue\n",
    "        for k in [\"node_id\",\"belongs_to\",\"owner_node\",\"nodes\",\"parents\",\"node_path\"]:\n",
    "            if k in ch:\n",
    "                ref = ch[k]\n",
    "                if not isinstance(ref, list): ref = [ref]\n",
    "                for nid in ref:\n",
    "                    if nid in node_by_id:\n",
    "                        node2chunks.setdefault(nid, []).append(cid)\n",
    "                break\n",
    "\n",
    "# 5) ë³´ì—¬ì£¼ê¸°: ì²­í¬ê°€ ë¶™ì€ ë…¸ë“œ í•˜ë‚˜ ì°¾ì•„ì„œ ìš”ì•½ + ì²­í¬ 1~2ê°œ ë¯¸ë¦¬ë³´ê¸°\n",
    "def preview_text(t, n=160):\n",
    "    s = t if isinstance(t, str) else str(t)\n",
    "    return (s[:n] + \"...\") if len(s) > n else s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cb46e4f-bacc-489f-81bd-599bf7a658f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ë…¸ë“œ]\n",
      " ID   : L1_N0001\n",
      " ë ˆë²¨ : 1\n",
      " ìì‹ : ['C0001', 'C0002']\n",
      " ìš”ì•½ : This is the story of the Dursleys and the Potters.\n",
      "\n",
      "[ì—°ê²°ëœ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°]\n",
      " - C0001: M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youâ€™d exp...\n",
      " - C0002: Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. â€œLittle tyke,â€...\n"
     ]
    }
   ],
   "source": [
    "if node2chunks:\n",
    "    # ì²­í¬ ë‹¬ë¦° ë…¸ë“œë“¤ë§Œ\n",
    "    candidates = [(nid, cids) for nid, cids in node2chunks.items() if cids]\n",
    "    if candidates:\n",
    "        nid, cids = candidates[0]  # ì²« ë²ˆì§¸ë§Œ ë³´ì—¬ì¤Œ\n",
    "        nd = node_by_id[nid]\n",
    "        print(\"\\n[ë…¸ë“œ]\")\n",
    "        print(\" ID   :\", nid)\n",
    "        print(\" ë ˆë²¨ :\", nd.get(\"level\"))\n",
    "        print(\" ìì‹ :\", nd.get(\"children\"))\n",
    "        print(\" ìš”ì•½ :\", nd.get(\"summary\"))\n",
    "        print(\"\\n[ì—°ê²°ëœ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°]\")\n",
    "        for cid in cids[:2]:\n",
    "            ch = chunk_by_id.get(cid, {})\n",
    "            print(f\" - {cid}:\", preview_text(ch.get(\"text\", \"\")))\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ ë§í¬ëŠ” ê°ì§€í–ˆì§€ë§Œ, ì‹¤ì œ ì—°ê²°ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ì²­í¬-ë…¸ë“œ ì—°ê²°ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   - outputs í´ë”ì— 'node_chunks.jsonl' ë˜ëŠ” 'chunk_node_links.jsonl' ê°™ì€ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸\")\n",
    "    print(\"   - íŒŒì¼ ì•ˆì—ì„œ 'node_id', 'chunk_ids/chunks/sources' ë“±ì˜ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸\")\n",
    "    print(\"   - í‚¤ ì´ë¦„ì´ ë‹¤ë¥´ë©´ ìœ„ NODE_CHUNK_KEYS / CHUNK_NODE_KEYSì— ì¶”ê°€ í›„ ë‹¤ì‹œ ì‹¤í–‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84efa18e-d37a-4b3b-b379-ee60fc962f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#íŠ¹ì • ë…¸ë“œ í•˜ë‚˜ë§Œ ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d771c02e-b67d-452d-9f8c-244477de42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë…¸ë“œID] L1_N0001\n",
      "[ë ˆë²¨] 1\n",
      "[ìì‹] ['C0001', 'C0002']\n",
      "[ìš”ì•½] This is the story of the Dursleys and the Potters.\n",
      "- C0001: M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youâ€™d exp...\n",
      "- C0002: Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. â€œLittle tyke,â€...\n"
     ]
    }
   ],
   "source": [
    "nid = \"L1_N0001\"  # ë³´ê³  ì‹¶ì€ ë…¸ë“œIDë¡œ ë³€ê²½\n",
    "lvl, kids, summ = node_by_id[nid].get(\"level\"), node_by_id[nid].get(\"children\"), node_by_id[nid].get(\"summary\")\n",
    "print(f\"[ë…¸ë“œID] {nid}\\n[ë ˆë²¨] {lvl}\\n[ìì‹] {kids}\\n[ìš”ì•½] {summ}\")\n",
    "for cid in node2chunks.get(nid, [])[:3]:  # ì²­í¬ 3ê°œ ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(f\"- {cid}: {chunk_by_id[cid]['text'][:160]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6bafa0b-7096-464b-8849-4652f8bcfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) â€œë£¨íŠ¸â†’ìì‹â†’leafâ€ ë”°ë¼ê°€ì„œ ì²« ì²­í¬ê¹Œì§€ ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58105140-b675-4155-90cf-8735ec5eb0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ ì°¾ì€ ë…¸ë“œ: L1_N0001\n",
      "- C0001: M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youâ€™d exp...\n",
      "- C0002: Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. â€œLittle tyke,â€...\n"
     ]
    }
   ],
   "source": [
    "def first_leaf_with_chunks(root_id):\n",
    "    stack = [root_id]\n",
    "    seen = set()\n",
    "    while stack:\n",
    "        nid = stack.pop()\n",
    "        if nid in seen: \n",
    "            continue\n",
    "        seen.add(nid)\n",
    "        cids = node2chunks.get(nid, [])\n",
    "        if cids:  # ì²­í¬ ë‹¬ë¦° leaf(ë˜ëŠ” ì¤‘ê°„ë…¸ë“œ) ë°œê²¬\n",
    "            return nid, cids\n",
    "        for child in node_by_id[nid].get(\"children\", [])[::-1]:\n",
    "            if child in node_by_id:\n",
    "                stack.append(child)\n",
    "    return None, []\n",
    "\n",
    "root = \"L1_N0001\"  # ì‹œì‘ ë…¸ë“œ\n",
    "nid, cids = first_leaf_with_chunks(root)\n",
    "print(\"â†’ ì°¾ì€ ë…¸ë“œ:\", nid)\n",
    "for cid in cids[:2]:\n",
    "    print(f\"- {cid}: {chunk_by_id[cid]['text'][:160]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec70dec1-704c-4b8a-81b0-59071108e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) ë ˆë²¨ë³„ë¡œ â€œí•œ ë…¸ë“œì”©â€ ìƒ˜í”Œ ë“¤ì—¬ë‹¤ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33c02059-b4eb-4b8a-891a-53b000a05a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ë ˆë²¨ 1 ìƒ˜í”Œ: L1_N0001 ===\n",
      "ìš”ì•½: This is the story of the Dursleys and the Potters.\n",
      "- C0001: M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youâ€™d exp...\n",
      "\n",
      "=== ë ˆë²¨ 2 ìƒ˜í”Œ: L2_N0001 ===\n",
      "ìš”ì•½: Harry Potter and the Philosopherâ€™s Stone by JK Rowling\n",
      "\n",
      "=== ë ˆë²¨ 3 ìƒ˜í”Œ: L3_N0001 ===\n",
      "ìš”ì•½: All images are copyrighted.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "levels = defaultdict(list)\n",
    "for nid, nd in node_by_id.items():\n",
    "    levels[nd.get(\"level\")].append(nid)\n",
    "\n",
    "for lvl in sorted(levels):\n",
    "    sample = levels[lvl][0]\n",
    "    print(f\"\\n=== ë ˆë²¨ {lvl} ìƒ˜í”Œ: {sample} ===\")\n",
    "    print(\"ìš”ì•½:\", node_by_id[sample].get(\"summary\"))\n",
    "    for cid in node2chunks.get(sample, [])[:1]:\n",
    "        print(f\"- {cid}: {chunk_by_id[cid]['text'][:160]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acb038fa-0090-4706-8c1b-029368bbcefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ì²­í¬ê°€ ë¶™ì€ ë…¸ë“œê°€ í•˜ë‚˜ë„ ì—†ìŒ\n"
     ]
    }
   ],
   "source": [
    "# ì²­í¬ê°€ ì—°ê²°ëœ ë…¸ë“œë¥¼ í•˜ë‚˜ ì°¾ê¸°\n",
    "chunked_nodes = [nid for nid, (_, _, _, cids) in node_info.items() if cids]\n",
    "\n",
    "if chunked_nodes:\n",
    "    nid = chunked_nodes[0]   # ì²« ë²ˆì§¸ë¡œ ì²­í¬ê°€ ë‹¬ë¦° ë…¸ë“œ\n",
    "    level, children, summary, node_chunks = node_info[nid]\n",
    "\n",
    "    print(f\"\\n[ë…¸ë“œID] {nid}\")\n",
    "    print(f\"[ë ˆë²¨]   {level}\")\n",
    "    print(f\"[ìì‹]   {children}\")\n",
    "    print(f\"[ìš”ì•½]   {summary}\")\n",
    "\n",
    "    print(\"\\n=== ì—°ê²°ëœ ì²­í¬ ì˜ˆì‹œ ===\")\n",
    "    for cid in node_chunks[:2]:  # ì•ì—ì„œ 2ê°œë§Œ\n",
    "        print(f\"chunk {cid}: {chunks.get(cid,'(ì—†ìŒ)')[:120]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì²­í¬ê°€ ë¶™ì€ ë…¸ë“œê°€ í•˜ë‚˜ë„ ì—†ìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "483c704c-a433-4e43-be5a-3ae7f5998125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9910ad76-7568-4099-8b22-27fe5c911e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "levels = defaultdict(list)  # level -> [(node_id, summary)]\n",
    "for nid, (lvl, children, summ) in node_info.items():\n",
    "    levels[lvl].append((nid, summ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94092e71-4786-402e-986e-afa6abc943d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF ìœ í‹¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5057afea-6758-4f2e-9f64-6b76519a8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def _fit(texts):\n",
    "    v = TfidfVectorizer(ngram_range=(1,2), max_features=60000)\n",
    "    return v, v.fit_transform(texts)\n",
    "\n",
    "def _score(M, v, q):\n",
    "    return cosine_similarity(M, v.transform([q])).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8db206d5-52ea-4865-9a50-221e680ce110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì‹œì‘ ë ˆë²¨ ì„ íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed45c5e3-aaa7-4a5d-89ce-4ee75ff0dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def select_start_level(q:str):\n",
    "    ks = sorted(levels.keys())\n",
    "    if not ks: return None\n",
    "    qn = len(re.findall(r\"\\w+\", q))\n",
    "    return (ks[-1] if qn<=5 else (ks[-2] if len(ks)>1 and qn<=12 else ks[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21ffb577-1254-4b1e-a305-5051ed1128c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 â€” ë ˆë²¨ ë‚´ ë…¸ë“œ ë­í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8c62179-a76a-4c62-ac8b-4c71a6a786f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_nodes_at_level(q, L, topk=6):\n",
    "    items = levels.get(L, [])\n",
    "    if not items: return []\n",
    "    ids  = [i for i,_ in items]\n",
    "    txts = [t for _,t in items]\n",
    "    v, M = _fit(txts); sims = _score(M, v, q)\n",
    "    idx = np.argsort(-sims)[:topk]\n",
    "    return [ids[i] for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b32c184c-bdf4-407d-a23e-81b7e93eb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í™•ì¥(FIX: ê³µë°±ì •ê·œí™” + í´ë°±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2cb6bc0-1e14-4380-87da-9fbc225595be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_id(x):  return x.strip() if isinstance(x, str) else x\n",
    "def _is_chunk(x): return isinstance(x, str) and x.startswith(\"C\")\n",
    "\n",
    "def expand_to_leaves(seed_ids, hops=2):\n",
    "    out, fr = [], list(map(_norm_id, seed_ids))\n",
    "    for _ in range(hops):\n",
    "        nxt=[]\n",
    "        for nid in fr:\n",
    "            if _is_chunk(nid): out.append(nid)\n",
    "            elif nid in node_info: nxt += [_norm_id(c) for c in node_info[nid][1]]\n",
    "        fr = nxt\n",
    "        if not fr: break\n",
    "    if out:\n",
    "        seen, uniq = set(), []\n",
    "        for cid in out:\n",
    "            if cid not in seen: uniq.append(cid); seen.add(cid)\n",
    "        return uniq\n",
    "    # í´ë°±1: í•œ hop ìì‹ ë°˜í™˜(L2->L1 ë“±)\n",
    "    one_hop=[]\n",
    "    for nid in map(_norm_id, seed_ids):\n",
    "        if nid in node_info: one_hop += [_norm_id(c) for c in node_info[nid][1]]\n",
    "    if one_hop: return one_hop\n",
    "    # í´ë°±2: seed ê·¸ëŒ€ë¡œ\n",
    "    return list(map(_norm_id, seed_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d8b04f0-8bf3-44cd-9e96-54fa4dd7db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) L1 â†’ C ê°•ì œ ë³€í™˜\n",
    "def to_chunks(ids, max_hops=3):\n",
    "    out, fr = [], list(ids)\n",
    "    for _ in range(max_hops):\n",
    "        nxt=[]\n",
    "        for x in fr:\n",
    "            if isinstance(x, str) and x.startswith(\"C\"):\n",
    "                out.append(x)\n",
    "            elif x in node_info:\n",
    "                nxt += node_info[x][1]\n",
    "        fr = nxt\n",
    "        if not fr: break\n",
    "    seen, uniq = set(), []\n",
    "    for c in out:\n",
    "        if c not in seen: uniq.append(c); seen.add(c)\n",
    "    return uniq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cbc17b9-aa49-4f00-95fb-b0d42ecdce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7 â€” ë¦¬í”„ ìš”ì•½ ë¡œë“œ(í´ë°±ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea41f740-efb7-4562-bd39-e974ea959ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… leaf_summary: 5\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    summ_smoke = OUT / \"chunk_summaries_smoke.jsonl\"\n",
    "    summ_all   = OUT / \"chunk_summaries.jsonl\"\n",
    "    summ_path  = summ_smoke if summ_smoke.exists() else summ_all\n",
    "    leaf_summary = {json.loads(l)[\"chunk_id\"]: json.loads(l)[\"summary\"] for l in open(summ_path, encoding=\"utf-8\")}\n",
    "except Exception:\n",
    "    leaf_summary = {}\n",
    "print(\"âœ… leaf_summary:\", len(leaf_summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b93f553e-23fd-46aa-a8ba-4c907973df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 8 â€” ì¬ë­í‚¹(ë³¸ë¬¸ ìš°ì„ , ìš”ì•½ í´ë°±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ab81d78-5ab5-4c5b-a623-284ae20f1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_chunks_with_fallback(q, candidate_cids, topk=5):\n",
    "    # ë³¸ë¬¸ ê¸°ì¤€\n",
    "    ids, txts = [], []\n",
    "    for cid in candidate_cids:\n",
    "        if cid in chunk_text:\n",
    "            ids.append(cid); txts.append(chunk_text[cid])\n",
    "    if ids:\n",
    "        v, M = _fit(txts); sims = _score(M, v, q)\n",
    "        idx = np.argsort(-sims)[:topk]\n",
    "        return [(ids[i], float(sims[i])) for i in idx]\n",
    "    # ìš”ì•½ í´ë°±\n",
    "    ids, txts = [], []\n",
    "    for cid in candidate_cids:\n",
    "        if cid in leaf_summary:\n",
    "            ids.append(cid); txts.append(leaf_summary[cid])\n",
    "    if not ids: return []\n",
    "    v, M = _fit(txts); sims = _score(M, v, q)\n",
    "    idx = np.argsort(-sims)[:topk]\n",
    "    return [(ids[i], float(sims[i])) for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc7deda4-e49b-4d1f-95b5-fea6271dafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 9 â€” RAPTOR ê²€ìƒ‰ + ë‹µë³€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06374b0b-f80f-45cc-b7af-a0651c003342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raptor_search(q, topk_nodes=6, topk_chunks=5, hops=2):\n",
    "    L = select_start_level(q)\n",
    "    if L is None: return {\"level\": None, \"nodes\": [], \"chunks\": []}\n",
    "    seeds = rank_nodes_at_level(q, L, topk=topk_nodes)\n",
    "    cand  = expand_to_leaves(seeds, hops=hops) or seeds\n",
    "    cand  = to_chunks(cand, max_hops=3) or cand   # â­ ì—¬ê¸°ë¥¼ ì¶”ê°€: L1 â†’ Cë¡œ ê°•ì œ ë³€í™˜\n",
    "    chunks = rerank_chunks_with_fallback(q, cand, topk=topk_chunks)\n",
    "    return {\"level\": L, \"nodes\": seeds, \"chunks\": chunks}\n",
    "\n",
    "def pretty_answer(q, sent_per_chunk=3):\n",
    "    res = raptor_search(q)\n",
    "    picks=[]\n",
    "    for cid,_ in res[\"chunks\"]:\n",
    "        text = chunk_text.get(cid, \"\")\n",
    "        sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "        if not sents: continue\n",
    "        v, M = _fit(sents); sims = _score(M, v, q)\n",
    "        idx = np.argsort(-sims)[:sent_per_chunk]\n",
    "        picks.append(f\"[{cid}] \" + \" \".join(sents[i] for i in idx))\n",
    "    print(f\"ğŸ” Q: {q}\\nğŸ“ start level: L{res['level']}\\nğŸ“‘ nodes: {res['nodes']}\\nğŸ’¬ A: {' '.join(picks) if picks else '(no matching evidence)'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c05095b1-c1fd-42ca-aabe-f6795bbf69ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Q: What strange events happened on Privet Drive?\n",
      "ğŸ“ start level: L2\n",
      "ğŸ“‘ nodes: ['L2_N0002', 'L2_N0001']\n",
      "ğŸ’¬ A: [C0002] There was a tabby cat standing on the corner of Privet Drive, but there wasnâ€™t a map in sight. It was now reading the sign that said Privet Drive â€” no, looking at the sign; cats couldnâ€™t read maps or signs. What could he have been thinking of? [C0001] Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. [C0003] It was on his way back past them, clutching a large doughnut in a bag, that he caught a few words of what they were saying. â€œThe Potters, thatâ€™s right, thatâ€™s what I heard â€”â€\n",
      "\n",
      "â€œ â€” yes, their son, Harry â€”â€\n",
      "\n",
      "Mr. Dursley arrived in the Grunnings parking lot, his mind back on drills. [C0004] It was now sitting on his garden wall. On the contrary, his face split into a wide smile and he said in a squeaky voice that made passersby stare, â€œDonâ€™t be sorry, my dear sir, for nothing could upset me today! He didnâ€™t blame her â€” if heâ€™d had a sister like thatâ€¦but all the same, those people in cloaks.â€¦\n",
      "\n",
      "He found it a lot harder to concentrate on drills that afternoon and when he left the building at five oâ€™clock, he was still so worried that he walked straight into someone just outside the door. [C0005] â€œWhy?â€\n",
      "\n",
      "â€œFunny stuff on the news,â€ Mr. When Dudley had been put to bed, he went into the living room in time to catch the last report on the evening news:\n",
      "\n",
      "â€œAnd finally, bird-watchers everywhere have reported that the nationâ€™s owls have been behaving very unusually today. â€œWell, I just thoughtâ€¦maybeâ€¦it was something to do withâ€¦you knowâ€¦her crowd.â€\n",
      "\n",
      "Mrs.\n",
      "ğŸ” Q: Describe the Dursleys and how they feel about magic.\n",
      "ğŸ“ start level: L2\n",
      "ğŸ“‘ nodes: ['L2_N0001', 'L2_N0002']\n",
      "ğŸ’¬ A: [C0001] The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. [C0004] Next Doorâ€™s problems with her daughter and how Dudley had learned a new word (â€œWonâ€™t!â€). She told him over dinner all about Mrs. Dursley around the middle and walked off. [C0002] They were whispering excitedly together. Dursley drove around the corner and up the road, he watched the cat in his mirror. Dursley blinked and stared at the cat. [C0005] And a whisper, a whisper about the Potters.â€¦\n",
      "\n",
      "Mrs. Viewers as far apart as Kent, Yorkshire, and Dundee have been phoning in to tell me that instead of the rain I promised yesterday, theyâ€™ve had a downpour of shooting stars! After all, they normally pretended she didnâ€™t have a sister. [C0003] Heâ€™d for gotten all about the people in cloaks until he passed a group of them next to the bakerâ€™s. He didnâ€™t see the owls swooping past in broad daylight, though people down in the street did; they pointed and gazed open-mouthed as owl after owl sped overhead. He didnâ€™t know why, but they made him uneasy.\n"
     ]
    }
   ],
   "source": [
    "pretty_answer(\"What strange events happened on Privet Drive?\")\n",
    "pretty_answer(\"Describe the Dursleys and how they feel about magic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e6c2ff8-f800-40de-b641-b092be07f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== RAPTOR Day3 : ì™•ì´ˆë³´ìš© ê²€ìƒ‰ ìŠ¤ì¼ˆë ˆí†¤ =====================\n",
    "# ì¿¼ë¦¬ â†’ ê´€ë ¨ ë…¸ë“œ(ìš”ì•½) â†’ ì—°ê²° ì²­í¬ â†’ ê°„ë‹¨ ìš”ì•½ ë‹µë³€\n",
    "# - ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ ë™ì‘ (ì •ê·œì‹/ì¤‘ë³µ ë‹¨ì–´ ê²¹ì¹¨ ê¸°ë°˜)\n",
    "# - íŒŒì¼: tree_nodes.jsonl, chunks.jsonl, (node_chunks.jsonl ë˜ëŠ” ìœ ì‚¬ ë§í¬ íŒŒì¼)\n",
    "# ================================================================================\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# --------------------------- ìœ í‹¸: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ---------------------------\n",
    "WORD_RE = re.compile(r\"[A-Za-zê°€-í£0-9']+\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    return [w.lower() for w in WORD_RE.findall(text)]\n",
    "\n",
    "def wordset(text: str):\n",
    "    return set(tokenize(text))\n",
    "\n",
    "def sent_split(text: str):\n",
    "    # ì•„ì£¼ ë‹¨ìˆœí•œ ë¬¸ì¥ ë¶„ë¦¬ (., !, ? ê¸°ì¤€)\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    parts = re.split(r\"(?<=[.!?\\n])\\s+\", text.strip())\n",
    "    # ë¹ˆ ë¬¸ì¥ ì œê±°\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def overlap_score(query_ws: set, text: str):\n",
    "    if not text: \n",
    "        return 0\n",
    "    ws = wordset(text)\n",
    "    if not ws:\n",
    "        return 0\n",
    "    return len(query_ws & ws)\n",
    "\n",
    "# --------------------------- JSONL ë¡œë“œ ---------------------------\n",
    "def load_jsonl(path: Path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "# --------------------------- ê²½ë¡œ ì¶”ì • ---------------------------\n",
    "def find_base():\n",
    "    CWD = Path.cwd()\n",
    "    if CWD.name == \"13_RAPTOR\":\n",
    "        return CWD\n",
    "    cands = [CWD/\"13_RAPTOR\", CWD.parent/\"13_RAPTOR\", CWD/\"09_Mini_Project\"/\"13_RAPTOR\"]\n",
    "    for p in cands:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return CWD\n",
    "\n",
    "BASE = find_base()\n",
    "OUT = BASE / \"outputs\"\n",
    "\n",
    "# --------------------------- ë°ì´í„° ì½ê¸° ---------------------------\n",
    "nodes_path  = OUT / \"tree_nodes.jsonl\"\n",
    "chunks_path = OUT / \"chunks.jsonl\"\n",
    "\n",
    "nodes  = list(load_jsonl(nodes_path))\n",
    "chunks = list(load_jsonl(chunks_path))\n",
    "\n",
    "node_by_id  = {nd[\"node_id\"]: nd for nd in nodes if \"node_id\" in nd}\n",
    "chunk_by_id = {ch[\"chunk_id\"]: ch for ch in chunks if \"chunk_id\" in ch}\n",
    "\n",
    "# --------------------------- ë…¸ë“œâ†”ì²­í¬ ë§í¬ ìë™ ê°ì§€ ---------------------------\n",
    "# (outputs í´ë”ì—ì„œ map/link íŒŒì¼ì„ ì°¾ì•„ node2chunksë¥¼ êµ¬ì„±)\n",
    "NODE_CHUNK_KEYS = [\"chunk_ids\", \"chunks\", \"sources\"]\n",
    "CHUNK_NODE_KEYS = [\"node_id\", \"belongs_to\", \"owner_node\", \"nodes\", \"parents\", \"node_path\"]\n",
    "\n",
    "def detect_node2chunks():\n",
    "    node2chunks = defaultdict(list)\n",
    "\n",
    "    # 1) ë³„ë„ ë§í¬ íŒŒì¼ ìŠ¤ìº”\n",
    "    for p in OUT.glob(\"*\"):\n",
    "        if p.suffix.lower() not in {\".jsonl\", \".json\", \".ndjson\"}:\n",
    "            continue\n",
    "        if not re.search(r\"(map|link|assign|attach|node.*chunk|chunk.*node|leaf)\", p.name, re.I):\n",
    "            continue\n",
    "        # (a) node -> chunks êµ¬ì¡°\n",
    "        try:\n",
    "            for obj in load_jsonl(p):\n",
    "                nid = obj.get(\"node_id\")\n",
    "                if not nid:\n",
    "                    continue\n",
    "                cids = None\n",
    "                for k in NODE_CHUNK_KEYS:\n",
    "                    if k in obj:\n",
    "                        cids = obj[k]\n",
    "                        break\n",
    "                if cids:\n",
    "                    if not isinstance(cids, list):\n",
    "                        cids = [cids]\n",
    "                    for cid in cids:\n",
    "                        if cid in chunk_by_id:\n",
    "                            node2chunks[nid].append(cid)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # ë§í¬ê°€ ìƒê²¼ë‹¤ë©´ OK\n",
    "        if node2chunks:\n",
    "            return dict(node2chunks)\n",
    "\n",
    "        # (b) chunk -> nodes êµ¬ì¡° (ì—­ì¸ë±ìŠ¤)\n",
    "        try:\n",
    "            tmp = defaultdict(list)\n",
    "            for obj in load_jsonl(p):\n",
    "                cid = obj.get(\"chunk_id\")\n",
    "                if not cid or cid not in chunk_by_id:\n",
    "                    continue\n",
    "                ref = None\n",
    "                for k in CHUNK_NODE_KEYS:\n",
    "                    if k in obj:\n",
    "                        ref = obj[k]\n",
    "                        break\n",
    "                if ref is None:\n",
    "                    continue\n",
    "                if not isinstance(ref, list):\n",
    "                    ref = [ref]\n",
    "                for nid in ref:\n",
    "                    if nid in node_by_id:\n",
    "                        tmp[nid].append(cid)\n",
    "            if tmp:\n",
    "                return dict(tmp)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) ë§í¬ íŒŒì¼ì´ ì—†ìœ¼ë©´ íœ´ë¦¬ìŠ¤í‹±ë“¤\n",
    "\n",
    "    # (A) childrenì— chunk_idê°€ ì„ì¸ ê²½ìš° (ì˜ˆ: 'C0001' íŒ¨í„´)\n",
    "    chunk_like = re.compile(r\"^c[\\w-]+\", re.I)\n",
    "    for nid, nd in node_by_id.items():\n",
    "        kids = nd.get(\"children\") or []\n",
    "        cids = [x for x in kids if isinstance(x, str) and chunk_like.match(x)]\n",
    "        for cid in cids:\n",
    "            if cid in chunk_by_id:\n",
    "                node2chunks[nid].append(cid)\n",
    "\n",
    "    if node2chunks:\n",
    "        return dict(node2chunks)\n",
    "\n",
    "    # (B) ì²­í¬ ìª½ì— ì†Œì† ë…¸ë“œ íŒíŠ¸ê°€ ìˆëŠ” ê²½ìš°\n",
    "    tmp = defaultdict(list)\n",
    "    for ch in chunks:\n",
    "        cid = ch.get(\"chunk_id\")\n",
    "        if not cid:\n",
    "            continue\n",
    "        for k in CHUNK_NODE_KEYS:\n",
    "            if k in ch:\n",
    "                ref = ch[k]\n",
    "                if not isinstance(ref, list):\n",
    "                    ref = [ref]\n",
    "                for nid in ref:\n",
    "                    if nid in node_by_id:\n",
    "                        tmp[nid].append(cid)\n",
    "                break\n",
    "    if tmp:\n",
    "        return dict(tmp)\n",
    "\n",
    "    # ì•„ë¬´ê²ƒë„ ëª» ì°¾ìŒ\n",
    "    return {}\n",
    "\n",
    "node2chunks = detect_node2chunks()\n",
    "\n",
    "# --------------------------- ë…¸ë“œ ìŠ¤ì½”ì–´ë§ (ìš”ì•½ ê¸°ë°˜) ---------------------------\n",
    "def rank_nodes_by_query(query: str, top_k: int = 3):\n",
    "    qset = wordset(query)\n",
    "    scored = []\n",
    "    for nid, nd in node_by_id.items():\n",
    "        summ = nd.get(\"summary\", \"\")\n",
    "        score = overlap_score(qset, summ)\n",
    "        # ë³´ë„ˆìŠ¤: ì œëª©/ìš”ì•½ì´ ì§§ì„ìˆ˜ë¡ ê°€ì  (ê°„ë‹¨í•œ ê¸¸ì´ ë³´ì •)\n",
    "        score += max(0, 5 - len(summ.split())//40)\n",
    "        if score > 0:\n",
    "            scored.append((score, nid))\n",
    "    scored.sort(reverse=True)\n",
    "    return [nid for _, nid in scored[:top_k]]\n",
    "\n",
    "# --------------------------- ì²­í¬ ìŠ¤ì½”ì–´ë§ (ë‚´ìš© ê¸°ë°˜) ---------------------------\n",
    "def rank_chunks_for_nodes(query: str, nids, per_node: int = 3):\n",
    "    qset = wordset(query)\n",
    "    results = []\n",
    "    for nid in nids:\n",
    "        cids = node2chunks.get(nid, [])\n",
    "        scored = []\n",
    "        for cid in cids:\n",
    "            text = chunk_by_id.get(cid, {}).get(\"text\", \"\")\n",
    "            sc = overlap_score(qset, text)\n",
    "            if sc > 0:\n",
    "                scored.append((sc, cid))\n",
    "        scored.sort(reverse=True)\n",
    "        results.extend([cid for _, cid in scored[:per_node]])\n",
    "    # ì¤‘ë³µ ì œê±°, ê°„ë‹¨ ì •ë ¬ ìœ ì§€\n",
    "    seen, ordered = set(), []\n",
    "    for cid in results:\n",
    "        if cid not in seen:\n",
    "            seen.add(cid)\n",
    "            ordered.append(cid)\n",
    "    return ordered\n",
    "\n",
    "# --------------------------- ê°„ë‹¨ ìš”ì•½ ìƒì„± (ì¶”ì¶œì‹) ---------------------------\n",
    "def make_answer(query: str, chunk_texts, max_sents: int = 5, max_chars: int = 800):\n",
    "    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ, ì¿¼ë¦¬ì™€ ê²¹ì¹˜ëŠ” ë¬¸ì¥ì„ ìš°ì„  ì„ íƒ\n",
    "    qset = wordset(query)\n",
    "    cand_sents = []\n",
    "    for txt in chunk_texts:\n",
    "        for s in sent_split(txt):\n",
    "            sc = overlap_score(qset, s)\n",
    "            cand_sents.append((sc, s))\n",
    "\n",
    "    # ì ìˆ˜ ë†’ì€ ë¬¸ì¥ë¶€í„° ë½‘ë˜, ë„ˆë¬´ ë¹„ìŠ·í•œ ë¬¸ì¥ ë°˜ë³µ ë°©ì§€(ê°„ë‹¨ ì¤‘ë³µ ì œê±°)\n",
    "    cand_sents.sort(key=lambda x: x[0], reverse=True)\n",
    "    picked, seen = [], set()\n",
    "    for sc, s in cand_sents:\n",
    "        norm = \" \".join(tokenize(s))[:120]  # ê°„ë‹¨í•œ ì¤‘ë³µ í‚¤\n",
    "        if norm in seen:\n",
    "            continue\n",
    "        picked.append(s)\n",
    "        seen.add(norm)\n",
    "        if len(picked) >= max_sents:\n",
    "            break\n",
    "\n",
    "    if not picked:\n",
    "        # ë°±ì—…: ì²« ì²­í¬ ì•ë¶€ë¶„ì„ ì˜ë¼ì„œ ë°˜í™˜\n",
    "        fallback = (chunk_texts[0] if chunk_texts else \"\").strip()\n",
    "        return (fallback[:max_chars] + \"...\") if len(fallback) > max_chars else fallback\n",
    "\n",
    "    ans = \" \".join(picked)\n",
    "    return (ans[:max_chars] + \"...\") if len(ans) > max_chars else ans\n",
    "\n",
    "# --------------------------- ë©”ì¸: raptor_search ---------------------------\n",
    "def raptor_search(query: str, top_k_nodes=3, chunks_per_node=3, max_sents=5, max_chars=800):\n",
    "    # 1) ê´€ë ¨ ë…¸ë“œ ê³ ë¥´ê¸° (ìš”ì•½ ë§¤ì¹­)\n",
    "    cand_nodes = rank_nodes_by_query(query, top_k=top_k_nodes)\n",
    "    # ë°±ì—…: ì ìˆ˜ 0ì´ë©´ ë£¨íŠ¸/ë ˆë²¨ ë‚®ì€ ìˆœì„œì—ì„œ ëª‡ ê°œë¼ë„ ì„ íƒ\n",
    "    if not cand_nodes:\n",
    "        lvl_pairs = sorted([(nd.get(\"level\", 999), nid) for nid, nd in node_by_id.items()])[:top_k_nodes]\n",
    "        cand_nodes = [nid for _, nid in lvl_pairs]\n",
    "\n",
    "    # 2) ë…¸ë“œì— ì—°ê²°ëœ ì²­í¬ ì¤‘ì—ì„œ ê´€ë ¨ë„ ë†’ì€ ê²ƒ ì¶”ë¦¬ê¸°\n",
    "    cand_chunks = rank_chunks_for_nodes(query, cand_nodes, per_node=chunks_per_node)\n",
    "    if not cand_chunks:\n",
    "        # ë°±ì—…: ë…¸ë“œì— ë‹¬ë¦° ì²« ì²­í¬ë¼ë„ ì‚¬ìš©\n",
    "        for nid in cand_nodes:\n",
    "            for cid in node2chunks.get(nid, [])[:chunks_per_node]:\n",
    "                cand_chunks.append(cid)\n",
    "        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì „ì²´ ì²­í¬ì—ì„œ ì¼ë¶€\n",
    "        if not cand_chunks:\n",
    "            cand_chunks = list(chunk_by_id.keys())[:chunks_per_node]\n",
    "\n",
    "    # 3) ë‹µë³€ ë§Œë“¤ê¸° (ì¶”ì¶œì‹)\n",
    "    chunk_texts = [chunk_by_id[cid][\"text\"] for cid in cand_chunks if cid in chunk_by_id]\n",
    "    answer = make_answer(query, chunk_texts, max_sents=max_sents, max_chars=max_chars)\n",
    "\n",
    "    # 4) ë””ë²„ê·¸/ì„¤ëª…ìš© ë©”íƒ€\n",
    "    used = {\n",
    "        \"nodes\": [{\"id\": nid, \"summary\": node_by_id[nid].get(\"summary\",\"\")} for nid in cand_nodes],\n",
    "        \"chunks\": [{\"id\": cid, \"preview\": (chunk_by_id[cid].get(\"text\",\"\")[:160] + \"...\")} for cid in cand_chunks]\n",
    "    }\n",
    "    return answer, used\n",
    "\n",
    "# ============================ ì‚¬ìš© ì˜ˆì‹œ ============================\n",
    "# query = \"Who are the Dursleys?\"\n",
    "# ans, meta = raptor_search(query)\n",
    "# print(\"[Answer]\\n\", ans)\n",
    "# print(\"\\n[Used nodes]\")\n",
    "# for it in meta[\"nodes\"]: print(\"-\", it[\"id\"], \":\", it[\"summary\"])\n",
    "# print(\"\\n[Used chunks]\")\n",
    "# for it in meta[\"chunks\"]: print(\"-\", it[\"id\"], \":\", it[\"preview\"])\n",
    "# ==================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ac8086-a906-4ed4-a004-028c0c1d7312",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'raptor_tree.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhp_chunks_100tok.dedup.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraptor_tree.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m     tree \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)  \u001b[38;5;66;03m# Day2ì—ì„œ ë§Œë“  ë…¸ë“œ-ìš”ì•½ êµ¬ì¡°\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# === 2. ê°„ë‹¨ ê²€ìƒ‰ í•¨ìˆ˜ ===\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'raptor_tree.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# === 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "with open(\"hp_chunks_100tok.dedup.jsonl\", \"r\") as f:\n",
    "    chunks = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"raptor_tree.json\", \"r\") as f:\n",
    "    tree = json.load(f)  # Day2ì—ì„œ ë§Œë“  ë…¸ë“œ-ìš”ì•½ êµ¬ì¡°\n",
    "\n",
    "# === 2. ê°„ë‹¨ ê²€ìƒ‰ í•¨ìˆ˜ ===\n",
    "def raptor_search(query, tree, chunks, topk=2):\n",
    "    # ë…¸ë“œ ìš”ì•½ í…ìŠ¤íŠ¸ ëª¨ìŒ\n",
    "    node_ids = list(tree.keys())\n",
    "    summaries = [tree[n][\"summary\"] for n in node_ids]\n",
    "\n",
    "    # TF-IDF ê¸°ë°˜ ê²€ìƒ‰\n",
    "    vec = TfidfVectorizer().fit(summaries + [query])\n",
    "    q_vec = vec.transform([query])\n",
    "    sims = cosine_similarity(q_vec, vec.transform(summaries))[0]\n",
    "\n",
    "    # ìƒìœ„ ë…¸ë“œ ë½‘ê¸°\n",
    "    top_idx = sims.argsort()[-topk:][::-1]\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        node_id = node_ids[idx]\n",
    "        node = tree[node_id]\n",
    "        child_chunks = [c for c in node[\"children\"] if c.startswith(\"C\")]\n",
    "        chunk_texts = [c[\"text\"] for c in chunks if c[\"id\"] in child_chunks]\n",
    "        results.append({\n",
    "            \"node\": node_id,\n",
    "            \"summary\": node[\"summary\"],\n",
    "            \"chunks\": chunk_texts\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# === 3. ì‹¤í–‰ ===\n",
    "query = \"What strange events happened on Privet Drive?\"\n",
    "res = raptor_search(query, tree, chunks)\n",
    "\n",
    "for r in res:\n",
    "    print(\"ğŸ“Œ Node:\", r[\"node\"])\n",
    "    print(\"ğŸ“ Summary:\", r[\"summary\"])\n",
    "    print(\"ğŸ“‘ Chunks:\", r[\"chunks\"][:2])  # ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32382daa-1930-48d3-ab2c-979b8fbc293f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
