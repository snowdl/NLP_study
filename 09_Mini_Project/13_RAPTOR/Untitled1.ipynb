{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df26e6a-fe56-4a2b-a766-d1a628393f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… node_info: 6 nodes\n",
      "âœ… chunk_text: 227 chunks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path.cwd().parents[0] / \"13_RAPTOR\" if (Path.cwd().name != \"13_RAPTOR\") else Path.cwd()\n",
    "OUT  = BASE / \"outputs\"\n",
    "\n",
    "nodes_path = OUT / \"tree_nodes.jsonl\"\n",
    "chunks_path = OUT / \"chunks.jsonl\"\n",
    "\n",
    "# 1) ë…¸ë“œ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "nodes = [json.loads(l) for l in open(nodes_path, encoding=\"utf-8\")]\n",
    "node_info = {nd[\"node_id\"]: (nd[\"level\"], nd[\"children\"], nd[\"summary\"]) for nd in nodes}\n",
    "\n",
    "# 2) ì²­í¬ í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "chunk_text = {json.loads(l)[\"chunk_id\"]: json.loads(l)[\"text\"] for l in open(chunks_path, encoding=\"utf-8\")}\n",
    "\n",
    "print(\"âœ… node_info:\", len(node_info), \"nodes\")\n",
    "print(\"âœ… chunk_text:\", len(chunk_text), \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "366babca-916d-47ed-adf0-395900e6ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ê¼­) node_infoë¥¼ ë¡œë“œí•œ \"ë°”ë¡œ ë‹¤ìŒ ì…€\"ì— ì‹¤í–‰\n",
    "from collections import defaultdict\n",
    "levels = defaultdict(list)\n",
    "for nid, (lvl, children, summ) in node_info.items():\n",
    "    levels[lvl].append((nid, summ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9660717c-5319-4dce-93f1-c4aeace3bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) âœ… íŒ¨ì¹˜ 2: í™•ì¥ í•¨ìˆ˜ ìƒˆ ë²„ì „\n",
    "def _norm_id(x):  return x.strip() if isinstance(x, str) else x\n",
    "def _is_chunk(x): return isinstance(x, str) and x.startswith(\"C\")\n",
    "\n",
    "def expand_to_leaves(seed_ids, hops=2):\n",
    "    out, fr = [], list(map(_norm_id, seed_ids))\n",
    "    for _ in range(hops):\n",
    "        nxt=[]\n",
    "        for nid in fr:\n",
    "            if _is_chunk(nid):\n",
    "                out.append(nid)\n",
    "            elif nid in node_info:\n",
    "                kids = [ _norm_id(c) for c in node_info[nid][1] ]\n",
    "                nxt += kids\n",
    "        fr = nxt\n",
    "        if not fr: break\n",
    "    if out:\n",
    "        seen, uniq = set(), []\n",
    "        for cid in out:\n",
    "            if cid not in seen: uniq.append(cid); seen.add(cid)\n",
    "        return uniq\n",
    "    # í´ë°±ë“¤\n",
    "    one_hop=[]\n",
    "    for nid in map(_norm_id, seed_ids):\n",
    "        if nid in node_info:\n",
    "            one_hop += [ _norm_id(c) for c in node_info[nid][1] ]\n",
    "    if one_hop: return one_hop\n",
    "    return list(map(_norm_id, seed_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b059cfb-5b73-41c9-880a-c43240038b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ìµœì†Œ ìœ í‹¸ & ì¸ë±ìŠ¤ ===\n",
    "import re, json, numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def _fit(texts):\n",
    "    v = TfidfVectorizer(ngram_range=(1,2), max_features=60000)\n",
    "    return v, v.fit_transform(texts)\n",
    "\n",
    "def _score(M, v, q):\n",
    "    return cosine_similarity(M, v.transform([q])).ravel()\n",
    "\n",
    "def _is_chunk(x): return isinstance(x, str) and x.startswith(\"C\")\n",
    "def _norm_id(x):  return x.strip() if isinstance(x, str) else x\n",
    "\n",
    "# ë ˆë²¨ ì¸ë±ìŠ¤ (level -> [(node_id, summary)])\n",
    "levels = defaultdict(list)\n",
    "for nid, (lvl, children, summ) in node_info.items():\n",
    "    levels[lvl].append((nid, summ))\n",
    "\n",
    "# leaf summary (ë³¸ë¬¸ ì—†ì„ ë•Œ í´ë°±ìš©)\n",
    "try:\n",
    "    summ_smoke = OUT / \"chunk_summaries_smoke.jsonl\"\n",
    "    summ_all   = OUT / \"chunk_summaries.jsonl\"\n",
    "    summ_path  = summ_smoke if summ_smoke.exists() else summ_all\n",
    "    leaf_summary = {json.loads(l)[\"chunk_id\"]: json.loads(l)[\"summary\"]\n",
    "                    for l in open(summ_path, encoding=\"utf-8\")}\n",
    "except Exception:\n",
    "    leaf_summary = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4fe46f-82b6-4b38-9323-45ce5cfe8385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Q: What strange events happened on Privet Drive?\n",
      "ğŸ“ start level: L2\n",
      "ğŸ“‘ nodes: ['L2_N0002', 'L2_N0001']\n",
      "ğŸ’¬ A: (no matching evidence)\n"
     ]
    }
   ],
   "source": [
    "def select_start_level(q):\n",
    "    ks = sorted(levels.keys())\n",
    "    if not ks: return None\n",
    "    qlen = len(re.findall(r\"\\w+\", q))\n",
    "    return (ks[-1] if qlen<=5 else (ks[-2] if len(ks)>1 and qlen<=12 else ks[0]))\n",
    "\n",
    "def rank_nodes_at_level(q, L, topk=6):\n",
    "    items = levels.get(L, [])\n",
    "    if not items: return []\n",
    "    ids  = [i for i,_ in items]\n",
    "    txts = [t for _,t in items]\n",
    "    v, M = _fit(txts); sims = _score(M, v, q)\n",
    "    idx  = np.argsort(-sims)[:topk]\n",
    "    return [ids[i] for i in idx]\n",
    "\n",
    "def expand_to_leaves(seed_ids, hops=2):\n",
    "    out, fr = [], list(map(_norm_id, seed_ids))\n",
    "    for _ in range(hops):\n",
    "        nxt=[]\n",
    "        for nid in fr:\n",
    "            if _is_chunk(nid): out.append(nid)\n",
    "            elif nid in node_info: nxt += [ _norm_id(c) for c in node_info[nid][1] ]\n",
    "        fr = nxt\n",
    "        if not fr: break\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    seen, uniq = set(), []\n",
    "    for cid in out:\n",
    "        if cid not in seen: uniq.append(cid); seen.add(cid)\n",
    "    return uniq\n",
    "\n",
    "def rerank_chunks_with_fallback(q, candidate_cids, topk=5):\n",
    "    # 1) ë³¸ë¬¸ ê¸°ì¤€\n",
    "    ids, txts = [], []\n",
    "    for cid in candidate_cids:\n",
    "        if cid in chunk_text:\n",
    "            ids.append(cid); txts.append(chunk_text[cid])\n",
    "    if ids:\n",
    "        v, M = _fit(txts); sims = _score(M, v, q)\n",
    "        idx = np.argsort(-sims)[:topk]\n",
    "        return [(ids[i], float(sims[i])) for i in idx]\n",
    "    # 2) í´ë°±: ìš”ì•½ ê¸°ì¤€\n",
    "    ids, txts = [], []\n",
    "    for cid in candidate_cids:\n",
    "        if cid in leaf_summary:\n",
    "            ids.append(cid); txts.append(leaf_summary[cid])\n",
    "    if not ids: return []\n",
    "    v, M = _fit(txts); sims = _score(M, v, q)\n",
    "    idx = np.argsort(-sims)[:topk]\n",
    "    return [(ids[i], float(sims[i])) for i in idx]\n",
    "\n",
    "def raptor_search(q, topk_nodes=6, topk_chunks=5, hops=2):\n",
    "    L = select_start_level(q)\n",
    "    if L is None: return {\"level\": None, \"nodes\": [], \"chunks\": []}\n",
    "    seeds = rank_nodes_at_level(q, L, topk=topk_nodes)\n",
    "    cand  = expand_to_leaves(seeds, hops=hops) or seeds\n",
    "    chunks = rerank_chunks_with_fallback(q, cand, topk=topk_chunks)\n",
    "    return {\"level\": L, \"nodes\": seeds, \"chunks\": chunks}\n",
    "\n",
    "def pretty_answer(q, sent_per_chunk=2):\n",
    "    res = raptor_search(q)\n",
    "    picks=[]\n",
    "    for cid,_ in res[\"chunks\"]:\n",
    "        text = chunk_text.get(cid, \"\")\n",
    "        sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "        if not sents: continue\n",
    "        v, M = _fit(sents); sims = _score(M, v, q)\n",
    "        idx = np.argsort(-sims)[:sent_per_chunk]\n",
    "        picks.append(f\"[{cid}] \" + \" \".join(sents[i] for i in idx))\n",
    "    print(f\"ğŸ” Q: {q}\\nğŸ“ start level: L{res['level']}\\nğŸ“‘ nodes: {res['nodes']}\\nğŸ’¬ A: {' '.join(picks) if picks else '(no matching evidence)'}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "pretty_answer(\"What strange events happened on Privet Drive?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e944b58d-8aa4-43e2-9b01-87e203eba9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start level: 2\n",
      "seeds: ['L2_N0002', 'L2_N0001']\n",
      "cand: []\n",
      "len(chunk_text): 227\n",
      "have chunk_text: []\n",
      "len(leaf_summary): 5\n",
      "have leaf_summary: []\n"
     ]
    }
   ],
   "source": [
    "q = \"What strange events happened on Privet Drive?\"\n",
    "L = select_start_level(q)\n",
    "seeds = rank_nodes_at_level(q, L, topk=6)\n",
    "cand  = expand_to_leaves(seeds, hops=2)\n",
    "\n",
    "print(\"start level:\", L)\n",
    "print(\"seeds:\", seeds)\n",
    "print(\"cand:\", cand)\n",
    "\n",
    "print(\"len(chunk_text):\", len(chunk_text))\n",
    "print(\"have chunk_text:\", [c for c in cand if c in chunk_text])\n",
    "\n",
    "try:\n",
    "    print(\"len(leaf_summary):\", len(leaf_summary))\n",
    "    print(\"have leaf_summary:\", [c for c in cand if c in leaf_summary])\n",
    "except NameError:\n",
    "    print(\"leaf_summary not loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b8dc6-fd28-4247-adfd-d2a5d9a1538a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
