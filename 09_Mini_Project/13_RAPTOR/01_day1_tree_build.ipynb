{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dd37d194-9aab-4f76-96cd-7f9d18c3ac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n BASE = 13_RAPTOR/\\n â”œâ”€â”€ DATA   â†’ 13_RAPTOR/data\\n â”‚    â””â”€â”€ harry_potter.txt   (ì›ë³¸ ë¬¸ì„œ)\\n â”‚\\n â”œâ”€â”€ SRC    â†’ 13_RAPTOR/src\\n â”‚    â”œâ”€â”€ chunking.py\\n â”‚    â”œâ”€â”€ summarize_chunks.py\\n â”‚    â””â”€â”€ build_tree.py      (ì½”ë“œ ëª¨ë“ˆë“¤)\\n â”‚\\n â””â”€â”€ OUT    â†’ 13_RAPTOR/outputs\\n      â”œâ”€â”€ chunks.jsonl            (Step 1 ê²°ê³¼: ë¬¸ì„œ â†’ ì²­í¬)\\n      â”œâ”€â”€ chunk_summaries.jsonl   (Step 2 ê²°ê³¼: ì²­í¬ â†’ ìš”ì•½)\\n      â”œâ”€â”€ tree_nodes.jsonl        (Step 3 ê²°ê³¼: íŠ¸ë¦¬ êµ¬ì¡° ë…¸ë“œ)\\n      â””â”€â”€ tree_root.json          (Step 3 ê²°ê³¼: ìµœì¢… ë£¨íŠ¸ ìš”ì•½)\\n\\n'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " BASE = 13_RAPTOR/\n",
    " â”œâ”€â”€ DATA   â†’ 13_RAPTOR/data\n",
    " â”‚    â””â”€â”€ harry_potter.txt   (ì›ë³¸ ë¬¸ì„œ)\n",
    " â”‚\n",
    " â”œâ”€â”€ SRC    â†’ 13_RAPTOR/src\n",
    " â”‚    â”œâ”€â”€ chunking.py\n",
    " â”‚    â”œâ”€â”€ summarize_chunks.py\n",
    " â”‚    â””â”€â”€ build_tree.py      (ì½”ë“œ ëª¨ë“ˆë“¤)\n",
    " â”‚\n",
    " â””â”€â”€ OUT    â†’ 13_RAPTOR/outputs\n",
    "      â”œâ”€â”€ chunks.jsonl            (Step 1 ê²°ê³¼: ë¬¸ì„œ â†’ ì²­í¬)\n",
    "      â”œâ”€â”€ chunk_summaries.jsonl   (Step 2 ê²°ê³¼: ì²­í¬ â†’ ìš”ì•½)\n",
    "      â”œâ”€â”€ tree_nodes.jsonl        (Step 3 ê²°ê³¼: íŠ¸ë¦¬ êµ¬ì¡° ë…¸ë“œ)\n",
    "      â””â”€â”€ tree_root.json          (Step 3 ê²°ê³¼: ìµœì¢… ë£¨íŠ¸ ìš”ì•½)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f426e557-4b80-4320-b910-421b7494e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sentencepiece tokenizers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4fec32f3-cac1-4ba0-a2fa-2fb22be934f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0. ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "77b595d9-8500-489c-95de-4783f2541a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "00afaf3c-e89e-407e-835a-e51ebbf3e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ë…¸íŠ¸ë¶ì´ 13_RAPTOR ì•ˆì—ì„œ ì—´ë ¤ ìˆìœ¼ë‹ˆ, CWD ê¸°ì¤€ìœ¼ë¡œ ê³ ì •\n",
    "BASE = Path.cwd()                 # /.../09_Mini_Project/13_RAPTOR\n",
    "DATA = BASE / \"data\"\n",
    "OUT  = BASE / \"outputs\"\n",
    "SRC  = BASE / \"src\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8480c67c-ddae-48b6-be1d-5de75a2689f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR\n",
      "DATA: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/data\n",
      "OUT : /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs\n"
     ]
    }
   ],
   "source": [
    "print(\"BASE:\", BASE)\n",
    "print(\"DATA:\", DATA)\n",
    "print(\"OUT :\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e66f6a4d-abc7-457e-b5cd-3759ade35795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "86e3f1a8-ac26-48a6-871d-f9b767c64974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë¬¸ì¥ ë¶„ë¦¬ í•¨ìˆ˜ ===\n",
    "def split_sentences(text: str):\n",
    "    \"\"\"ê°„ë‹¨í•œ ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ê¸°ì¤€)\"\"\"\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s for s in sents if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "531ee492-14df-454d-b70f-352212702544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì²­í¬ ìƒì„± í•¨ìˆ˜ ===\n",
    "def chunk_by_sentences(sents, max_chars=2000):\n",
    "    \"\"\"ë¬¸ì¥ì„ ì´ì–´ ë¶™ì´ë‹¤ê°€ max_chars ë„˜ìœ¼ë©´ ìƒˆ ì²­í¬ ì‹œì‘\"\"\"\n",
    "    chunks, cur, cur_len = [], [], 0\n",
    "    for s in sents:\n",
    "        if cur_len + len(s) > max_chars and cur:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur, cur_len = [], 0\n",
    "        cur.append(s); cur_len += len(s) + 1\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "013ecc35-428c-4fc7-bf68-bbd719ffb0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Step 1: ë¬¸ì„œ ë¡œë“œ â†’ ì²­í¬ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "95e34b4a-f367-4125-81ef-c5c03e7dbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë¬¸ì„œ ê²½ë¡œ ì§€ì • (í˜„ì¬ ìœ„ì¹˜ì—ì„œ ../../11_data/ ì•ˆì— ìˆìŒ) ===\n",
    "# ë¬¸ì„œ ê²½ë¡œ: í˜„ì¬ ìœ„ì¹˜ì—ì„œ ../../11_data/ ì•„ë˜ì— íŒŒì¼ì´ ìˆìŒ\n",
    "DOC_NAME = \"01 Harry Potter and the Sorcerers Stone.txt\"\n",
    "doc_path = Path(\"../../11_data\") / DOC_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c9fa8d56-dff3-4ef3-992b-3e3c49cc5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ì½ê¸° â†’ ë¬¸ì¥ ë¶„ë¦¬ â†’ ì²­í¬ ìƒì„±\n",
    "text   = doc_path.read_text(encoding=\"utf-8\")\n",
    "sents  = split_sentences(text)\n",
    "chunks = chunk_by_sentences(sents, max_chars=2000)  # Jessicaê°€ 2000 ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d4fcbb47-9ec1-4511-a064-56a310e7daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë¬¸ì¥ ë¶„ë¦¬ & ì²­í¬ ìƒì„± ===\n",
    "# chunks.jsonl ì €ì¥\n",
    "chunk_path = OUT / \"chunks.jsonl\"\n",
    "with chunk_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for i, ch in enumerate(chunks, 1):\n",
    "        f.write(json.dumps({\n",
    "            \"chunk_id\": f\"C{i:04d}\",\n",
    "            \"text\": ch,\n",
    "            \"tokens\": len(ch.split())\n",
    "        }, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "db159c77-759e-44fe-9a73-9c6fbff2a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… chunks.jsonl ì €ì¥ ì™„ë£Œ: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs/chunks.jsonl\n",
      "ì´ ë¬¸ì¥ ìˆ˜: 5003\n",
      "ìƒì„±ëœ ì²­í¬ ìˆ˜: 227\n",
      "ì²« ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:\n",
      " M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youâ€™d expect to be involved in anything strange or mysterious, because they just didnâ€™t hold with such nonsense. Mr. Dursley was the director of a fi ...\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… chunks.jsonl ì €ì¥ ì™„ë£Œ:\", chunk_path)\n",
    "print(\"ì´ ë¬¸ì¥ ìˆ˜:\", len(sents))\n",
    "print(\"ìƒì„±ëœ ì²­í¬ ìˆ˜:\", len(chunks))\n",
    "print(\"ì²« ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:\\n\", chunks[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bdbfbc89-9813-4024-931c-801b9a6919d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ… ì…€ 3 â€” PEGASUS ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ef2c18e3-aba2-4e8c-9e5c-05938904cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2d6350ef-c7b8-465b-a47a-442bd5198042",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/pegasus-xsum\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "04d4c1ef-a7d6-4add-94c0-b7cecc314da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e696e49721654787a8c249d433b686b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ceddb9fddaf465189415b918e5b2d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0852e585d8444baa1046e19086038d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c52e126f7143c0b05b5b3db7115ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137603865fa24ba0a3fc1e9846fc92ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05473b1a3ad463caae7bdec5eba4797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9586f62e406c49969c16a1940bad9d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ë¡œì»¬ì— ìˆìœ¼ë©´ ë¡œì»¬ë¡œ ë¡œë“œ, ì—†ìœ¼ë©´ ìë™ ë‹¤ìš´ë¡œë“œ\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "238ff5db-45cf-4478-ad6a-a84292f2c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 â€” Chunk Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "63ece75b-3e14-4f61-81f2-e60c1861212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PEGASUS ì¤€ë¹„ ì™„ë£Œ (device=mps)\n"
     ]
    }
   ],
   "source": [
    "def summarize_pegasus(text, max_in=512, max_out=64, num_beams=4):\n",
    "    inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=max_in).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_out,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"âœ… PEGASUS ì¤€ë¹„ ì™„ë£Œ (device={device})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "60d02794-5adf-42db-9001-cc7ce0a38c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PEGASUS ë¡œì»¬ì— ì´ë¯¸ ìˆìŠµë‹ˆë‹¤.\n",
      "ì‚¬ìš© ì¥ì¹˜: mps\n"
     ]
    }
   ],
   "source": [
    "if have_local_model(model_name):\n",
    "    print(\"âœ… PEGASUS ë¡œì»¬ì— ì´ë¯¸ ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"â¬‡ï¸ PEGASUS ë‹¤ìš´ë¡œë“œ ì¤‘â€¦ (ì¸í„°ë„· í•„ìš”)\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)             # ë‹¤ìš´ë¡œë“œ\n",
    "    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name)     # ë‹¤ìš´ë¡œë“œ\n",
    "    print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"ì‚¬ìš© ì¥ì¹˜:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f167e5ef-4943-4cc3-9787-2d9e03a6810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) ìš”ì•½ í•¨ìˆ˜ (PEGASUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fd34834d-334d-450f-a4bf-d5bc47ecfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "chunks_path = OUT / \"chunks.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "27a5cd1c-01f0-4e08-af8c-a47eee15ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4-1) ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸: ì• 5ê°œë§Œ ---\n",
    "summ_smoke = OUT / \"chunk_summaries_smoke.jsonl\"\n",
    "sample = []\n",
    "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        sample.append(json.loads(line))\n",
    "        if i >= 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6727da59-1b15-4f9b-ab24-8cdc28481f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52417af31c02403c9c15550c0dd87cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Smoke summarizing (5):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìŠ¤ëª¨í¬ ìš”ì•½ ì €ì¥: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs/chunk_summaries_smoke.jsonl\n"
     ]
    }
   ],
   "source": [
    "with open(summ_smoke, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for obj in tqdm(sample, desc=\"Smoke summarizing (5)\"):\n",
    "        cid, text = obj[\"chunk_id\"], obj[\"text\"]\n",
    "        summ = summarize_pegasus(text, max_in=512, max_out=96, num_beams=4)  # ì‚´ì§ ê¸¸ê²Œ\n",
    "        item = {\n",
    "            \"chunk_id\": cid,\n",
    "            \"summary\": summ,\n",
    "            \"key_points\": [s.strip() for s in summ.split(\". \") if s.strip()][:4]\n",
    "        }\n",
    "        fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"âœ… ìŠ¤ëª¨í¬ ìš”ì•½ ì €ì¥:\", summ_smoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9c60908f-86d3-447c-907e-99f5e75ac819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>key_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0001</td>\n",
       "      <td>This is the story of the Dursleys and the Potters.</td>\n",
       "      <td>[This is the story of the Dursleys and the Potters.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C0002</td>\n",
       "      <td>The Dursleys left the house for the day, with Mr. Dursley couldnâ€™t bear people who dressed in funny clothes â€” the getups you saw on young people!</td>\n",
       "      <td>[The Dursleys left the house for the day, with Mr, Dursley couldnâ€™t bear people who dressed in funny clothes â€” the getups you saw on young people!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0003</td>\n",
       "      <td>On the morning of the first day of school, Mr.</td>\n",
       "      <td>[On the morning of the first day of school, Mr.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0004</td>\n",
       "      <td>The first thing Mr.</td>\n",
       "      <td>[The first thing Mr.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0005</td>\n",
       "      <td>Dudley and Petunia Dursley had a strange day.</td>\n",
       "      <td>[Dudley and Petunia Dursley had a strange day.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_id  \\\n",
       "0    C0001   \n",
       "1    C0002   \n",
       "2    C0003   \n",
       "3    C0004   \n",
       "4    C0005   \n",
       "\n",
       "                                                                                                                                             summary  \\\n",
       "0                                                                                                 This is the story of the Dursleys and the Potters.   \n",
       "1  The Dursleys left the house for the day, with Mr. Dursley couldnâ€™t bear people who dressed in funny clothes â€” the getups you saw on young people!   \n",
       "2                                                                                                     On the morning of the first day of school, Mr.   \n",
       "3                                                                                                                                The first thing Mr.   \n",
       "4                                                                                                      Dudley and Petunia Dursley had a strange day.   \n",
       "\n",
       "                                                                                                                                            key_points  \n",
       "0                                                                                                 [This is the story of the Dursleys and the Potters.]  \n",
       "1  [The Dursleys left the house for the day, with Mr, Dursley couldnâ€™t bear people who dressed in funny clothes â€” the getups you saw on young people!]  \n",
       "2                                                                                                     [On the morning of the first day of school, Mr.]  \n",
       "3                                                                                                                                [The first thing Mr.]  \n",
       "4                                                                                                      [Dudley and Petunia Dursley had a strange day.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df_smoke = pd.read_json(summ_smoke, lines=True)\n",
    "display(df_smoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2ba35e02-a4e9-40f5-8c4c-2db1cb1d829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… meta: pegasus-xsum(í”„ë¡¬í”„íŠ¸ ê°•í™”) ì‚¬ìš©\n",
      "fanout = 2, leaves = 5\n"
     ]
    }
   ],
   "source": [
    "# 3. meta_summarize ì„ íƒ (ë©€í‹°ë‰´ìŠ¤ ì‹œë„ â†’ ì‹¤íŒ¨í•˜ë©´ XSum í´ë°±)\n",
    "TRY_MULTINEWS = False  # ë©€í‹°ë‰´ìŠ¤ì—ì„œ ì—ëŸ¬ ë§ì´ ë‚¬ìœ¼ë‹ˆ ìš°ì„  False ê¶Œì¥. OKë©´ Trueë¡œ ë°”ê¿”ë„ ë¨.\n",
    "\n",
    "# summarize_pegasusê°€ ìœ„ì—ì„œ ì •ì˜ë¼ ìˆì–´ì•¼ XSum í´ë°±ì´ ì‘ë™í•´ìš”.\n",
    "if TRY_MULTINEWS:\n",
    "    try:\n",
    "        meta_summarize = _build_meta_with_multinews()\n",
    "        print(\"âœ… meta: pegasus-multi_news ì‚¬ìš©\")\n",
    "    except Exception as e:\n",
    "        print(f\"â„¹ï¸ multi_news ë¡œë“œ ì‹¤íŒ¨ â†’ XSumìœ¼ë¡œ í´ë°±: {e}\")\n",
    "        meta_summarize = _build_meta_with_xsum()\n",
    "        print(\"âœ… meta: pegasus-xsum(í”„ë¡¬í”„íŠ¸ ê°•í™”) ì‚¬ìš©\")\n",
    "else:\n",
    "    meta_summarize = _build_meta_with_xsum()\n",
    "    print(\"âœ… meta: pegasus-xsum(í”„ë¡¬í”„íŠ¸ ê°•í™”) ì‚¬ìš©\")\n",
    "\n",
    "# 4. fanout ì„¤ì • (ìŠ¤ëª¨í¬=2, ì „ëŸ‰=6)\n",
    "fanout = 2 if len(leaves) <= 10 else 6\n",
    "print(f\"fanout = {fanout}, leaves = {len(leaves)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2875c2b3-42ad-4f45-a256-764e722de774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#íŠ¸ë¦¬ë¹Œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "74a5e6d4-35d0-446e-9b04-ad78795c4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. íŠ¸ë¦¬ ë¹Œë“œ\n",
    "level, nodes, current = 0, [], leaves\n",
    "while len(current) > 1:\n",
    "    level += 1\n",
    "    grouped = [current[i:i+fanout] for i in range(0, len(current), fanout)]\n",
    "    next_level = []\n",
    "    for gi, group in enumerate(grouped, 1):\n",
    "        children = [cid for cid,_ in group]\n",
    "        texts    = [t   for _,t   in group]\n",
    "        summ = meta_summarize(texts)\n",
    "        node_id = f\"L{level}_N{gi:04d}\"\n",
    "        nodes.append({\"node_id\": node_id, \"level\": level, \"children\": children, \"summary\": summ})\n",
    "        next_level.append((node_id, summ))\n",
    "    current = next_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "500a6187-a217-4666-9bc1-d213f30863e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tree_nodes.jsonl: /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs/tree_nodes.jsonl\n",
      "âœ… tree_root.json : /Users/jessicahong/gitclone/NLP_study/09_Mini_Project/13_RAPTOR/outputs/tree_root.json\n",
      "\n",
      "ğŸ“Œ Root Summary:\n",
      " {\n",
      "  \"root_id\": \"L3_N0001\",\n",
      "  \"summary\": \"Do you think you know what is going on in the story?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## 6. ì €ì¥ + ë£¨íŠ¸ ë¯¸ë¦¬ë³´ê¸°\n",
    "root_id, root_text = current[0]\n",
    "nodes_path.write_text(\"\\n\".join(json.dumps(n, ensure_ascii=False) for n in nodes), encoding=\"utf-8\")\n",
    "root_path.write_text(json.dumps({\"root_id\": root_id, \"summary\": root_text}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… tree_nodes.jsonl:\", nodes_path)\n",
    "print(\"âœ… tree_root.json :\", root_path)\n",
    "print(\"\\nğŸ“Œ Root Summary:\\n\", root_path.read_text(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b4bc4da1-581b-44c6-a2cf-9d64b64131f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_meta_with_xsum():\n",
    "    # summarize_pegasus() ê°€ ì´ë¯¸ ì •ì˜ë¼ ìˆë‹¤ê³  ê°€ì • (google/pegasus-xsum)\n",
    "    def _fn(texts, max_in=512, max_out=220, num_beams=8):\n",
    "        prompt = (\n",
    "            \"Summarize the following bullet points into a cohesive 4â€“6 sentence paragraph. \"\n",
    "            \"Write declarative sentences only (no questions, no instructions). \"\n",
    "            \"Include main characters, setting, key events/conflict, and why it matters.\\n\\n\"\n",
    "            + \"\\n\".join(f\"- {t}\" for t in texts)\n",
    "        )\n",
    "        out = summarize_pegasus(prompt, max_in=max_in, max_out=max_out, num_beams=num_beams)\n",
    "        bad = out.strip().endswith(\"?\") or out.strip().lower().startswith((\"how \", \"do you \", \"what \")) or len(out.split()) < 35\n",
    "        if bad:  # í•œ ë²ˆ ì¬ì‹œë„\n",
    "            out = summarize_pegasus(prompt, max_in=max_in, max_out=max_out+40, num_beams=num_beams+2)\n",
    "        return out\n",
    "    return _fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f25d4f59-8f89-4108-8c23-e62bf941d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ multi_news ë¡œë“œ ì‹¤íŒ¨, XSumìœ¼ë¡œ í´ë°±: Couldn't instantiate the backend tokenizer from one of: \n",
      "(1) a `tokenizers` library serialization file, \n",
      "(2) a slow tokenizer instance to convert or \n",
      "(3) an equivalent slow tokenizer class to instantiate and convert. \n",
      "You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n",
      "âœ… meta: pegasus-xsum(í”„ë¡¬í”„íŠ¸ ê°•í™”) ì‚¬ìš©\n"
     ]
    }
   ],
   "source": [
    "# --- meta_summarize ì¤€ë¹„ (multi_news â†’ ì‹¤íŒ¨ ì‹œ xsum) ---\n",
    "try:\n",
    "    meta_summarize = _build_meta_with_multinews()\n",
    "    print(\"âœ… meta: pegasus-multi_news ì‚¬ìš©\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸ multi_news ë¡œë“œ ì‹¤íŒ¨, XSumìœ¼ë¡œ í´ë°±: {e}\")\n",
    "    meta_summarize = _build_meta_with_xsum()\n",
    "    print(\"âœ… meta: pegasus-xsum(í”„ë¡¬í”„íŠ¸ ê°•í™”) ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "070efe1c-ffd9-4379-a81d-b3e75a5a109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAPTOR retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7f7cde94-5ddc-49a9-b4cc-c17256e1f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9008c9de-0620-4e22-8e33-9e0c5eb69759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT ì„ë² ë”© ìš°ì„ , ì‹¤íŒ¨ì‹œ TF-IDFë¡œ í´ë°±\n",
    "_USE_SBERT = True\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    _USE_SBERT = False\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea58b7e-4b43-4137-9d84-657651d81504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d651fd9d-e109-4349-b0e4-38ca9a085ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) íŒŒì´í”„ë¼ì¸ ê²€ì¦: ìŠ¤ëª¨í¬ 5ê°œë¡œ íŠ¸ë¦¬ ë§Œë“¤ì–´ë³´ê¸°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
