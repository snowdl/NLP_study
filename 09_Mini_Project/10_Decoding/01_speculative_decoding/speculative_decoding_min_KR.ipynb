{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9142b1f5-3ce0-48a3-bb82-769079447061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install & Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "86805c62-3986-4ace-a88f-422bb2eeb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4fd98278-dc91-4b37-b9c6-eecb5531d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… device: mps\n"
     ]
    }
   ],
   "source": [
    "# Pick device (Apple Silicon â†’ mps, else cuda if available, else cpu)\n",
    "device = (\n",
    "    \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"âœ… device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a81aefb-a833-4c45-8d85-aea72bb9152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft (fast, tiny) and Target (a bit larger)\n",
    "draft_id  = \"distilgpt2\"\n",
    "target_id = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5de1993b-12be-4569-89dd-3999fd299985",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(target_id)\n",
    "tok.pad_token = tok.eos_token  # silence warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d819d900-1441-419a-82ba-4b95e8305937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft  = AutoModelForCausalLM.from_pretrained(draft_id)\n",
    "draft  = draft.to(device)\n",
    "draft.eval()\n",
    "\n",
    "target = AutoModelForCausalLM.from_pretrained(target_id)\n",
    "target = target.to(device)\n",
    "target.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b025984-a8cd-4e3e-b836-48b61ecb6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: reproducibility\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a5cd3d98-69de-4d78-a542-7390b3d3b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 Draft â†’ propose ONE token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bcdaf90b-9cae-42c9-b8cd-b0241346baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d7831be6-0c36-4aa2-8887-820d69b7142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Draft â†’ propose ONE token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0876a9b2-53fe-417e-b91a-ee640d19aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def draft_next_token(input_ids: torch.Tensor) -> Tuple[torch.Tensor, int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    draft ëª¨ë¸ë¡œ ë‹¤ìŒ í† í° 1ê°œë¥¼ greedyë¡œ ì œì•ˆí•˜ê³ ,\n",
    "    ê·¸ í† í°ì„ input_idsì— ë°”ë¡œ ì´ì–´ë¶™ì—¬ ë°˜í™˜.\n",
    "\n",
    "    Args:\n",
    "        input_ids: [1, T] í† í° IDs (draftì™€ ê°™ì€ device)\n",
    "\n",
    "    Returns:\n",
    "        new_ids : [1, T+1]  ì´ì–´ë¶™ì¸ ì‹œí€€ìŠ¤\n",
    "        token_id: int       ì œì•ˆëœ í† í° ID (ìŠ¤ì¹¼ë¼)\n",
    "        logits  : [1, V]    ë§ˆì§€ë§‰ ìŠ¤í… ë¡œì§“(softmax ì „)\n",
    "    \"\"\"\n",
    "    # 1) forward\n",
    "    out = draft(input_ids=input_ids)          # out.logits: [1, T, V]\n",
    "\n",
    "    # 2) ë§ˆì§€ë§‰ ìŠ¤í… ë¡œì§“ë§Œ ì¶”ì¶œ\n",
    "    logits = out.logits[:, -1, :]             # [1, V]\n",
    "\n",
    "    # 3) greedyë¡œ ë‹¤ìŒ í† í° ì„ íƒ\n",
    "    next_id = torch.argmax(logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "    # 4) ì„ íƒ í† í°ì„ ì‹œí€€ìŠ¤ì— ì´ì–´ë¶™ì´ê¸°\n",
    "    new_ids = torch.cat([input_ids, next_id], dim=1)      # [1, T+1]\n",
    "\n",
    "    return new_ids, int(next_id.item()), logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14cab615-4a6d-4ab6-94dd-e9923a475fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added token: 262 ' the'\n",
      "new shape: torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a quiet village by the sea,\"\n",
    "enc = tok(prompt, return_tensors=\"pt\")\n",
    "ids = enc.input_ids.to(device)                # [1, T]\n",
    "\n",
    "ids, token_id, logits = draft_next_token(ids)\n",
    "print(\"added token:\", token_id, repr(tok.decode([token_id])))\n",
    "print(\"new shape:\", ids.shape)                # [1, T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1804b605-bd21-41d3-a6a8-8bb254654c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Draft â†’ propose K tokens  (draft_next_tokenì´ (new_ids, int, logits) ë¥¼ ë°˜í™˜í•˜ëŠ” ë²„ì „ê³¼ í˜¸í™˜)\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def draft_propose_k(input_ids: torch.Tensor, k: int = 4, temperature: float = 0.8) -> List[int]:\n",
    "    \"\"\"\n",
    "    Greedyë¡œ K í† í°ì„ ì—°ì† ì œì•ˆ.\n",
    "    NOTE: draft_next_tokenì´ (new_ids, token_id, logits)ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ\n",
    "          ì—¬ê¸°ì„œëŠ” ì´ì–´ë¶™ì„ì„ ì§ì ‘ í•˜ì§€ ì•Šê³ , draft_next_tokenì´ ëŒë ¤ì¤€ new_idsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    ids = input_ids.clone()\n",
    "    proposals: List[int] = []\n",
    "    for _ in range(k):\n",
    "        ids, nid, _ = draft_next_token(ids)     # â† 3ê°œ ì–¸íŒ¨í‚¹ (ì´ì–´ë¶™ì¸ idsê°€ ëŒì•„ì˜´)\n",
    "        proposals.append(nid)\n",
    "    return proposals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d957f9ea-ac00-491f-aaec-33d695228642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Target â†’ top-1 token (Jupyter cell)\n",
    "\n",
    "@torch.no_grad()\n",
    "def target_top1(input_ids: torch.Tensor) -> int:\n",
    "    out = target(input_ids=input_ids)      # [1, T, V]\n",
    "    logits = out.logits[:, -1, :]          # [1, V]\n",
    "    return int(torch.argmax(logits, dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4a927d7-cf7a-4479-9b00-90d5ed984841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Target â†’ sample ONE token on reject (Jupyter cell)\n",
    "\n",
    "@torch.no_grad()\n",
    "def target_sample_one(input_ids: torch.Tensor, temperature: float = 0.7) -> int:\n",
    "    out = target(input_ids=input_ids)\n",
    "    logits = out.logits[:, -1, :]\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    next_id = torch.multinomial(probs, num_samples=1)\n",
    "    return int(next_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8a9b391c-12a1-4c3d-83b4-3ffe35ce94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Verify one cycle \n",
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "@torch.no_grad()\n",
    "def verify_one_cycle(input_ids: torch.Tensor, proposed: List[int], temperature: float = 0.7) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Iterate over proposed tokens:\n",
    "      - If target top-1 == proposed token â†’ accept (append).\n",
    "      - Else â†’ sample ONE token from target, append it, and STOP this cycle.\n",
    "    Returns: (new_input_ids, num_accepted_in_this_cycle)\n",
    "    \"\"\"\n",
    "    ids = input_ids.clone()\n",
    "    accepted = 0\n",
    "    for t in proposed:\n",
    "        top1 = target_top1(ids)\n",
    "        if top1 == t:  # accept\n",
    "            ids = torch.cat([ids, torch.tensor([[t]], device=ids.device)], dim=1)\n",
    "            accepted += 1\n",
    "        else:          # reject â†’ sample 1 and stop cycle\n",
    "            samp = target_sample_one(ids, temperature=temperature)\n",
    "            ids = torch.cat([ids, torch.tensor([[samp]], device=ids.device)], dim=1)\n",
    "            break\n",
    "    return ids, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e9d97f9-8d2c-4263-af0f-5bd4137587c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Speculative loop (Jupyter cell)\n",
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def speculative_generate_minimal(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 60,\n",
    "    k: int = 4,\n",
    "    draft_temp: float = 0.8,   # ì „ë‹¬ì€ í•˜ì§€ë§Œ draft_propose_kì—ì„œ ì‹¤ì œë¡œëŠ” ë¬´ì‹œ(greedy ë²„ì „)\n",
    "    target_temp: float = 0.7,  # verify_one_cycleì—ì„œ ì‚¬ìš©\n",
    ") -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Minimal speculative decoding ë£¨í”„:\n",
    "      1) draftê°€ Kê°œ í† í°ì„ ì œì•ˆ(í˜„ì¬ draft_propose_këŠ” greedyì´ë¯€ë¡œ temperatureë¥¼ ë¬´ì‹œ)\n",
    "      2) targetì´ ìˆœì°¨ ê²€ì¦(ì¼ì¹˜í•˜ë©´ ì±„íƒ, ë¶ˆì¼ì¹˜ ì‹œ 1í† í° ìƒì„± í›„ ì‚¬ì´í´ ì¢…ë£Œ)\n",
    "      3) ì˜ˆì‚°(max_new_tokens)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ\n",
    "\n",
    "    Returns:\n",
    "        (ìƒì„± í…ìŠ¤íŠ¸, ì´ ìˆ˜ë½ëœ í† í° ìˆ˜)\n",
    "    \"\"\"\n",
    "    enc = tok(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    base_len = input_ids.shape[1]\n",
    "    total_accepted = 0\n",
    "\n",
    "    while (input_ids.shape[1] - base_len) < max_new_tokens:\n",
    "        # 1) ì œì•ˆ: draft_propose_këŠ” ì‹œê·¸ë‹ˆì²˜ìƒ temperatureë¥¼ ë°›ì§€ë§Œ í˜„ì¬ êµ¬í˜„ì€ greedyë¼ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "        proposed = draft_propose_k(input_ids, k=k, temperature=draft_temp)\n",
    "\n",
    "        # 2) ê²€ì¦/ë³‘í•©: targetì´ ì œì•ˆ í† í°ì„ í™•ì¸ (ì—¬ê¸°ì„œ target_tempëŠ” ì‹¤ì œ ì‚¬ìš©)\n",
    "        input_ids, acc = verify_one_cycle(input_ids, proposed, temperature=target_temp)\n",
    "        total_accepted += acc\n",
    "\n",
    "        # 3) ì•ˆì „ì¥ì¹˜: ì´ë²ˆ ì‚¬ì´í´ì—ì„œ ì•„ë¬´ê²ƒë„ ìˆ˜ë½ë˜ì§€ ì•Šì•˜ë‹¤ë©´ target top-1ë¡œ í•œ ìŠ¤í… ì „ì§„\n",
    "        if acc == 0 and (input_ids.shape[1] - base_len) < max_new_tokens:\n",
    "            nid = target_top1(input_ids)\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[nid]], device=device)], dim=1)\n",
    "\n",
    "    text = tok.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return text, total_accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d3961525-e914-4530-b9c4-085cc452b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Prompt: In a quiet village by the sea,\n",
      "âœ… Accepted tokens (by target): 32\n",
      "\n",
      "=== Output ===\n",
      " In a quiet village by the sea, the village is a mess. A few soldiers are stationed there, but none of them are there to help.\n",
      "\n",
      "\"I'm not going to say anything! I'm just going to say what I want to say!\"\n",
      "\n",
      "\"I'm going to say what I want to say!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.7 Run test\n",
    "\n",
    "prompt = \"In a quiet village by the sea,\"\n",
    "text, accepted = speculative_generate_minimal(\n",
    "    prompt, max_new_tokens=60, k=3, draft_temp=0.7, target_temp=0.7\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Prompt:\", prompt)\n",
    "print(\"âœ… Accepted tokens (by target):\", accepted)\n",
    "print(\"\\n=== Output ===\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f0146-3ffb-41b2-be7b-6bfd3bacc176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
