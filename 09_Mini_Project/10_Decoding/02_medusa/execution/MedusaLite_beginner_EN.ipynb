{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e46353b7-7078-415f-9040-3f20f26b8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medusa-lite flow : drafter ‚Üí verifier ‚Üí multi-branch prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ab43540-a679-4dfc-bea9-767a75607adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b722306-3387-4100-a24c-ac353774f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Device ÏÑ†ÌÉù\n",
    "import torch\n",
    "\n",
    "def pick_device():\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"     # üñ•Ô∏è Îß•Î∂ÅÏù¥Î©¥ mps\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"    # üíª GPU ÏûàÏúºÎ©¥ cuda\n",
    "    return \"cpu\"         # ÎÇòÎ®∏ÏßÄÎäî cpu\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"‚úÖ DEVICE =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca4d20be-51af-47dc-b8ed-c2d34cf39cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert DEVICE in {\"cpu\", \"cuda\", \"mps\"}\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd100859-26a7-428d-b08c-8f99c54e7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed Í≥†Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7f0a711-b21e-4678-93c3-14d51d3c550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ seed set\n"
     ]
    }
   ],
   "source": [
    "import random, torch\n",
    "def set_seed(seed:int=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print('‚úÖ seed set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d7b0024-611a-4f72-ac17-ba8b68b9b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87d19e12-3cde-40b6-9910-5a95ecaa3118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cfg(DRAFTER_ID='distilgpt2', VERIFIER_ID='gpt2', TOPK_BRANCH=3, DRAFT_SPAN=3, MAX_NEW_TOKENS=80, TEMPERATURE=0.7, DEVICE='mps')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    DRAFTER_ID: str = 'distilgpt2'     # ÏÜåÌòï drafter\n",
    "    VERIFIER_ID: str = 'gpt2'   # Í≤ÄÏ¶ùÏö©\n",
    "    TOPK_BRANCH: int = 3\n",
    "    DRAFT_SPAN: int = 3\n",
    "    MAX_NEW_TOKENS: int = 80\n",
    "    TEMPERATURE: float = 0.7\n",
    "    DEVICE: str = DEVICE\n",
    "\n",
    "cfg = Cfg()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c043eb52-3954-4c4c-9f2a-7e5a53e8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Draft model (Tokenizer-> Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d41fc38-94fc-45b5-b5ef-ca9cf8f17a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ drafter ready\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "drafter_tok = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "drafter     = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE)\n",
    "drafter.eval()\n",
    "\n",
    "print('üî∏ drafter ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ecdb4e0-8a50-4942-88a1-b90a9c8b0bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drafter loaded? -> True\n"
     ]
    }
   ],
   "source": [
    "ok = (drafter_tok is not None) and (drafter is not None)\n",
    "print('drafter loaded? ->', ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e42e5382-50d3-4124-b0e6-67ae4935ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifier load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c104a01-09e4-425b-b95a-0b6561b9c787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ verifier ready\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "verifier     = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE)\n",
    "verifier.eval()\n",
    "\n",
    "print('üî∏ verifier ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7aa2a9d3-a8bb-4ef5-9020-87683f8a8fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verifier loaded? -> True\n"
     ]
    }
   ],
   "source": [
    "ok = (verifier_tok is not None) and (verifier is not None)\n",
    "print('verifier loaded? ->', ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aeb72db2-cd74-48aa-a0a3-70d618b1c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ok? True | shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "#Prompt & Context preparation# Prompt & Context\n",
    "prompt = \"In a distant future, a small crew of explorers discovers \"\n",
    "\n",
    "# drafter ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ°ú Ïù∏ÏΩîÎî© + DEVICE Ïò¨Î¶¨Í∏∞\n",
    "ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "\n",
    "# input_idsÎßå Îî∞Î°ú Í∫ºÎÇ¥Í∏∞\n",
    "input_ids = ctx[\"input_ids\"]\n",
    "\n",
    "print(\"context ok?\", ctx is not None, \"| shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8516d202-1746-4430-9347-17506532cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draft Ìïú ÌÜ†ÌÅ∞ ÏÉùÏÑ± Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a23bc6a6-8263-4f83-8858-1c9d93865d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.inference_mode()\n",
    "def draft_one_token(model, ids, temperature:float=1.0):\n",
    "    # 1) ÎßàÏßÄÎßâ ÌÜ†ÌÅ∞ ÏúÑÏπòÏùò logits Í∫ºÎÇ¥Í∏∞\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "\n",
    "    # 2) softmax ‚Üí ÌôïÎ•† Î∂ÑÌè¨\n",
    "    probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    # 3) ÌôïÎ•† Î∂ÑÌè¨ÏóêÏÑú Ìïú Í∞ú ÌÜ†ÌÅ∞ ÎΩëÍ∏∞\n",
    "    next_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    return next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ca643d6-c40b-42a0-abc4-4d98157578d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏÉòÌîåÎßÅÎêú ÌÜ†ÌÅ∞ ID: tensor([[2575]], device='mps:0')\n",
      "ÌÜ†ÌÅ∞ Î¨∏ÏûêÏó¥: urch\n"
     ]
    }
   ],
   "source": [
    " # Ïã§Ï†úÎ°ú Ïã§ÌñâÌï¥Î≥¥Í∏∞\n",
    "sample_id = draft_one_token(drafter, input_ids, 0.8)\n",
    "print(\"ÏÉòÌîåÎßÅÎêú ÌÜ†ÌÅ∞ ID:\", sample_id)\n",
    "print(\"ÌÜ†ÌÅ∞ Î¨∏ÏûêÏó¥:\", drafter_tok.decode(sample_id[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2392dfbb-de90-4c85-93cc-324c928f2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Î©ÄÌã∞-Î∏åÎûúÏπò Draft Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de327be5-fa7d-47cc-8ec8-3cf41de470da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topkÍ∞úÏùò Î∏åÎûúÏπò, Í∞Å Î∏åÎûúÏπòÎßàÎã§ span Í∏∏Ïù¥ÎßåÌÅº draft_one_token Î∞òÎ≥µ\n",
    "import torch\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, topk:int, span:int, temperature:float):\n",
    "    branches = []\n",
    "    for _ in range(topk):\n",
    "        cur = ids.clone()\n",
    "        branch = []\n",
    "        for __ in range(span):\n",
    "            nxt = draft_one_token(drafter, cur, temperature)\n",
    "            cur = torch.cat([cur, nxt], dim=1)\n",
    "            branch.append(int(nxt[0,0]))\n",
    "        branches.append(branch)\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12798343-a426-4754-a9c9-db3ee0304748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 branches; span len: 3\n",
      "ÏòàÏãú Î∏åÎûúÏπò 1: [1849, 286, 262]\n"
     ]
    }
   ],
   "source": [
    "b = drafter_propose(input_ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(len(b), 'branches; span len:', len(b[0]) if b else None)\n",
    "print('ÏòàÏãú Î∏åÎûúÏπò 1:', b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93eaab97-53d5-4d35-9772-a90b97043a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifier: Ìïú ÌÜ†ÌÅ∞ ÏòàÏ∏°(greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a602aa44-d7b8-4cdf-a48d-9c8a37fd274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def verifier_next_token(ids) -> int:\n",
    "    logits = verifier(ids).logits[:, -1, :]\n",
    "    pred = int(torch.argmax(logits, dim=-1)[0])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6abed687-dd54-4c6f-b1dc-895d46f33f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred id = 1849 | token = ¬†\n"
     ]
    }
   ],
   "source": [
    "vid = verifier_next_token(input_ids)\n",
    "print('pred id =', vid, '| token =', verifier_tok.decode([vid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42acd454-c409-4673-8d3b-c63d24b3c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix-Accept (mismatchÍπåÏßÄ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02cdbb53-4055-490f-80b6-b70f3d967954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch(context_ids, branch_tokens:List[int]) -> Tuple[torch.Tensor, List[int], bool]:\n",
    "    ids = context_ids.clone()\n",
    "    accepted = []\n",
    "    mismatched = False\n",
    "    for tid in branch_tokens:\n",
    "        pred = verifier_next_token(ids)\n",
    "        if pred == tid:\n",
    "            ids = torch.cat([ids, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "            accepted.append(tid)\n",
    "        else:\n",
    "            ids = torch.cat([ids, torch.tensor([[pred]], device=ids.device)], dim=1)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return ids, accepted, mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45b684a3-ec85-4092-ab4f-88c9a384cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted len: 1 | mismatched? True\n",
      "ÏÉà Í∏∏Ïù¥: 14 | Ï∂îÍ∞ÄÎêú ÌÜ†ÌÅ∞ Ïàò: 2\n"
     ]
    }
   ],
   "source": [
    "# Î∞©Í∏à ÎßåÎì† Î∏åÎûúÏπòÎì§ Ï§ë Ï≤´ Î≤àÏß∏Î•º Í≤ÄÏÇ¨Ìï¥Î≥¥Í∏∞\n",
    "new_ids, accepted, mism = accept_until_mismatch(input_ids, b[0])\n",
    "print('accepted len:', len(accepted), '| mismatched?', mism)\n",
    "print('ÏÉà Í∏∏Ïù¥:', new_ids.shape[1], '| Ï∂îÍ∞ÄÎêú ÌÜ†ÌÅ∞ Ïàò:', new_ids.shape[1] - input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b938a293-6969-4edb-8784-0e1b34b473a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator (medusa_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "510d8429-152b-42b2-ab3d-46c2994a0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee1be8b8-5047-4ee2-adde-26409e63722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch Ï†êÏàò Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8cea79e-4e6c-474b-895c-e745b078be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def score_branch(accepted, mismatched):\n",
    "    # prefix-acceptÎêú ÌÜ†ÌÅ∞ Ïàò - mismatch Ìå®ÎÑêÌã∞\n",
    "    return len(accepted) - (1 if mismatched else 0)\n",
    "\n",
    "# ‚úîÔ∏è Ï≤¥ÌÅ¨\n",
    "print(score_branch([1,2,3], False))  # 3\n",
    "print(score_branch([1,2], True))     # 1 (2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6414314-abf1-484f-9a9a-d782b3773b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt ‚Üí ÌÜ†ÌÅ∞Ìôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "757a2021-07af-452b-b469-d550e46e48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def encode_prompt(prompt: str):\n",
    "    ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    return ctx[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "512fc442-7b76-4fb3-9ffd-c618d859642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids.shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# ‚úîÔ∏è Ï≤¥ÌÅ¨\n",
    "ids = encode_prompt(\"In a distant future, \")\n",
    "print(\"ids.shape:\", ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c5a09ad-485a-4887-9fd8-c623132712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ìïú Ïä§ÌÖù ÏàòÌñâ(multi-branch‚ÜíÍ≤ÄÏ¶ù‚ÜíÏµúÍ≥† Ï†êÏàò Ï±ÑÌÉù)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9036549-1494-49c6-b2cb-0c55ea2ce21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_step(ids, topk_branch:int, draft_span:int, temperature:float):\n",
    "    branches = drafter_propose(ids, topk_branch, draft_span, temperature)\n",
    "    best_score = -10**9\n",
    "    best_ids = None\n",
    "    for br in branches:\n",
    "        new_ids, accepted, mism = accept_until_mismatch(ids, br)\n",
    "        s = score_branch(accepted, mism)\n",
    "        if s > best_score:\n",
    "            best_score, best_ids = s, new_ids\n",
    "    return best_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "689ee0dd-6059-4bcd-9556-fdf4a4b5d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 6 ‚Üí after: 7\n"
     ]
    }
   ],
   "source": [
    "ids2 = medusa_step(ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(\"before:\", ids.shape[1], \"‚Üí after:\", ids2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21a7e22b-51dd-4218-82f7-20555827dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21accee7-8bde-4019-a992-5255ca3a5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_generate(prompt:str,\n",
    "                    max_new_tokens:int=None,\n",
    "                    topk_branch:int=None,\n",
    "                    draft_span:int=None,\n",
    "                    temperature:float=None) -> str:\n",
    "    if max_new_tokens is None: max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "    if topk_branch   is None: topk_branch   = cfg.TOPK_BRANCH\n",
    "    if draft_span    is None: draft_span    = cfg.DRAFT_SPAN\n",
    "    if temperature   is None: temperature   = cfg.TEMPERATURE\n",
    "\n",
    "    ids = encode_prompt(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "\n",
    "    steps = math.ceil(max_new_tokens / draft_span)\n",
    "    for _ in range(steps):\n",
    "        ids = medusa_step(ids, topk_branch, draft_span, temperature)\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "    return drafter_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "78601636-7a26-4a4b-a2be-0f1cbd210e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, ¬†the world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "out = medusa_generate(\"In a distant future, \", 40)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a00ab2b-8e08-4ae9-8277-13002c95529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13) Greedy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3ed7155-d9e8-47b4-a337-4e64f12a4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int = None) -> str:\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    # verifier Í∏∞Ï§ÄÏúºÎ°ú ÏÉùÏÑ± (ÎπÑÍµêÍµ∞)\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = verifier(ids).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])  # greedy\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "200e4cc7-141e-4a4f-b3be-7925962cff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, ¬†the world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would\n"
     ]
    }
   ],
   "source": [
    "txt = greedy_generate(\"In a distant future, \", 40)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b6f4292-a915-4184-8f0b-b279ccffaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) A/B ÏÜçÎèÑ¬∑ÌÖçÏä§Ìä∏ ÎπÑÍµê ÏÖÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f924eb0c-f5c1-4ac3-a144-e3904c3ffc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è± greedy: 5.022 s\n",
      "‚è± medusa: 7.378 s\n",
      "\n",
      "--- greedy ---\n",
      " In a distant future, ¬†the world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place\n",
      "\n",
      "--- medusa ---\n",
      " In a distant future, ¬†the world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def time_it(fn, *args, **kwargs):\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    return out, time.perf_counter() - t0\n",
    "\n",
    "g_txt, g_t = time_it(greedy_generate, \"In a distant future, \", 80)\n",
    "m_txt, m_t = time_it(medusa_generate, \"In a distant future, \", 80)\n",
    "\n",
    "print(\"‚è± greedy:\", round(g_t, 3), \"s\")\n",
    "print(\"‚è± medusa:\", round(m_t, 3), \"s\")\n",
    "print(\"\\n--- greedy ---\\n\", g_txt[:400])\n",
    "print(\"\\n--- medusa ---\\n\", m_txt[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66a17-0016-4606-a4dc-03c8dae9450f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
