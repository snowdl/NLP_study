{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fee7308-0935-4af8-adfa-7fed10790122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Greedy ===\n",
      "In a distant future,   the world is a place where the world is a place where the world is a place where the world is a place where the world is a place \n",
      "\n",
      "=== Medusa-tiny ===\n",
      "In a distant future,   the world is a place where the world is a place where the world is a place where the world is a place where the world is a place where the\n"
     ]
    }
   ],
   "source": [
    "# medusa_super_tiny.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"distilgpt2\"\n",
    "device = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tok.eos_token_id is None: tok.eos_token = \"\"\n",
    "if tok.pad_token_id is None: tok.pad_token = tok.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(device).eval()\n",
    "model.config.use_cache = True\n",
    "EOS_ID = tok.eos_token_id\n",
    "\n",
    "def encode(s): return tok(s, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "def decode(ids): return tok.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def last_logits(ids):  # [V]\n",
    "    return model(ids).logits[0, -1, :]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def greedy_next(ids):  # argmax만\n",
    "    return int(torch.argmax(last_logits(ids)).item())\n",
    "\n",
    "@torch.inference_mode()\n",
    "def softmax_temp(logits, t=0.9):\n",
    "    t = max(float(t), 1e-6)\n",
    "    return torch.softmax(logits / t, dim=-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def top_p_indices(probs, top_p=0.95):\n",
    "    V = probs.numel()\n",
    "    if top_p is None or top_p >= 1: return torch.arange(V, device=probs.device)\n",
    "    sp, sx = torch.sort(probs, descending=True)\n",
    "    csum = torch.cumsum(sp, dim=0)\n",
    "    keep = csum <= top_p\n",
    "    keep[0] = True\n",
    "    return sx[keep]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def sample_next(ids, temperature=0.9, top_p=0.95):\n",
    "    probs = softmax_temp(last_logits(ids), temperature)\n",
    "    pool_ix = top_p_indices(probs, top_p)\n",
    "    pool = probs[pool_ix]\n",
    "    pool = pool / pool.sum()\n",
    "    pick_local = torch.multinomial(pool, 1)[0].item()\n",
    "    return int(pool_ix[pick_local].item())\n",
    "\n",
    "@torch.inference_mode()\n",
    "def propose_branch(ids, span=3, temperature=0.9, top_p=0.95):\n",
    "    cur = ids.clone()\n",
    "    out = []\n",
    "    for _ in range(span):\n",
    "        t = sample_next(cur, temperature, top_p)\n",
    "        out.append(t)\n",
    "        cur = torch.cat([cur, torch.tensor([[t]], device=cur.device)], dim=1)\n",
    "    return out  # list[int]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def prefix_accept_once(ids, branch):\n",
    "    cur = ids.clone()\n",
    "    accepted = 0\n",
    "    for t in branch:\n",
    "        g = greedy_next(cur)\n",
    "        if g == t:  # 일치 → 수락\n",
    "            cur = torch.cat([cur, torch.tensor([[t]], device=cur.device)], dim=1)\n",
    "            accepted += 1\n",
    "        else:       # 불일치 → 그리디 채택 후 중단\n",
    "            cur = torch.cat([cur, torch.tensor([[g]], device=cur.device)], dim=1)\n",
    "            break\n",
    "    return cur, accepted\n",
    "\n",
    "@torch.inference_mode()\n",
    "def medusa_tiny(prompt, max_new_tokens=30, span=3, temperature=0.9, top_p=0.95):\n",
    "    ids = encode(prompt)\n",
    "    start = ids.shape[1]\n",
    "    # 목표 길이 채울 때까지 반복 (진짜 최소형)\n",
    "    while ids.shape[1] - start < max_new_tokens:\n",
    "        branch = propose_branch(ids, span=span, temperature=temperature, top_p=top_p)\n",
    "        ids, _ = prefix_accept_once(ids, branch)\n",
    "    return decode(ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"In a distant future, \"\n",
    "    # 1) Greedy만\n",
    "    ids = encode(prompt)\n",
    "    for _ in range(30):\n",
    "        nxt = greedy_next(ids)\n",
    "        if EOS_ID is not None and nxt == EOS_ID: break\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "    print(\"=== Greedy ===\")\n",
    "    print(decode(ids), \"\\n\")\n",
    "\n",
    "    # 2) Medusa-tiny (초간단)\n",
    "    print(\"=== Medusa-tiny ===\")\n",
    "    print(medusa_tiny(prompt, max_new_tokens=30, span=3, temperature=0.9, top_p=0.95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9302e-a740-475b-88e5-9344b238b685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
