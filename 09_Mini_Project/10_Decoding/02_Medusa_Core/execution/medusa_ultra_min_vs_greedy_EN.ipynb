{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3475c78-d8a0-4fcc-b48f-56f515476238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMedusa Ultra-Min (Greedy vs Medusa) — module header with thorough English comments.\\n\\nThis file is organized into small, toggleable steps so you can learn and run the\\npipeline incrementally. Turn individual steps ON/OFF with the RUN_STEP_* flags\\nbelow. The typical execution order is STEP0 → STEP1 → STEP2 → … → STEP7.\\n\\nWorkflow overview (translated from your Korean outline):\\n\\nSTEP0 — Device selection\\n    - Detect a compute device in this priority: Apple Silicon `mps` → `cuda` → `cpu`.\\n    - Optionally fix random seeds for reproducibility.\\n\\nSTEP1 — Tokenizer / Model preparation\\n    - Load `tok` (AutoTokenizer) and `model` (AutoModelForCausalLM).\\n    - Ensure `eos_token` / `pad_token` are set; cache `EOS_ID`.\\n\\nSTEP2 — Utility functions\\n    - `encode`, `decode`, `append_token`, `last_logits`, `greedy_next`.\\n\\nSTEP3 — Sampling functions  ➜ (this is the part you said you’re focusing on now)\\n    - `softmax_temp`, `top_p_indices`, `sample_next` (with optional EOS-ban and repetition-ban).\\n\\nSTEP4 — Drafter (`propose_branch`)\\n    - Use sampling to propose a short branch of tokens (span length).\\n\\nSTEP5 — Prefix-accept (`prefix_accept_once`)\\n    - Compare drafter tokens vs greedy tokens; accept matching prefix.\\n    - On the first mismatch, take the greedy token and stop.\\n\\nSTEP6 — Loop execution (`medusa_tiny` / `run_greedy`)\\n    - Repeat propose + prefix-accept until `max_new_tokens` is reached.\\n\\nSTEP7 — Demo (print Greedy vs Medusa outputs)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Medusa Ultra-Min (Greedy vs Medusa) — module header with thorough English comments.\n",
    "\n",
    "This file is organized into small, toggleable steps so you can learn and run the\n",
    "pipeline incrementally. Turn individual steps ON/OFF with the RUN_STEP_* flags\n",
    "below. The typical execution order is STEP0 → STEP1 → STEP2 → … → STEP7.\n",
    "\n",
    "Workflow overview (translated from your Korean outline):\n",
    "\n",
    "STEP0 — Device selection\n",
    "    - Detect a compute device in this priority: Apple Silicon `mps` → `cuda` → `cpu`.\n",
    "    - Optionally fix random seeds for reproducibility.\n",
    "\n",
    "STEP1 — Tokenizer / Model preparation\n",
    "    - Load `tok` (AutoTokenizer) and `model` (AutoModelForCausalLM).\n",
    "    - Ensure `eos_token` / `pad_token` are set; cache `EOS_ID`.\n",
    "\n",
    "STEP2 — Utility functions\n",
    "    - `encode`, `decode`, `append_token`, `last_logits`, `greedy_next`.\n",
    "\n",
    "STEP3 — Sampling functions  ➜ (this is the part you said you’re focusing on now)\n",
    "    - `softmax_temp`, `top_p_indices`, `sample_next` (with optional EOS-ban and repetition-ban).\n",
    "\n",
    "STEP4 — Drafter (`propose_branch`)\n",
    "    - Use sampling to propose a short branch of tokens (span length).\n",
    "\n",
    "STEP5 — Prefix-accept (`prefix_accept_once`)\n",
    "    - Compare drafter tokens vs greedy tokens; accept matching prefix.\n",
    "    - On the first mismatch, take the greedy token and stop.\n",
    "\n",
    "STEP6 — Loop execution (`medusa_tiny` / `run_greedy`)\n",
    "    - Repeat propose + prefix-accept until `max_new_tokens` is reached.\n",
    "\n",
    "STEP7 — Demo (print Greedy vs Medusa outputs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34aed94a-face-4fff-ade1-e7b82fd55787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# =============================\n",
    "# RUN SWITCHES (turn individual steps ON/OFF)\n",
    "# =============================\n",
    "RUN_STEP_0 = True  # Device selection (and optional seeding)\n",
    "RUN_STEP_1 = True  # Tokenizer/model load\n",
    "RUN_STEP_2 = True  # Utilities\n",
    "RUN_STEP_3 = True  # Sampling\n",
    "RUN_STEP_4 = True  # Drafter\n",
    "RUN_STEP_5 = True  # Prefix-accept\n",
    "RUN_STEP_6 = True  # Loops\n",
    "RUN_STEP_7 = True  # Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d321dd9-5565-4e52-b8ad-bbbabaeea32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 0) Config\n",
    "# =============================\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    MODEL_ID: str = \"distilgpt2\"\n",
    "    TEMPERATURE: float = 0.9\n",
    "    TOP_P: float = 0.95\n",
    "    SPAN: int = 3\n",
    "    MAX_NEW_TOKENS: int = 30\n",
    "    BAN_EOS_FIRST_N: int = 0\n",
    "    REP_BAN_N: int = 0\n",
    "    SEED: int | None = 7\n",
    "    DEBUG: bool = False\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a81014a-e195-459a-b269-f5d989ae942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP0] DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "if RUN_STEP_0:\n",
    "    DEVICE = (\n",
    "        \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "        else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    "\n",
    "    def set_seed(seed: int | None) -> None:\n",
    "        if seed is None:\n",
    "            return\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    set_seed(cfg.SEED)\n",
    "    print(f\"[STEP0] DEVICE = {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079af4e7-4a88-425d-a710-443b348aaa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP1] MODEL = distilgpt2, EOS_ID=50256\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 1 —  Tokenizer/model load (eos/pad 보정)\n",
    "# =============================\n",
    "if RUN_STEP_1:\n",
    "    def load_tokenizer(model_id: str):\n",
    "        tok = AutoTokenizer.from_pretrained(model_id)\n",
    "         # If the tokenizer does not already define an EOS token,\n",
    "        # assign an empty string \"\" as a placeholder.\n",
    "        # (This ensures the model has a valid end-of-sequence marker.)\n",
    "        if tok.eos_token_id is None:\n",
    "            tok.eos_token = \"\"\n",
    "          # If the tokenizer does not already define a PAD token,\n",
    "        # reuse the EOS token as padding.\n",
    "        # (Many GPT-style models do not have a PAD token by default,\n",
    "        # but they can safely use EOS as padding since extra EOS tokens\n",
    "        # at the end of input do not change the model’s behavior.)          \n",
    "        if tok.pad_token_id is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        return tok\n",
    "\n",
    "    def load_model(model_id: str, device: str):\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id).to(device).eval()\n",
    "        model.config.use_cache = True\n",
    "        return model\n",
    "\n",
    "    tok = load_tokenizer(cfg.MODEL_ID)\n",
    "    model = load_model(cfg.MODEL_ID, DEVICE)\n",
    "    EOS_ID = tok.eos_token_id\n",
    "\n",
    "    print(f\"[STEP1] MODEL = {cfg.MODEL_ID}, EOS_ID={EOS_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bee3f608-e586-4a30-9d64-c7d8424a4576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP2] utils ready: encode/decode/append/last_logits/greedy_next\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 2 — Utility functions\n",
    "# (encode / decode / append / last_logits / greedy_next)\n",
    "# =============================\n",
    "if RUN_STEP_2:\n",
    "    def encode(text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert a string into token IDs.\n",
    "        - Input: raw text string\n",
    "        - Output: tensor of shape [1, T] moved onto DEVICE (mps/cuda/cpu)\n",
    "        \"\"\"\n",
    "        return tok(text, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "\n",
    "    def decode(ids: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Convert token IDs back into a string.\n",
    "        - skip_special_tokens=True removes EOS, PAD, etc. from the output\n",
    "        \"\"\"\n",
    "        return tok.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def append_token(ids: torch.Tensor, token_id: int) -> torch.Tensor:  #Helper to add one token to the end of the sequence.\n",
    "        \"\"\"\n",
    "        Append a single token ID to the current sequence.\n",
    "        - ids: tensor [1, T]\n",
    "        - token_id: int, the new token to add\n",
    "        - return: tensor [1, T+1]\n",
    "        \"\"\"\n",
    "        t = torch.tensor([[token_id]], device=ids.device)\n",
    "        return torch.cat([ids, t], dim=1)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def last_logits(ids: torch.Tensor) -> torch.Tensor:  #Get the model’s score distribution for the next token.\n",
    "        \"\"\"\n",
    "        Get the logits (unnormalized scores) of the last position.\n",
    "        - Forward pass the model on 'ids'\n",
    "        - Take logits from the last time step → shape [V]\n",
    "        where V = vocabulary size\n",
    "        \"\"\"\n",
    "        return model(ids).logits[0, -1, :]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def greedy_next(ids: torch.Tensor) -> int: #Pick the highest-scoring token (greedy decoding).\n",
    "        \"\"\"\n",
    "        Select the next token using greedy decoding.\n",
    "        - Take argmax over the last logits\n",
    "        - Return the token ID (int)\n",
    "        \"\"\"\n",
    "        return int(torch.argmax(last_logits(ids)).item())\n",
    "\n",
    "    print(\"[STEP2] utils ready: encode/decode/append/last_logits/greedy_next\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8996215f-d5ca-4c3a-b2b6-d6dd8601bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# STEP 3 — Sampling Functions\n",
    "# (softmax_temp / top_p_indices / sample_next)\n",
    "# ============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8186ae64-5524-40e6-ab01-f5199c9939c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP3a] ready: softmax_temp\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 3a — softmax_temp\n",
    "# =============================\n",
    "if RUN_STEP_3:\n",
    "    @torch.inference_mode()\n",
    "    def softmax_temp(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply temperature-scaled softmax to logits.\n",
    "        - logits: tensor [V], raw scores for each vocabulary token\n",
    "        - temperature < 1.0 → sharper distribution (more confident)\n",
    "        - temperature > 1.0 → flatter distribution (more random)\n",
    "        - returns: probability vector [V]\n",
    "        \"\"\"\n",
    "        t = max(float(temperature), 1e-6)   # avoid division by zero\n",
    "        return torch.softmax(logits / t, dim=-1)\n",
    "\n",
    "    print(\"[STEP3a] ready: softmax_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "024d635a-bc5b-44eb-b9a4-1a808bc725a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP3b] ready: top_p_indices\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 3b — top_p_indices/Nucleus Sampling (Top-p Sampling)\n",
    "# =============================\n",
    "if RUN_STEP_3:\n",
    "    @torch.inference_mode()\n",
    "    def top_p_indices(probs: torch.Tensor, top_p: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform nucleus (top-p) filtering.\n",
    "        - Sort tokens by probability (descending)\n",
    "        - Keep the smallest set of tokens whose cumulative prob ≤ top_p\n",
    "        - Always keep at least the top-1 token\n",
    "        - returns: indices of selected tokens\n",
    "        \"\"\"\n",
    "        V = probs.numel()\n",
    "        if top_p is None or top_p >= 1:\n",
    "            # keep all tokens\n",
    "            return torch.arange(V, device=probs.device)\n",
    "        sp, sx = torch.sort(probs, descending=True)   # sorted probs & indices\n",
    "        csum = torch.cumsum(sp, dim=0)                # cumulative sum\n",
    "        keep = csum <= top_p\n",
    "        keep[0] = True                                # ensure top-1 is included\n",
    "        return sx[keep]\n",
    "\n",
    "    print(\"[STEP3b] ready: top_p_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e54911-2633-453b-a008-21feeceebf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# STEP 3c — sample_next\n",
    "#Apply temperature scaling + nucleus (top-p) filtering + repetition ban + EOS ban option to sample the next token.\n",
    "# ============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f88bdbc9-f544-46fb-a0ec-1e6dbffd878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP3c] ready: sample_next\n"
     ]
    }
   ],
   "source": [
    "if RUN_STEP_3:\n",
    "    @torch.inference_mode()\n",
    "    def sample_next(ids: torch.Tensor, temperature: float, top_p: float,\n",
    "                    ban_eos: bool = False, rep_ban_n: int = 0) -> int:\n",
    "\n",
    "        # 1. Get logits for the last position and apply temperature-scaled softmax\n",
    "        logits = last_logits(ids)\n",
    "        probs = softmax_temp(logits, temperature)\n",
    "\n",
    "        # 2. Repetition ban: set probability of the last N tokens to 0\n",
    "        if rep_ban_n > 0:\n",
    "            tail = ids[0, -rep_ban_n:].tolist()\n",
    "            probs[tail] = 0\n",
    "            s = probs.sum()\n",
    "            # Re-normalize if valid; if all zero, fall back to original distribution\n",
    "            probs = probs if s <= 0 else probs / s\n",
    "            if s <= 0:\n",
    "                probs = softmax_temp(logits, temperature)  # fallback\n",
    "\n",
    "        # 3. Apply nucleus (top-p) filtering\n",
    "        pool_ix = top_p_indices(probs, top_p)\n",
    "        pool = probs[pool_ix]\n",
    "        pool = pool / pool.sum()\n",
    "\n",
    "        # 4. Randomly sample one token from the filtered distribution\n",
    "        pick_local = int(torch.multinomial(pool, 1)[0].item())\n",
    "        picked = int(pool_ix[pick_local].item())\n",
    "\n",
    "        # 5. EOS ban (optional): if EOS is chosen, resample once without EOS\n",
    "        if ban_eos and EOS_ID is not None and picked == EOS_ID:\n",
    "            mask = pool_ix != EOS_ID\n",
    "            if mask.any():\n",
    "                pool_ix2 = pool_ix[mask]\n",
    "                pool2 = pool[mask]\n",
    "                pool2 = pool2 / pool2.sum()\n",
    "                pick_local = int(torch.multinomial(pool2, 1)[0].item())\n",
    "                picked = int(pool_ix2[pick_local].item())\n",
    "\n",
    "        # Return the final sampled token ID\n",
    "        return picked\n",
    "\n",
    "    print(\"[STEP3c] ready: sample_next\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5293a22d-02c6-4641-9253-e4a6902c2fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP4] drafter ready: propose_one/propose_branch\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 4 — Drafter (propose_one / propose_branch)\n",
    "# =============================\n",
    "if RUN_STEP_4:\n",
    "    @torch.inference_mode()\n",
    "    def propose_one(cur_ids: torch.Tensor, temperature: float, top_p: float,\n",
    "                    accepted_so_far: int) -> int:\n",
    "        \"\"\"\n",
    "        Draft a single token proposal.\n",
    "        - Decide whether to ban EOS (ban_eos=True) based on how many tokens\n",
    "          have already been accepted (before BAN_EOS_FIRST_N threshold).\n",
    "        - Call sample_next() once with the given settings.\n",
    "        - Return: one proposed token ID.\n",
    "        \"\"\"\n",
    "        ban_eos = accepted_so_far < cfg.BAN_EOS_FIRST_N\n",
    "        return sample_next(cur_ids, temperature, top_p,\n",
    "                           ban_eos=ban_eos, rep_ban_n=cfg.REP_BAN_N)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def propose_branch(ids: torch.Tensor, span: int, temperature: float, top_p: float,\n",
    "                       accepted_so_far: int = 0) -> List[int]:\n",
    "        \"\"\"\n",
    "        Draft a short branch of tokens (length = span).\n",
    "        - Repeatedly call propose_one() 'span' times.\n",
    "        - Append each proposed token to the running sequence so that the next\n",
    "          proposal is conditioned on the previous draft tokens.\n",
    "        - Collect all proposed token IDs in a list.\n",
    "        - If DEBUG is enabled, print each drafted token and its decoded form.\n",
    "        - Return: list of proposed token IDs (length = span).\n",
    "        \"\"\"\n",
    "        cur = ids.clone()\n",
    "        out: List[int] = []\n",
    "        for i in range(span):\n",
    "            t = propose_one(cur, temperature, top_p, accepted_so_far)\n",
    "            out.append(t)\n",
    "            cur = append_token(cur, t)\n",
    "            if cfg.DEBUG:\n",
    "                print(f\"  [draft {i}] pick={t} ({tok.decode([t])!r})\")\n",
    "        return out\n",
    "\n",
    "    print(\"[STEP4] drafter ready: propose_one/propose_branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e34e5ed8-f83e-48d0-8ede-67188de91849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP5] prefix-accept ready: accept_one/prefix_accept_once\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 5 — Prefix-Accept (accept_one / prefix_accept_once)\n",
    "# =============================\n",
    "if RUN_STEP_5:\n",
    "    @torch.inference_mode()\n",
    "    def accept_one(cur: torch.Tensor, token: int) -> Tuple[torch.Tensor, bool]:\n",
    "        \"\"\"\n",
    "        Compare one drafted token vs the greedy token.\n",
    "        - Compute the greedy token 'g' for the current sequence.\n",
    "        - If the drafted token == greedy token → accept it and continue.\n",
    "        - If different → append the greedy token instead, then stop.\n",
    "        - Return: (new sequence, continue_flag)\n",
    "        \"\"\"\n",
    "        g = greedy_next(cur)\n",
    "        if cfg.DEBUG:\n",
    "            print(f\"    compare draft={token} ({tok.decode([token])!r}) \"\n",
    "                  f\"vs greedy={g} ({tok.decode([g])!r})\")\n",
    "        if g == token:\n",
    "            return append_token(cur, token), True   # match → keep going\n",
    "        else:\n",
    "            return append_token(cur, g), False      # mismatch → stop\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def prefix_accept_once(ids: torch.Tensor, branch: List[int]) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Compare an entire drafted branch (list of tokens) against greedy decoding.\n",
    "        - Iterate token by token:\n",
    "            - Accept while tokens match greedy predictions.\n",
    "            - On the first mismatch: insert greedy token and stop.\n",
    "        - Count how many tokens were accepted from the branch.\n",
    "        - Return: (new sequence including accepted tokens, number_accepted)\n",
    "        \"\"\"\n",
    "        cur = ids.clone()\n",
    "        accepted = 0\n",
    "        for t in branch:\n",
    "            cur, ok = accept_one(cur, t)\n",
    "            if ok:\n",
    "                accepted += 1\n",
    "            else:\n",
    "                break\n",
    "        return cur, accepted\n",
    "\n",
    "    print(\"[STEP5] prefix-accept ready: accept_one/prefix_accept_once\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61196b25-f992-4e4b-8f95-9cc32a95ab6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP6] loops ready: medusa_loop_step/medusa_tiny/run_greedy\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 6 — Loop execution (ultra-split)\n",
    "# =============================\n",
    "if RUN_STEP_6:\n",
    "    @torch.inference_mode()\n",
    "    def medusa_loop_step(ids: torch.Tensor, span: int, temperature: float,\n",
    "                         top_p: float, accepted_total: int, loop_idx: int):\n",
    "        \"\"\"\n",
    "        One Medusa loop step:\n",
    "        1) Draft a short branch with the drafter (propose_branch).\n",
    "        2) Apply prefix-accept once to compare with greedy and extend the sequence.\n",
    "        Returns:\n",
    "            - ids: the updated sequence after accepting tokens (and possibly 1 greedy token on mismatch)\n",
    "            - acc: how many draft tokens were accepted in this step\n",
    "        \"\"\"\n",
    "        if cfg.DEBUG:\n",
    "            print(f\"[loop {loop_idx}] cur_len={ids.shape[1]}\")\n",
    "        branch = propose_branch(ids, span, temperature, top_p, accepted_total)\n",
    "        ids, acc = prefix_accept_once(ids, branch)\n",
    "        return ids, acc\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def medusa_tiny(prompt: str, max_new_tokens: int, span: int,\n",
    "                    temperature: float, top_p: float) -> str:\n",
    "        \"\"\"\n",
    "        End-to-end Medusa-tiny decoding:\n",
    "        - Start from the encoded prompt.\n",
    "        - Repeat medusa_loop_step until we generate max_new_tokens.\n",
    "        - Keep track of how many draft tokens were accepted total (optional metric).\n",
    "        - Decode the final sequence to text.\n",
    "        \"\"\"\n",
    "        ids = encode(prompt)\n",
    "        start = ids.shape[1]\n",
    "        accepted_total = 0\n",
    "        loop_idx = 0\n",
    "        while ids.shape[1] - start < max_new_tokens:\n",
    "            ids, acc = medusa_loop_step(ids, span, temperature, top_p,\n",
    "                                        accepted_total, loop_idx)\n",
    "            accepted_total += acc\n",
    "            loop_idx += 1\n",
    "        return decode(ids)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def run_greedy(prompt: str, max_new_tokens: int) -> str:\n",
    "        \"\"\"\n",
    "        Plain greedy decoding baseline:\n",
    "        - Iteratively pick argmax (greedy_next) and append it to the sequence.\n",
    "        - Stop if EOS appears or we reach max_new_tokens.\n",
    "        \"\"\"\n",
    "        ids = encode(prompt)\n",
    "        start = ids.shape[1]\n",
    "        while ids.shape[1] - start < max_new_tokens:\n",
    "            nxt = greedy_next(ids)\n",
    "            if EOS_ID is not None and nxt == EOS_ID:\n",
    "                break\n",
    "            ids = append_token(ids, nxt)\n",
    "        return decode(ids)\n",
    "\n",
    "    print(\"[STEP6] loops ready: medusa_loop_step/medusa_tiny/run_greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd42e98f-b474-4faa-a90f-9088aa9bb884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Greedy ===\n",
      "In a distant future,   the world is a place where the world is a place where the world is a place where the world is a place where the world is a place \n",
      "\n",
      "=== Medusa-tiny ===\n",
      "In a distant future,   the world is a place where the world is a place where the world is a place where the world is a place where the world is a place where the\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 7 — 데모 (Greedy vs Medusa 결과)\n",
    "# =============================\n",
    "if RUN_STEP_7:\n",
    "    prompt = \"In a distant future, \"\n",
    "    print(\"\\n=== Greedy ===\")\n",
    "    print(run_greedy(prompt, cfg.MAX_NEW_TOKENS), \"\\n\")\n",
    "\n",
    "    print(\"=== Medusa-tiny ===\")\n",
    "    print(medusa_tiny(prompt, cfg.MAX_NEW_TOKENS,\n",
    "                      cfg.SPAN, cfg.TEMPERATURE, cfg.TOP_P))\n",
    "\n",
    "    # 디버그 보고 싶으면 아래 두 줄로 토글 후 재실행\n",
    "    # cfg.DEBUG = True\n",
    "    # print(medusa_tiny(prompt, cfg.MAX_NEW_TOKENS,\n",
    "    #                   cfg.SPAN, cfg.TEMPERATURE, cfg.TOP_P))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a1d15-6d02-4975-b8ad-6f9c683b3727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2175f90-7a45-43a7-ba5c-50038086be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
