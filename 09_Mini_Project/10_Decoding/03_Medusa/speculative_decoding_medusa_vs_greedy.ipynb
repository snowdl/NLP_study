{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "e46353b7-7078-415f-9040-3f20f26b8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medusa-lite flow : drafter → verifier → multi-branch prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab43540-a679-4dfc-bea9-767a75607adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b722306-3387-4100-a24c-ac353774f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Device selection\n",
    "\n",
    "def pick_device():\n",
    "    # Check if Apple Silicon (MPS) is available\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"     # Use MPS on Mac\n",
    "    # Otherwise, check if CUDA GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"    # Use CUDA if available\n",
    "    # Fallback to CPU if no GPU/MPS is found\n",
    "    return \"cpu\"         \n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"✅ DEVICE =\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4d20be-51af-47dc-b8ed-c2d34cf39cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert DEVICE in {\"cpu\", \"cuda\", \"mps\"}\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd100859-26a7-428d-b08c-8f99c54e7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f0a711-b21e-4678-93c3-14d51d3c550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch, random\n",
    "\n",
    "# Set seeds for reproducibility \n",
    "# (not critical if sampling is not used, but still good practice)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "7d7b0024-611a-4f72-ac17-ba8b68b9b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d19e12-3cde-40b6-9910-5a95ecaa3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # IDs of the models used\n",
    "    DRAFTER_ID: str = \"distilgpt2\"      # Small, fast draft model\n",
    "    VERIFIER_ID: str = \"gpt2-medium\"    # Larger, more accurate verifier model\n",
    "\n",
    "    # Generation length\n",
    "    MAX_NEW_TOKENS: int = 30            # Maximum number of tokens to generate\n",
    "\n",
    "    # Sampling parameters\n",
    "    TEMPERATURE: float = 0.8            # Controls randomness (lower → more deterministic)\n",
    "    TOP_P: float = 0.9                  # Nucleus sampling (probability mass cutoff)\n",
    "\n",
    "    # Repetition control\n",
    "    REPETITION_PENALTY: float = 1.3     # Penalize repeating tokens\n",
    "    NO_REPEAT_NGRAM: int = 5            # Prevent repeating n-grams of size 5\n",
    "\n",
    "    # Speculative decoding settings\n",
    "    TOPK_BRANCH: int = 4                # How many draft tokens to branch for verification\n",
    "    DRAFT_SPAN: int = 3                 # Number of tokens the drafter proposes at once\n",
    "\n",
    "    # Runtime settings\n",
    "    DEVICE: str = DEVICE                # Device to run on (mps / cuda / cpu)\n",
    "    DEBUG: bool = False                 # Debug mode toggle\n",
    "\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c043eb52-3954-4c4c-9f2a-7e5a53e8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Draft and Verifier Models (Tokenizer → Model) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16d4bd6-e6eb-4989-b3fe-f3f6c77a1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ models ready: distilgpt2 / gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizers for drafter and verifier models\n",
    "drafter_tok  = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "\n",
    "# 🔧 Fix for GPT-2 family: often eos_token / pad_token are missing\n",
    "if verifier_tok.eos_token_id is None:\n",
    "    verifier_tok.eos_token = \"\"   # Add EOS token if missing\n",
    "if verifier_tok.pad_token_id is None:\n",
    "    verifier_tok.pad_token = verifier_tok.eos_token  # Use EOS as padding if missing\n",
    "\n",
    "# Save EOS token ID for reference\n",
    "EOS_ID = verifier_tok.eos_token_id\n",
    "\n",
    "# Load models and move them to the chosen device\n",
    "drafter  = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE).eval()\n",
    "verifier = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE).eval()\n",
    "\n",
    "# Ensure caching is enabled (default is True, but set explicitly)\n",
    "drafter.config.use_cache  = True\n",
    "verifier.config.use_cache = True\n",
    "\n",
    "print(\"✅ models ready:\", cfg.DRAFTER_ID, \"/\", cfg.VERIFIER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8be34b4-8927-4182-bbe4-99e2275c1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4) Prompt & Context Preparation ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeb72db2-cd74-48aa-a0a3-70d618b1c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ok? True | shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# Define the initial prompt text\n",
    "prompt = \"In a distant future, a small crew of explorers discovers \"\n",
    "\n",
    "# Encode the prompt with the drafter tokenizer\n",
    "# and move the tensor to the selected DEVICE (mps / cuda / cpu)\n",
    "ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "\n",
    "# Extract only the input_ids (token IDs for the prompt)\n",
    "input_ids = ctx[\"input_ids\"]\n",
    "\n",
    "# Debug print: confirm context preparation and tensor shape\n",
    "print(\"context ok?\", ctx is not None, \"| shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8516d202-1746-4430-9347-17506532cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function: Draft k candidate tokens ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a23bc6a6-8263-4f83-8858-1c9d93865d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()  # Disable gradient calculation for efficiency\n",
    "def drafter_sample_first_tokens_basic(model, ids, k: int, temperature: float = 0.8):\n",
    "    # Forward pass → get logits for the last token position\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    \n",
    "    # Apply temperature scaling + softmax to convert logits into probabilities\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "    \n",
    "    # Ensure k does not exceed vocabulary size\n",
    "    k = min(k, probs.numel())\n",
    "    \n",
    "    # Sample k distinct token IDs (multinomial sampling without replacement)\n",
    "    picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "    \n",
    "    # Return as a Python list of integers\n",
    "    return [int(i) for i in picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca643d6-c40b-42a0-abc4-4d98157578d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token str (repr): '\\xa0'\n",
      "gpt2 piece: Âł\n",
      "is space? True\n"
     ]
    }
   ],
   "source": [
    "# === Debug: Inspect a single token ID ===\n",
    "tid = 1849\n",
    "\n",
    "# Decode the token ID back into a string (repr shows invisible characters)\n",
    "print(\"token str (repr):\", repr(drafter_tok.decode([tid])))\n",
    "\n",
    "# Show the raw GPT-2 subword token (BPE piece)\n",
    "print(\"gpt2 piece:\", drafter_tok.convert_ids_to_tokens([tid])[0])\n",
    "\n",
    "# Check if the decoded token is only whitespace\n",
    "print(\"is space?\", drafter_tok.decode([tid]).isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "2392dfbb-de90-4c85-93cc-324c928f2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#멀티-브랜치 Draft 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "052dc313-d5f5-44be-9148-b5472f7684fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Optional\n",
    "\n",
    "# =========================\n",
    "# Small, focused utilities\n",
    "# =========================\n",
    "\n",
    "@torch.inference_mode()\n",
    "def last_token_logits(model, ids: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run a forward pass and return logits for the last time step.\n",
    "    ids: [B, T] LongTensor on the same device as the model.\n",
    "    returns: [V] 1D logits for the last position (batch assumed 1).\n",
    "    \"\"\"\n",
    "    out = model(ids)\n",
    "    # shape: [B, T, V] → take last step, squeeze batch\n",
    "    return out.logits[:, -1, :][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b187f5a-7e69-4d11-a14c-58eb2f6c5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply temperature scaling and softmax.\n",
    "    Clamps temperature to a minimum to avoid division by zero.\n",
    "    returns: probability vector over vocabulary [V].\n",
    "    \"\"\"\n",
    "    t = max(float(temperature), 1e-6)\n",
    "    return torch.softmax(logits / t, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cd301e3-b7dd-4a1a-9457-957584c8bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_top_p_filter(probs: torch.Tensor, top_p: Optional[float]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return the indices of tokens inside the nucleus (top-p) set.\n",
    "    - Sort by prob desc, take smallest prefix whose cumulative prob ≤ top_p.\n",
    "    - Always keep at least the top-1 token.\n",
    "    If top_p is None, returns all indices (torch.arange(V)).\n",
    "    \"\"\"\n",
    "    V = probs.numel()\n",
    "    if top_p is None:\n",
    "        return torch.arange(V, device=probs.device)\n",
    "\n",
    "    # Bound top_p into (0, 1]; treat <=0 as keep only top-1, >1 as keep all.\n",
    "    if top_p <= 0:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        return sorted_ix[:1]\n",
    "    if top_p >= 1:\n",
    "        return torch.arange(V, device=probs.device)\n",
    "\n",
    "    sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "    cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "    keep_mask = cumsum <= top_p\n",
    "    # Ensure at least one token remains\n",
    "    keep_mask[0] = True\n",
    "    return sorted_ix[keep_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e530cc0-28f1-4ec0-9016-1031e05926c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample_without_replacement(probs: torch.Tensor, k: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Multinomial sampling WITHOUT replacement from probs.\n",
    "    Caps k by available items and returns Python ints.\n",
    "    \"\"\"\n",
    "    k = max(0, min(int(k), probs.numel()))\n",
    "    if k == 0:\n",
    "        return []\n",
    "    picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "    return [int(i) for i in picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80f1d851-ce61-4d61-a69e-60a483c28f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_token(ids: torch.Tensor, tok_id: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Append a single token id to a [1, T] LongTensor on same device/dtype.\n",
    "    \"\"\"\n",
    "    tok = torch.tensor([[tok_id]], dtype=ids.dtype, device=ids.device)\n",
    "    return torch.cat([ids, tok], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f32a565-d369-4e15-af92-bf8d39b0d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Drafting: first-token + greedy rollout\n",
    "# ======================================\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens_basic(model, ids: torch.Tensor, k: int,\n",
    "                                      temperature: float = 0.8) -> List[int]:\n",
    "    \"\"\"\n",
    "    Simple temperature sampling over the full vocab (no top-p).\n",
    "    \"\"\"\n",
    "    logits = last_token_logits(model, ids)\n",
    "    probs = softmax_with_temperature(logits, temperature)\n",
    "    return sample_without_replacement(probs, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc396657-2aff-4597-8ef5-972d76123d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids: torch.Tensor, k: int,\n",
    "                                temperature: float = 0.8,\n",
    "                                top_p: Optional[float] = 0.9) -> List[int]:\n",
    "    \"\"\"\n",
    "    Nucleus (top-p) sampling for the FIRST next-token proposals.\n",
    "    \"\"\"\n",
    "    logits = last_token_logits(model, ids)\n",
    "    probs = softmax_with_temperature(logits, temperature)\n",
    "    pool_ix = safe_top_p_filter(probs, top_p)\n",
    "    pool_probs = probs[pool_ix]\n",
    "    pool_probs = pool_probs / pool_probs.sum()  # renormalize\n",
    "    k = min(k, pool_ix.numel())\n",
    "    if k == 0:\n",
    "        return []\n",
    "    picks_local = torch.multinomial(pool_probs, num_samples=k, replacement=False)\n",
    "    return [int(pool_ix[i]) for i in picks_local]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "362227c1-99bc-4f8b-8974-6d503a0da86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_rollout_greedy(model, ids: torch.Tensor,\n",
    "                           first_tok: int, span: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Greedy rollout for 'span' tokens starting with 'first_tok'.\n",
    "    The first token is fixed; subsequent tokens use argmax.\n",
    "    \"\"\"\n",
    "    span = max(1, int(span))\n",
    "    cur = append_token(ids, first_tok)\n",
    "    seq = [first_tok]\n",
    "\n",
    "    for _ in range(span - 1):\n",
    "        logits = last_token_logits(model, cur)\n",
    "        nxt = int(torch.argmax(logits).item())\n",
    "        seq.append(nxt)\n",
    "        cur = append_token(cur, nxt)\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ede50e6-aa32-4705-a354-c7fad89effbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_propose(ids: torch.Tensor, k: int, span: int,\n",
    "                    temperature: float = 0.8, top_p: Optional[float] = 0.9) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Propose K branches:\n",
    "      1) sample K first tokens (nucleus + temperature)\n",
    "      2) greedy rollout for remaining (span-1) steps per branch\n",
    "    Uses the global 'drafter' model and current cfg.* settings.\n",
    "    \"\"\"\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    return [drafter_rollout_greedy(drafter, ids, f, span) for f in firsts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "93eaab97-53d5-4d35-9772-a90b97043a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verifier: predict one token (greedy) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a602aa44-d7b8-4cdf-a48d-9c8a37fd274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def verifier_next_token(ids) -> int:\n",
    "    \"\"\"\n",
    "    Use the verifier model to predict the next token ID greedily.\n",
    "    - Runs a forward pass on the current ids\n",
    "    - Takes the last-step logits\n",
    "    - Returns the argmax token ID as int\n",
    "    \"\"\"\n",
    "    logits = verifier(ids).logits[:, -1, :]\n",
    "    return int(torch.argmax(logits, dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df8e4703-5107-4707-80f6-2dca543b9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pretty-print token information ===\n",
    "def pretty_token(tokenizer, tid: int):\n",
    "    \"\"\"\n",
    "    Convert a token ID into multiple human-readable formats for debugging.\n",
    "    \n",
    "    Returns a dict with:\n",
    "      - \"id\": the token ID (int)\n",
    "      - \"decode_repr\": decoded string (repr to reveal hidden chars, e.g., '\\xa0')\n",
    "      - \"token_repr\": raw BPE token string (repr form)\n",
    "      - \"token_fixed\": attempt to fix mojibake via latin1 → utf-8 roundtrip\n",
    "      - \"codepoints\": list of Unicode codepoints in hex\n",
    "      - \"bytes\": list of raw UTF-8 bytes\n",
    "    \"\"\"\n",
    "    # Decode the token ID into text (keep special tokens and spaces)\n",
    "    s_decode = tokenizer.decode([tid],\n",
    "                                skip_special_tokens=False,\n",
    "                                clean_up_tokenization_spaces=False)\n",
    "\n",
    "    # Get the raw BPE subword token string\n",
    "    s_token = tokenizer.convert_ids_to_tokens([tid])[0]\n",
    "\n",
    "    # Try to re-encode/decode to fix potential mojibake (encoding artifacts)\n",
    "    try:\n",
    "        s_fixed = s_token.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        s_fixed = s_token\n",
    "\n",
    "    return {\n",
    "        \"id\": tid,\n",
    "        \"decode_repr\": repr(s_decode),   # decoded string, repr shows hidden chars\n",
    "        \"token_repr\": repr(s_token),     # raw token string as stored by tokenizer\n",
    "        \"token_fixed\": repr(s_fixed),    # mojibake-fixed token string\n",
    "        \"codepoints\": [hex(ord(c)) for c in s_decode],  # Unicode codepoints\n",
    "        \"bytes\": list(s_decode.encode(\"utf-8\")),        # raw UTF-8 bytes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6abed687-dd54-4c6f-b1dc-895d46f33f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def next_human_token(ids, tokenizer, tries=10):\n",
    "    \"\"\"\n",
    "    Predict tokens with the verifier until a *human-visible* token appears.\n",
    "    \n",
    "    A token is considered \"human-visible\" if:\n",
    "      - It contains at least one printable character\n",
    "      - That character is not just whitespace\n",
    "    \n",
    "    Args:\n",
    "        ids: current input sequence (tensor [1, T])\n",
    "        tokenizer: the tokenizer used for decoding\n",
    "        tries: maximum number of attempts before giving up\n",
    "    \n",
    "    Returns:\n",
    "        (tid, s) → the token ID and its decoded string\n",
    "    \"\"\"\n",
    "    cur = ids.clone()\n",
    "    for _ in range(tries):\n",
    "        # Predict next token (greedy)\n",
    "        tid = verifier_next_token(cur)\n",
    "\n",
    "        # Decode into a string without skipping special tokens\n",
    "        s = tokenizer.decode(\n",
    "            [tid],\n",
    "            skip_special_tokens=False,\n",
    "            clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        # If the decoded string contains a visible (printable, non-space) character, return\n",
    "        if any(ch.isprintable() and not ch.isspace() for ch in s):\n",
    "            return tid, s\n",
    "\n",
    "        # Otherwise, append token and continue searching\n",
    "        cur = torch.cat([cur, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "\n",
    "    # If no human-visible token found after max tries, return the last one\n",
    "    return tid, s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f30ef52a-82c6-4466-90e6-a8ad6d4c1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token info: {'id': 488, 'decode_repr': \"'ich'\", 'token_repr': \"'ich'\", 'token_fixed': \"'ich'\", 'codepoints': ['0x69', '0x63', '0x68'], 'bytes': [105, 99, 104]}\n",
      "Next human-visible token: 488 'ich'\n"
     ]
    }
   ],
   "source": [
    "# === Example usage ===\n",
    "vid = verifier_next_token(input_ids)\n",
    "info = pretty_token(verifier_tok, vid)\n",
    "\n",
    "print(\"Predicted token info:\", info)\n",
    "\n",
    "t2, s2 = next_human_token(input_ids, verifier_tok)\n",
    "print(\"Next human-visible token:\", t2, repr(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "42acd454-c409-4673-8d3b-c63d24b3c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix-Accept (mismatch까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02cdbb53-4055-490f-80b6-b70f3d967954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch(context_ids, branch_tokens:List[int]) -> Tuple[torch.Tensor, List[int], bool]:\n",
    "    ids = context_ids.clone()\n",
    "    accepted = []\n",
    "    mismatched = False\n",
    "    for tid in branch_tokens:\n",
    "        pred = verifier_next_token(ids)\n",
    "        if pred == tid:\n",
    "            ids = torch.cat([ids, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "            accepted.append(tid)\n",
    "        else:\n",
    "            ids = torch.cat([ids, torch.tensor([[pred]], device=ids.device)], dim=1)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return ids, accepted, mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34fd27c9-b087-4f7f-b4a6-fe7def8c1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted len: 0 | mismatched? True\n",
      "new length: 13 | tokens added: 1\n"
     ]
    }
   ],
   "source": [
    "# Generate proposed branches from drafter\n",
    "branches = drafter_propose(\n",
    "    input_ids,\n",
    "    k=cfg.TOPK_BRANCH,\n",
    "    span=cfg.DRAFT_SPAN,\n",
    "    temperature=cfg.TEMPERATURE,\n",
    "    top_p=cfg.TOP_P\n",
    ")\n",
    "\n",
    "# Test prefix-accept on the first branch\n",
    "new_ids, accepted, mism = accept_until_mismatch(input_ids, branches[0])\n",
    "\n",
    "print(\"accepted len:\", len(accepted), \"| mismatched?\", mism)\n",
    "print(\"new length:\", new_ids.shape[1],\n",
    "      \"| tokens added:\", new_ids.shape[1] - input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b938a293-6969-4edb-8784-0e1b34b473a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Branch Scoring Utility ===\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "510d8429-152b-42b2-ab3d-46c2994a0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "ee1be8b8-5047-4ee2-adde-26409e63722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch 점수 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8cea79e-4e6c-474b-895c-e745b078be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_branch(accepted, mismatched):\n",
    "    \"\"\"\n",
    "    Score a drafter branch based on prefix-accept results.\n",
    "\n",
    "    Args:\n",
    "        accepted: list of tokens accepted before mismatch\n",
    "        mismatched: bool, True if a mismatch occurred\n",
    "\n",
    "    Returns:\n",
    "        int → simple score = (#accepted tokens) - (1 if mismatch happened else 0)\n",
    "\n",
    "    Intuition:\n",
    "        - Longer accepted prefix → higher score\n",
    "        - If mismatch occurred → apply small penalty (-1)\n",
    "    \"\"\"\n",
    "    return len(accepted) - (1 if mismatched else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7227c78d-ee2e-454c-b9a4-baa43a3d7aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Quick checks\n",
    "print(score_branch([1,2,3], False))  # 3 (3 accepted, no penalty)\n",
    "print(score_branch([1,2], True))     # 1 (2 accepted, -1 penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "f6414314-abf1-484f-9a9a-d782b3773b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Encode Prompt into Token IDs ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "757a2021-07af-452b-b469-d550e46e48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def encode_prompt(prompt: str):\n",
    "    \"\"\"\n",
    "    Tokenize a text prompt using the drafter tokenizer and\n",
    "    move it to the configured device (mps/cuda/cpu).\n",
    "\n",
    "    Args:\n",
    "        prompt: input string\n",
    "\n",
    "    Returns:\n",
    "        input_ids: tensor of shape [1, T] with token IDs\n",
    "    \"\"\"\n",
    "    ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    return ctx[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "512fc442-7b76-4fb3-9ffd-c618d859642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids.shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "ids = encode_prompt(\"In a distant future, \")\n",
    "print(\"ids.shape:\", ids.shape)   # e.g. torch.Size([1, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "9c5a09ad-485a-4887-9fd8-c623132712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한 스텝 수행(multi-branch→검증→최고 점수 채택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9036549-1494-49c6-b2cb-0c55ea2ce21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === One Medusa Step ===\n",
    "@torch.inference_mode()\n",
    "def medusa_step(ids, topk_branch: int, draft_span: int, temperature: float):\n",
    "    \"\"\"\n",
    "    Perform one Medusa decoding step:\n",
    "      1) Drafter proposes multiple candidate branches\n",
    "      2) Each branch is verified with prefix-accept\n",
    "      3) Branches are scored (longer accepted prefix is better; mismatch penalized)\n",
    "      4) Best-scoring branch is chosen and returned\n",
    "\n",
    "    Args:\n",
    "        ids: tensor [1, T] → current context sequence\n",
    "        topk_branch: number of branches to propose\n",
    "        draft_span: number of tokens per branch\n",
    "        temperature: sampling temperature (≥0.9 enforced for diversity)\n",
    "\n",
    "    Returns:\n",
    "        best_ids: updated context tensor after accepting one branch\n",
    "    \"\"\"\n",
    "    # Drafter proposes branches with stronger sampling (top-p = 0.95)\n",
    "    branches = drafter_propose(\n",
    "        ids,\n",
    "        topk_branch,\n",
    "        draft_span,\n",
    "        temperature=max(0.9, float(temperature)),  # force ≥ 0.9 for diversity\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    # Select the branch with the highest score\n",
    "    best_score = -10**9\n",
    "    best_ids = None\n",
    "    for br in branches:\n",
    "        new_ids, accepted, mism = accept_until_mismatch(ids, br)\n",
    "        s = score_branch(accepted, mism)\n",
    "        if s > best_score:\n",
    "            best_score, best_ids = s, new_ids\n",
    "\n",
    "    return best_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "689ee0dd-6059-4bcd-9556-fdf4a4b5d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 6 → after: 8\n"
     ]
    }
   ],
   "source": [
    "ids2 = medusa_step(ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(\"before:\", ids.shape[1], \"→ after:\", ids2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "21a7e22b-51dd-4218-82f7-20555827dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21accee7-8bde-4019-a992-5255ca3a5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Full Orchestrator: Medusa Generate ===\n",
    "@torch.inference_mode()\n",
    "def medusa_generate(prompt: str,\n",
    "                    max_new_tokens: int = None,\n",
    "                    topk_branch: int = None,\n",
    "                    draft_span: int = None,\n",
    "                    temperature: float = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Medusa decoding (drafter + verifier + prefix-accept).\n",
    "\n",
    "    Args:\n",
    "        prompt: starting string\n",
    "        max_new_tokens: max number of tokens to add\n",
    "        topk_branch: number of branches to propose per step\n",
    "        draft_span: number of tokens per branch\n",
    "        temperature: sampling temperature for drafter\n",
    "\n",
    "    Returns:\n",
    "        Decoded string (str) including the prompt and generated text\n",
    "    \"\"\"\n",
    "    # Fill with defaults from cfg if not specified\n",
    "    if max_new_tokens is None: max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "    if topk_branch   is None: topk_branch   = cfg.TOPK_BRANCH\n",
    "    if draft_span    is None: draft_span    = cfg.DRAFT_SPAN\n",
    "    if temperature   is None: temperature   = cfg.TEMPERATURE\n",
    "\n",
    "    # Encode the prompt into token IDs\n",
    "    ids = encode_prompt(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "\n",
    "    # Number of Medusa steps (ceil to cover full length)\n",
    "    steps = math.ceil(max_new_tokens / draft_span)\n",
    "\n",
    "    # Iteratively expand with Medusa steps\n",
    "    for _ in range(steps):\n",
    "        ids = medusa_step(ids, topk_branch, draft_span, temperature)\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "    # Decode back into human-readable text\n",
    "    return drafter_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78601636-7a26-4a4b-a2be-0f1cbd210e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants\n"
     ]
    }
   ],
   "source": [
    "# ✔️ Example run\n",
    "out = medusa_generate(\"In a distant future, \", 40)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "9a00ab2b-8e08-4ae9-8277-13002c95529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13) Greedy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3ed7155-d9e8-47b4-a337-4e64f12a4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Baseline: Greedy Decoding with Verifier ===\n",
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using plain greedy decoding with the verifier model.\n",
    "    Acts as a baseline for comparison against Medusa decoding.\n",
    "\n",
    "    Args:\n",
    "        prompt: starting string\n",
    "        max_new_tokens: number of tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Decoded string (prompt + generated text)\n",
    "    \"\"\"\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    # Encode the prompt with verifier tokenizer\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "\n",
    "    # Iteratively add one token at a time (greedy argmax)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = verifier(ids).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])  # pick most likely token\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "    # Decode back to human-readable text\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "200e4cc7-141e-4a4f-b3be-7925962cff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants to control the world's resources so that he can rule the world. \n"
     ]
    }
   ],
   "source": [
    "txt = greedy_generate(\"In a distant future, \", 40)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "7b6f4292-a915-4184-8f0b-b279ccffaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) A/B Speed & Text Comparison ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f924eb0c-f5c1-4ac3-a144-e3904c3ffc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ greedy: 7.495 s\n",
      "⏱ medusa: 9.798 s\n",
      "\n",
      "--- greedy ---\n",
      " In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants to control the world's resources so that he can rule the world.  He wants to control the world's resources so that he can rule the world.  He wants to control the world's resources so that he can rule the world.  He wants to\n",
      "\n",
      "--- medusa ---\n",
      " In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants to control the world's resources so that he can rule the world.  He wants to control the world's resources so that he can rule the world.  He\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def time_it(fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a function and measure its execution time.\n",
    "    Returns:\n",
    "        (output, elapsed_seconds)\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    return out, time.perf_counter() - t0\n",
    "\n",
    "\n",
    "# Run both generators on the same prompt\n",
    "g_txt, g_t = time_it(greedy_generate, \"In a distant future, \", 80)\n",
    "m_txt, m_t = time_it(medusa_generate, \"In a distant future, \", 80)\n",
    "\n",
    "# Compare runtime\n",
    "print(\"⏱ greedy:\", round(g_t, 3), \"s\")\n",
    "print(\"⏱ medusa:\", round(m_t, 3), \"s\")\n",
    "\n",
    "# Show a preview of outputs (first 400 chars)\n",
    "print(\"\\n--- greedy ---\\n\", g_txt[:400])\n",
    "print(\"\\n--- medusa ---\\n\", m_txt[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66a17-0016-4606-a4dc-03c8dae9450f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc0d46-c787-4048-b06c-a0ee7057924b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
