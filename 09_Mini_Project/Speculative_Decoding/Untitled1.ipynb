{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4040d8bb-b8f7-4ddd-afd1-413196b62725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 디바이스 자동 선택 (M1/M2 → mps, GPU → cuda, 없으면 cpu)\n",
    "DEVICE = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"✅ device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d53727e-bf50-4486-94ef-1ab835eb3d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ models ready: distilgpt2 / gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "# drafter = 초안기 (빠르고 단순)\n",
    "drafter_id = \"distilgpt2\"\n",
    "drafter_tok = AutoTokenizer.from_pretrained(drafter_id)\n",
    "drafter = AutoModelForCausalLM.from_pretrained(drafter_id).to(DEVICE).eval()\n",
    "\n",
    "# verifier = 검증기 (품질↑)\n",
    "verifier_id = \"gpt2-medium\"\n",
    "verifier_tok = AutoTokenizer.from_pretrained(verifier_id)\n",
    "verifier = AutoModelForCausalLM.from_pretrained(verifier_id).to(DEVICE).eval()\n",
    "\n",
    "# eos/pad 보정 (gpt2 계열은 기본이 없음)\n",
    "if verifier_tok.eos_token_id is None:\n",
    "    verifier_tok.eos_token = \"\"\n",
    "if verifier_tok.pad_token_id is None:\n",
    "    verifier_tok.pad_token = verifier_tok.eos_token\n",
    "EOS_ID = verifier_tok.eos_token_id\n",
    "\n",
    "print(\"✅ models ready:\", drafter_id, \"/\", verifier_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b2685f-9a3d-4576-8f06-f904e1edcc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def draft_one_token(model, ids: torch.Tensor, temperature: float = 1.0) -> int:\n",
    "    \"\"\"\n",
    "    drafter에서 다음 토큰 1개 샘플링.\n",
    "    ids: shape [1, T]\n",
    "    \"\"\"\n",
    "    logits = model(ids).logits[:, -1, :]        # 마지막 위치 로짓\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)  # 확률 분포\n",
    "    next_id = torch.multinomial(probs[0], num_samples=1)             # 샘플링\n",
    "    return int(next_id.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de95195d-7cbc-4431-90ea-a8ea4943d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링된 토큰 ID: 262\n",
      "토큰 문자열:  the\n"
     ]
    }
   ],
   "source": [
    "ids = drafter_tok(\"In the distant future,\", return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "sample_id = draft_one_token(drafter, ids, temperature=0.8)\n",
    "print(\"샘플링된 토큰 ID:\", sample_id)\n",
    "print(\"토큰 문자열:\", drafter_tok.decode([sample_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d41158-f415-4b38-b276-1a113d84719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens_basic(model, ids, k: int, temperature: float = 0.8):\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "    k = min(k, probs.numel())\n",
    "    picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "    return [int(i) for i in picks]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout_basic(ids, first_tok: int, span: int):\n",
    "    cur = torch.cat([ids, torch.tensor([[first_tok]], device=ids.device)], dim=1)\n",
    "    seq = [first_tok]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        seq.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return seq\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose_basic(ids, k: int, span: int, temperature: float = 0.8):\n",
    "    firsts = drafter_sample_first_tokens_basic(drafter, ids, k, temperature)\n",
    "    return [drafter_rollout_basic(ids, t, span) for t in firsts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3781735a-47b5-43fe-8c12-e8104002391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 branches; 예시: [356, 389, 287]\n"
     ]
    }
   ],
   "source": [
    "branches = drafter_propose_basic(ids, k=3, span=3, temperature=0.8)\n",
    "print(len(branches), \"branches; 예시:\", branches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b01ee9-93bd-4adf-9436-ef33efd12a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b943485b-ba20-4701-af9d-05e8ce535a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_next_basic(ids: torch.Tensor, repetition_penalty: float = 1.3, no_repeat_ngram: int = 5) -> int:\n",
    "    logits = verifier(ids).logits[:, -1, :].clone()\n",
    "    V = logits.size(-1)\n",
    "\n",
    "    # repetition penalty: 이미 나온 토큰 로짓 낮추기\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        seen = torch.bincount(ids[0].to(torch.int64), minlength=V).bool()\n",
    "        logits[:, seen] = logits[:, seen] / repetition_penalty\n",
    "\n",
    "    # no-repeat n-gram: 마지막 n-1 패턴의 n번째 토큰 차단\n",
    "    n = int(no_repeat_ngram or 0)\n",
    "    if n > 1 and ids.size(1) >= n - 1:\n",
    "        tail = ids[0].tolist()\n",
    "        prefix = tail[-(n-1):]\n",
    "        blocked = set()\n",
    "        for i in range(len(tail) - n + 1):\n",
    "            if tail[i:i+n-1] == prefix:\n",
    "                blocked.add(tail[i+n-1])\n",
    "        if blocked:\n",
    "            logits[:, list(blocked)] = -1e9\n",
    "\n",
    "    return int(torch.argmax(logits, dim=-1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91647e8-9b8f-4899-8543-c3f67b6c7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_lite_generate_basic(\n",
    "    prompt: str,\n",
    "    *,\n",
    "    max_new_tokens: int = 30,\n",
    "    k_branches: int = 4,\n",
    "    span: int = 3,\n",
    "    temperature: float = 0.8,\n",
    "    repetition_penalty: float = 1.3,\n",
    "    no_repeat_ngram: int = 5,\n",
    ") -> str:\n",
    "    # 시작 컨텍스트 (verifier 토크나이저 기준)\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "    committed = 0\n",
    "\n",
    "    while committed < max_new_tokens:\n",
    "        # 1) drafter가 K개 분기 제안 (각각 길이 span)\n",
    "        branches = drafter_propose_basic(ids, k=k_branches, span=span, temperature=temperature)\n",
    "        if not branches:  # (희박) 분기 못 만들면 verifier 1토큰만 커밋\n",
    "            vtok = verifier_next_basic(ids, repetition_penalty, no_repeat_ngram)\n",
    "            ids = torch.cat([ids, torch.tensor([[vtok]], device=ids.device)], dim=1)\n",
    "            committed += 1\n",
    "            break\n",
    "\n",
    "        # 2) 각 분기에 대해 prefix-accept 길이 측정\n",
    "        best_len, best_branch, best_mismatch = -1, None, None\n",
    "        for cand in branches:\n",
    "            cur = ids\n",
    "            pref = 0\n",
    "            mismatch = None\n",
    "            for t in cand:\n",
    "                v_next = verifier_next_basic(cur, repetition_penalty, no_repeat_ngram)\n",
    "                if v_next == EOS_ID:\n",
    "                    mismatch = EOS_ID; break\n",
    "                if v_next == t:\n",
    "                    pref += 1\n",
    "                    cur = torch.cat([cur, torch.tensor([[t]], device=cur.device)], dim=1)\n",
    "                else:\n",
    "                    mismatch = v_next; break\n",
    "            if pref > best_len:\n",
    "                best_len, best_branch, best_mismatch = pref, cand, mismatch\n",
    "            if pref == len(cand):  # 완전 일치면 즉시 채택\n",
    "                break\n",
    "\n",
    "        # 3) 커밋\n",
    "        if best_len <= 0:\n",
    "            vtok = verifier_next_basic(ids, repetition_penalty, no_repeat_ngram)\n",
    "            ids = torch.cat([ids, torch.tensor([[vtok]], device=ids.device)], dim=1)\n",
    "            committed += 1\n",
    "            if vtok == EOS_ID:\n",
    "                break\n",
    "        else:\n",
    "            commit_seq = (\n",
    "                best_branch if best_len == len(best_branch)\n",
    "                else best_branch[:best_len] + ([best_mismatch] if best_mismatch is not None else [])\n",
    "            )\n",
    "            ids = torch.cat([ids, torch.tensor([commit_seq], device=ids.device)], dim=1)\n",
    "            committed += len(commit_seq)\n",
    "            if commit_seq and commit_seq[-1] == EOS_ID:\n",
    "                break\n",
    "\n",
    "        # 4) 문장부호면 조기 종료(늘어짐 방지)\n",
    "        tail = verifier_tok.decode(ids[0][-40:], skip_special_tokens=True).strip()\n",
    "        if tail.endswith((\".\", \"!\", \"?\")):\n",
    "            break\n",
    "\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "893c1990-6380-4883-b201-42d19fe8e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Medusa-lite ===\n",
      "In the distant future,      is the   ?\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Medusa-lite ===\")\n",
    "print(medusa_lite_generate_basic(\n",
    "    \"In the distant future, \",\n",
    "    max_new_tokens=30,\n",
    "    k_branches=4,\n",
    "    span=3,\n",
    "    temperature=0.8,         # drafter 다양성\n",
    "    repetition_penalty=1.3,  # 반복 억제\n",
    "    no_repeat_ngram=5\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41487f9-a6a3-437d-8c80-e4814da8f292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
