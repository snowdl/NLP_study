{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f06f292b-a91a-4548-ad74-c4f6f08b7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1. Device & Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b38c9d83-0a53-4ef7-b3d3-57e0abdd223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch, random, os\n",
    "\n",
    "# Pick device (Apple Silicon ‚Üí mps, else cuda if available, else cpu)\n",
    "DEVICE = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"‚úÖ device:\", DEVICE)\n",
    "\n",
    "# Reproducibility for sampling (ÏÉòÌîåÎßÅ Ïû¨ÌòÑÏÑ±)\n",
    "SEED = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3c2b9fc8-8152-496e-82a8-962ca208983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8bffcc5a-f477-4734-85f8-0b3067f4962e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cfg(DRAFTER_ID='distilgpt2', VERIFIER_ID='gpt2', MAX_NEW_TOKENS=30, TEMPERATURE=0.8, TOP_P=0.9, REPETITION_PENALTY=1.2, NO_REPEAT_NGRAM=4, TOPK_BRANCH=3, DRAFT_SPAN=3, DEVICE='mps', DEBUG=False)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Models\n",
    "    DRAFTER_ID: str = \"distilgpt2\"  # small, fast\n",
    "    VERIFIER_ID: str = \"gpt2\"       # baseline verifier\n",
    "\n",
    "    # Decoding defaults (Sampling)\n",
    "    MAX_NEW_TOKENS: int = 30        # ÏßßÍ≥† Íµ≥Í≤å\n",
    "    TEMPERATURE: float = 0.8        # ÏûêÏó∞Ïä§Îü¨ÏõÄ‚Üë\n",
    "    TOP_P: float = 0.9              # ÏûêÏó∞Ïä§Îü¨ÏõÄ‚Üë\n",
    "    REPETITION_PENALTY: float = 1.2 # Î∞òÎ≥µ ÏñµÏ†ú Í∞ïÌôî\n",
    "    NO_REPEAT_NGRAM: int = 4        # Î∞òÎ≥µ ÏñµÏ†ú Í∞ïÌôî (simple check in medusa-lite)\n",
    "\n",
    "    # Medusa-lite branching\n",
    "    TOPK_BRANCH: int = 3            # K branches\n",
    "    DRAFT_SPAN: int = 3             # span length per branch\n",
    "\n",
    "    # Misc\n",
    "    DEVICE: str = DEVICE\n",
    "    DEBUG: bool = False             # TrueÎ©¥ Î∏åÎûúÏπò/ÌîÑÎ¶¨ÌîΩÏä§ Î°úÍ∑∏ Ï∂úÎ†•\n",
    "\n",
    "cfg = Cfg()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2449cfd0-5237-4af1-aec1-5c9d475ab7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models & Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f15d2243-69c9-4a54-9a22-21f7599e56b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ models ready: distilgpt2 / gpt2 | EOS: 50256 PAD: 50256\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizers\n",
    "drafter_tok  = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "\n",
    "# Ensure EOS/PAD (gpt2Îäî padÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ Â§ö)\n",
    "def ensure_eos_pad(tokenizer):\n",
    "    if tokenizer.eos_token_id is None:\n",
    "        tokenizer.eos_token = \"\"\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer.eos_token_id, tokenizer.pad_token_id\n",
    "\n",
    "EOS_ID, PAD_ID = ensure_eos_pad(verifier_tok)\n",
    "\n",
    "# Load models\n",
    "drafter  = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE).eval()\n",
    "verifier = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE).eval()\n",
    "\n",
    "print(\"‚úÖ models ready:\", cfg.DRAFTER_ID, \"/\", cfg.VERIFIER_ID, \"| EOS:\", EOS_ID, \"PAD:\", PAD_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8a2ee3ac-5e24-4455-a0d8-bbe3ac207c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Baseline: Greedy Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a9ef28bb-55a5-43e6-b6b4-090c79ce808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int | None = None) -> str:\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = verifier(ids).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        if nxt == EOS_ID:\n",
    "            break\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "        # simple early stop on punctuation (ÎäòÏñ¥Ïßê Î∞©ÏßÄ)\n",
    "        tail = verifier_tok.decode(ids[0][-12:], skip_special_tokens=True).strip()\n",
    "        if tail.endswith((\".\", \"!\", \"?\")):\n",
    "            break\n",
    "\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2ac5fe89-02f6-49aa-960b-55d27cf4924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline: Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d0eb1eaf-755d-4953-a564-41c09f7cf262",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample_generate(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int | None = None,\n",
    "    *,\n",
    "    temperature: float | None = None,\n",
    "    top_p: float | None = None,\n",
    "    repetition_penalty: float | None = None,\n",
    "    no_repeat_ngram_size: int | None = None,\n",
    ") -> str:\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    temperature = cfg.TEMPERATURE if temperature is None else temperature\n",
    "    top_p = cfg.TOP_P if top_p is None else top_p\n",
    "    repetition_penalty = cfg.REPETITION_PENALTY if repetition_penalty is None else repetition_penalty\n",
    "    no_repeat_ngram_size = cfg.NO_REPEAT_NGRAM if no_repeat_ngram_size is None else no_repeat_ngram_size\n",
    "\n",
    "    inputs = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    out_ids = verifier.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        eos_token_id=EOS_ID,\n",
    "        pad_token_id=PAD_ID,\n",
    "    )[0]\n",
    "\n",
    "    return verifier_tok.decode(out_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "58d5585e-70f8-403a-9b87-4cceaedf3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drafter/Verifier Helper Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5652a679-828d-4fba-af8e-1545b3e50c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def verifier_next_argmax(ids: torch.Tensor) -> int:\n",
    "    \"\"\"Get verifier's next-token argmax.\"\"\"\n",
    "    logits = verifier(ids).logits[:, -1, :]\n",
    "    return int(torch.argmax(logits, dim=-1)[0])\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_topk_first_tokens(ids: torch.Tensor, k: int) -> list[int]:\n",
    "    \"\"\"Get top-k next tokens from drafter.\"\"\"\n",
    "    logits = drafter(ids).logits[:, -1, :]\n",
    "    topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n",
    "    return topk\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout(ids: torch.Tensor, first_token: int, span: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Fix first_token, then greedy rollout to get total 'span' tokens from the drafter.\n",
    "    \"\"\"\n",
    "    cur = torch.cat([ids, torch.tensor([[first_token]], device=ids.device)], dim=1)\n",
    "    drafted = [first_token]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        drafted.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return drafted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e166f136-a2c0-4e61-a4e8-90f306fcf107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medusa-lite Core (drafter ‚Üí verifier ‚Üí multi-branch prefix-accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e552d2a3-3a9d-4f98-b8ed-6317c217a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_lite_generate(prompt: str, max_new_tokens: int | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Loop:\n",
    "      1) drafter: propose K branches (each of span M)\n",
    "      2) verifier: prefix-accept; on first mismatch, correct with verifier token\n",
    "      3) commit the best (longest accepted prefix; full if perfect match, else prefix+1 correction)\n",
    "      4) stop by EOS/punctuation/max tokens\n",
    "    \"\"\"\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "    committed = 0\n",
    "\n",
    "    while committed < max_new_tokens:\n",
    "        # 1) branches\n",
    "        topk = drafter_topk_first_tokens(ids, cfg.TOPK_BRANCH)\n",
    "        branches = [drafter_rollout(ids, t0, cfg.DRAFT_SPAN) for t0 in topk]\n",
    "\n",
    "        # 2) evaluate by verifier (prefix-accept)\n",
    "        best_prefix_len = -1\n",
    "        best_branch = None\n",
    "        best_mismatch_vtok = None\n",
    "\n",
    "        for b_idx, cand in enumerate(branches):\n",
    "            cur_ids = ids\n",
    "            prefix_len = 0\n",
    "            mismatch_vtok = None\n",
    "\n",
    "            for j, tok in enumerate(cand):\n",
    "                v_next = verifier_next_argmax(cur_ids)\n",
    "\n",
    "                if v_next == EOS_ID:\n",
    "                    mismatch_vtok = EOS_ID\n",
    "                    break\n",
    "\n",
    "                if v_next == tok:\n",
    "                    prefix_len += 1\n",
    "                    cur_ids = torch.cat([cur_ids, torch.tensor([[tok]], device=cur_ids.device)], dim=1)\n",
    "                else:\n",
    "                    mismatch_vtok = v_next\n",
    "                    break\n",
    "\n",
    "            if cfg.DEBUG:\n",
    "                print(f\"[branch {b_idx}] cand={cand[:6]}... prefix_len={prefix_len} mismatch={mismatch_vtok}\")\n",
    "\n",
    "            if prefix_len > best_prefix_len:\n",
    "                best_prefix_len = prefix_len\n",
    "                best_branch = cand\n",
    "                best_mismatch_vtok = mismatch_vtok\n",
    "\n",
    "            if prefix_len == len(cand):  # perfect match early-exit\n",
    "                break\n",
    "\n",
    "        # 3) commit\n",
    "        if best_prefix_len <= 0:\n",
    "            # nothing matched from the start ‚Üí commit verifier's token\n",
    "            vtok = verifier_next_argmax(ids)\n",
    "            if cfg.NO_REPEAT_NGRAM and _forms_repeated_ngram(ids, vtok, cfg.NO_REPEAT_NGRAM):\n",
    "                # if repetition would happen, just skip this token (rare case)\n",
    "                if cfg.DEBUG:\n",
    "                    print(\"[skip] repetition would form; skipping one step\")\n",
    "            else:\n",
    "                ids = torch.cat([ids, torch.tensor([[vtok]], device=ids.device)], dim=1)\n",
    "                committed += 1\n",
    "                if vtok == EOS_ID:\n",
    "                    break\n",
    "        else:\n",
    "            # perfect match ‚Üí commit all; partial match ‚Üí commit prefix + correction token\n",
    "            commit_seq = (\n",
    "                best_branch\n",
    "                if best_prefix_len == len(best_branch)\n",
    "                else best_branch[:best_prefix_len] + ([best_mismatch_vtok] if best_mismatch_vtok is not None else [])\n",
    "            )\n",
    "\n",
    "            # optional: n-gram repetition guard (simple)\n",
    "            filtered = []\n",
    "            for t in commit_seq:\n",
    "                if cfg.NO_REPEAT_NGRAM and _forms_repeated_ngram(ids, t, cfg.NO_REPEAT_NGRAM):\n",
    "                    if cfg.DEBUG:\n",
    "                        print(f\"[filter] dropping tok {t} due to n-gram repeat\")\n",
    "                    continue\n",
    "                filtered.append(t)\n",
    "            commit_seq = filtered or commit_seq[:1]  # ensure progress\n",
    "\n",
    "            ids = torch.cat([ids, torch.tensor([commit_seq], device=ids.device)], dim=1)\n",
    "            committed += len(commit_seq)\n",
    "\n",
    "            if commit_seq and commit_seq[-1] == EOS_ID:\n",
    "                break\n",
    "\n",
    "        # punctuation-based early stop\n",
    "        tail = verifier_tok.decode(ids[0][-12:], skip_special_tokens=True).strip()\n",
    "        if tail.endswith((\".\", \"!\", \"?\")):\n",
    "            break\n",
    "\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879dd45-f0de-4b68-90a2-127fc4a47640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sampling ===\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In the distant future, \"\n",
    "\n",
    "print(\"=== sampling ===\")\n",
    "print(sample_generate(prompt))\n",
    "\n",
    "print(\"\\n=== greedy (baseline) ===\")\n",
    "print(greedy_generate(prompt))\n",
    "\n",
    "print(\"\\n=== medusa-lite ===\")\n",
    "print(medusa_lite_generate(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1f95b-7565-4d0f-93f7-d7e4e30416db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9762485-3975-48ac-ac56-028559815200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medusa-lite flow : drafter ‚Üí verifier ‚Üí multi-branch prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6299ea84-b582-446e-bd6a-0af724e42748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : ÏÑ§Ïπò & Î≤ÑÏ†Ñ ÌôïÏù∏\n",
    "#Step 2 : Device ÏÑ†ÌÉù (mps / cuda / cpu Ï§ë ÌïòÎÇò)\n",
    "#Step 3 : Seed Í≥†Ï†ï\n",
    "#Step 4 : Config ÏûëÏÑ±\n",
    "#Step 5~ : Drafter / Verifier Î°úÎìú ‚Üí Prompt Ï§ÄÎπÑ ‚Üí Draft/Verify Ìï®ÏàòÎì§ ‚Ä¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d278f2b1-6d94-40e5-94fd-f6a4486f3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a4b816c-0752-4c21-87bf-6e3353c09c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Device ÏÑ†ÌÉù\n",
    "import torch\n",
    "\n",
    "def pick_device():\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"     # üñ•Ô∏è Îß•Î∂ÅÏù¥Î©¥ mps\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"    # üíª GPU ÏûàÏúºÎ©¥ cuda\n",
    "    return \"cpu\"         # ÎÇòÎ®∏ÏßÄÎäî cpu\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"‚úÖ DEVICE =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be179cbe-9553-4ae4-ba55-b6fd8a86443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert DEVICE in {\"cpu\", \"cuda\", \"mps\"}\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d40831c4-7374-42c5-8ae6-071a3fcca4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed Í≥†Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "761bcb37-1091-41ab-9f7f-0919b5d333f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ seed set\n"
     ]
    }
   ],
   "source": [
    "import random, torch\n",
    "def set_seed(seed:int=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print('‚úÖ seed set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf7595e9-a584-4c8e-ba14-565770f20edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acebf27b-9e2b-4f04-8d6a-66f91040c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    DRAFTER_ID: str = \"distilgpt2\"\n",
    "    VERIFIER_ID: str = \"gpt2\"\n",
    "    TOPK_BRANCH: int = 3\n",
    "    DRAFT_SPAN: int = 3\n",
    "    MAX_NEW_TOKENS: int = 30     # 40 ‚Üí 30ÏúºÎ°ú\n",
    "    TEMPERATURE: float = 0.8\n",
    "    TOP_P: float = 0.9\n",
    "    REPETITION_PENALTY: float = 1.2\n",
    "    NO_REPEAT_NGRAM: int = 4\n",
    "    DEVICE: str = DEVICE\n",
    "\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20cb78d3-0d62-44f6-be57-aaeaa4a320ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ drafter ready\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "drafter_tok = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "drafter     = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE)\n",
    "drafter.eval()\n",
    "\n",
    "print('üî∏ drafter ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "619d9bb7-e79a-47c1-adea-59cd6ea44469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drafter loaded? -> True\n"
     ]
    }
   ],
   "source": [
    "ok = (drafter_tok is not None) and (drafter is not None)\n",
    "print('drafter loaded? ->', ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a318b2df-5512-490e-a859-ef89653f131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ verifier ready\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "verifier     = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE)\n",
    "verifier.eval()\n",
    "\n",
    "print('üî∏ verifier ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "887abcfd-0027-4f48-afab-82c69b54adfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verifier loaded? -> True\n"
     ]
    }
   ],
   "source": [
    "ok = (verifier_tok is not None) and (verifier is not None)\n",
    "print('verifier loaded? ->', ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c85928e-c0ca-45e6-98e6-bca74978f2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ok? True | shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "#Prompt & Context preparation# Prompt & Context\n",
    "prompt = \"In a distant future, a small crew of explorers discovers \"\n",
    "\n",
    "# drafter ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ°ú Ïù∏ÏΩîÎî© + DEVICE Ïò¨Î¶¨Í∏∞\n",
    "ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "\n",
    "# input_idsÎßå Îî∞Î°ú Í∫ºÎÇ¥Í∏∞\n",
    "input_ids = ctx[\"input_ids\"]\n",
    "\n",
    "print(\"context ok?\", ctx is not None, \"| shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "98b8ff42-9731-41a4-801f-45d721747401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draft Ìïú ÌÜ†ÌÅ∞ ÏÉùÏÑ± Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ffb7537-f560-4aab-ac6e-1b1623ff5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.inference_mode()\n",
    "def draft_one_token(model, ids, temperature:float=1.0):\n",
    "    # 1) ÎßàÏßÄÎßâ ÌÜ†ÌÅ∞ ÏúÑÏπòÏùò logits Í∫ºÎÇ¥Í∏∞\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "\n",
    "    # 2) softmax ‚Üí ÌôïÎ•† Î∂ÑÌè¨\n",
    "    probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    # 3) ÌôïÎ•† Î∂ÑÌè¨ÏóêÏÑú Ìïú Í∞ú ÌÜ†ÌÅ∞ ÎΩëÍ∏∞\n",
    "    next_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    return next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5f13abc-d291-46f3-a58d-8d07f1038e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏÉòÌîåÎßÅÎêú ÌÜ†ÌÅ∞ ID: tensor([[2575]], device='mps:0')\n",
      "ÌÜ†ÌÅ∞ Î¨∏ÏûêÏó¥: urch\n"
     ]
    }
   ],
   "source": [
    " # Ïã§Ï†úÎ°ú Ïã§ÌñâÌï¥Î≥¥Í∏∞\n",
    "sample_id = draft_one_token(drafter, input_ids, 0.8)\n",
    "print(\"ÏÉòÌîåÎßÅÎêú ÌÜ†ÌÅ∞ ID:\", sample_id)\n",
    "print(\"ÌÜ†ÌÅ∞ Î¨∏ÏûêÏó¥:\", drafter_tok.decode(sample_id[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "05f3c58f-5c17-489b-b307-a7d2dd64b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Î©ÄÌã∞-Î∏åÎûúÏπò Draft Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6acbe69-70cb-4b4f-afdb-7908f2bf7701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, topk:int, span:int, temperature:float):\n",
    "    branches = []\n",
    "    for _ in range(topk):\n",
    "        cur = ids.clone()\n",
    "        branch = []\n",
    "        for __ in range(span):\n",
    "            nxt = draft_one_token(drafter, cur, temperature)\n",
    "            cur = torch.cat([cur, nxt], dim=1)\n",
    "            branch.append(int(nxt[0,0]))\n",
    "        branches.append(branch)\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "108d223a-931f-4789-b78b-33d65026e453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 branches; span len: 3\n",
      "ÏòàÏãú Î∏åÎûúÏπò 1: [1849, 286, 262]\n"
     ]
    }
   ],
   "source": [
    "b = drafter_propose(input_ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(len(b), 'branches; span len:', len(b[0]) if b else None)\n",
    "print('ÏòàÏãú Î∏åÎûúÏπò 1:', b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7e166399-a8cb-48ef-b757-f4a3562162ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifier: Ìïú ÌÜ†ÌÅ∞ ÏòàÏ∏°(greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fcb55830-37c2-43e4-9240-c417743ff55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def verifier_next_token(ids) -> int:\n",
    "    logits = verifier(ids).logits[:, -1, :]\n",
    "    pred = int(torch.argmax(logits, dim=-1)[0])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5ece301-43be-417e-ad46-ec08c23665eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred id = 1849 | token = ¬†\n"
     ]
    }
   ],
   "source": [
    "vid = verifier_next_token(input_ids)\n",
    "print('pred id =', vid, '| token =', verifier_tok.decode([vid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "62302574-89ee-47e3-9d54-452b7c689232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix-Accept (mismatchÍπåÏßÄ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cacfc7d1-22c3-48a7-98eb-fc52f6aa1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch(context_ids, branch_tokens:List[int]) -> Tuple[torch.Tensor, List[int], bool]:\n",
    "    ids = context_ids.clone()\n",
    "    accepted = []\n",
    "    mismatched = False\n",
    "    for tid in branch_tokens:\n",
    "        pred = verifier_next_token(ids)\n",
    "        if pred == tid:\n",
    "            ids = torch.cat([ids, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "            accepted.append(tid)\n",
    "        else:\n",
    "            ids = torch.cat([ids, torch.tensor([[pred]], device=ids.device)], dim=1)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return ids, accepted, mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d59c679d-f816-45d7-9888-37455ca8f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted len: 1 | mismatched? True\n",
      "ÏÉà Í∏∏Ïù¥: 14 | Ï∂îÍ∞ÄÎêú ÌÜ†ÌÅ∞ Ïàò: 2\n"
     ]
    }
   ],
   "source": [
    "# Î∞©Í∏à ÎßåÎì† Î∏åÎûúÏπòÎì§ Ï§ë Ï≤´ Î≤àÏß∏Î•º Í≤ÄÏÇ¨Ìï¥Î≥¥Í∏∞\n",
    "new_ids, accepted, mism = accept_until_mismatch(input_ids, b[0])\n",
    "print('accepted len:', len(accepted), '| mismatched?', mism)\n",
    "print('ÏÉà Í∏∏Ïù¥:', new_ids.shape[1], '| Ï∂îÍ∞ÄÎêú ÌÜ†ÌÅ∞ Ïàò:', new_ids.shape[1] - input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f87105b-f16f-469a-94b1-bca68816c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator (medusa_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "643f23de-59cc-4bee-b5ad-1a8e0e6b881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "285399ef-d503-4d20-85ae-6fdbc20d0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch Ï†êÏàò Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d206eebe-4c18-4eb1-9f3f-c9cf89496c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def score_branch(accepted, mismatched):\n",
    "    # prefix-acceptÎêú ÌÜ†ÌÅ∞ Ïàò - mismatch Ìå®ÎÑêÌã∞\n",
    "    return len(accepted) - (1 if mismatched else 0)\n",
    "\n",
    "# ‚úîÔ∏è Ï≤¥ÌÅ¨\n",
    "print(score_branch([1,2,3], False))  # 3\n",
    "print(score_branch([1,2], True))     # 1 (2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "167ccec6-3cfa-4b9b-a714-8d7170d4fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt ‚Üí ÌÜ†ÌÅ∞Ìôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7c184ea1-2fb6-4d9f-a76f-b81409ae3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def encode_prompt(prompt: str):\n",
    "    ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    return ctx[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c68db797-01f6-4701-acff-4eaca8999703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids.shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# ‚úîÔ∏è Ï≤¥ÌÅ¨\n",
    "ids = encode_prompt(\"In a distant future, \")\n",
    "print(\"ids.shape:\", ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07366b1d-95a5-4489-973b-ba1b058f81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ìïú Ïä§ÌÖù ÏàòÌñâ(multi-branch‚ÜíÍ≤ÄÏ¶ù‚ÜíÏµúÍ≥† Ï†êÏàò Ï±ÑÌÉù)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f435d555-4430-40e7-be8c-f69e96f882cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_step(ids, topk_branch:int, draft_span:int, temperature:float):\n",
    "    branches = drafter_propose(ids, topk_branch, draft_span, temperature)\n",
    "    best_score = -10**9\n",
    "    best_ids = None\n",
    "    for br in branches:\n",
    "        new_ids, accepted, mism = accept_until_mismatch(ids, br)\n",
    "        s = score_branch(accepted, mism)\n",
    "        if s > best_score:\n",
    "            best_score, best_ids = s, new_ids\n",
    "    return best_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aa24f1c1-7666-4c0d-a92a-75b3a684ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 6 ‚Üí after: 7\n"
     ]
    }
   ],
   "source": [
    "ids2 = medusa_step(ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(\"before:\", ids.shape[1], \"‚Üí after:\", ids2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "815d20d2-2529-4062-a0f9-b2aaee3bfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "382dd505-017f-4401-acfc-f514444add48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_generate(prompt:str,\n",
    "                    max_new_tokens:int=None,\n",
    "                    topk_branch:int=None,\n",
    "                    draft_span:int=None,\n",
    "                    temperature:float=None) -> str:\n",
    "    if max_new_tokens is None: max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "    if topk_branch   is None: topk_branch   = cfg.TOPK_BRANCH\n",
    "    if draft_span    is None: draft_span    = cfg.DRAFT_SPAN\n",
    "    if temperature   is None: temperature   = cfg.TEMPERATURE\n",
    "\n",
    "    ids = encode_prompt(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "\n",
    "    steps = math.ceil(max_new_tokens / draft_span)\n",
    "    for _ in range(steps):\n",
    "        ids = medusa_step(ids, topk_branch, draft_span, temperature)\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "    return drafter_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d4e20072-9b94-4ef3-a184-7517d4bee71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, ¬†the world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place\n"
     ]
    }
   ],
   "source": [
    "out = medusa_generate(\"In a distant future, \", 40)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "150f8842-980f-490f-bd83-b18debebf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13) Greedy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "49d1e7b2-2ee6-4a1f-85f8-5f4c2efbc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int = None) -> str:\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    # verifier Í∏∞Ï§ÄÏúºÎ°ú ÏÉùÏÑ± (ÎπÑÍµêÍµ∞)\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = verifier(ids).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])  # greedy\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be857f32-1aef-4682-bfdf-693ba442fa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, ¬†the world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would be a better place.\n",
      "The world would\n"
     ]
    }
   ],
   "source": [
    "txt = greedy_generate(\"In a distant future, \", 40)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb4ffe-b3ab-4512-aec1-3700130e74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) A/B ÏÜçÎèÑ¬∑ÌÖçÏä§Ìä∏ ÎπÑÍµê ÏÖÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
