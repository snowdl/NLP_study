{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "027b6ef1-40ec-4572-abce-5be522107754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ imports ok — torch 2.8.0\n",
      "✅ device: mps\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 1) Imports =====\n",
    "import time, math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Print library versions to confirm environment is set up\n",
    "print(\"✅ imports ok — torch\", torch.__version__)\n",
    "\n",
    "# ===== Step 2) Pick device =====\n",
    "# Prefer the fastest available backend:\n",
    "# - On Apple Silicon, use Metal Performance Shaders ('mps')\n",
    "# - Else if a CUDA-capable GPU is present, use 'cuda'\n",
    "# - Otherwise fall back to CPU\n",
    "DEVICE = (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "print(\"✅ device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a25ae03a-64e3-4ba2-bffb-843f68f23af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ models ready: distilgpt2 / gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 3) Load models & tokenizers =====\n",
    "\n",
    "# Model IDs (Hugging Face Hub names)\n",
    "DRAFTER_ID  = \"distilgpt2\"     # small, fast \"drafter\" model\n",
    "VERIFIER_ID = \"gpt2-medium\"    # larger, stronger \"verifier\" model\n",
    "\n",
    "# Load tokenizers (text ↔ token IDs)\n",
    "drafter_tok  = AutoTokenizer.from_pretrained(DRAFTER_ID)\n",
    "verifier_tok = AutoTokenizer.from_pretrained(VERIFIER_ID)\n",
    "\n",
    "# GPT-2 family often has no EOS/PAD defined by default.\n",
    "# Define them so decoding and padding behave consistently.\n",
    "if verifier_tok.eos_token_id is None:\n",
    "    verifier_tok.eos_token = \"\"      # set EOS to the special  token\n",
    "if verifier_tok.pad_token_id is None:\n",
    "    verifier_tok.pad_token = verifier_tok.eos_token  # use EOS as PAD to avoid mismatch\n",
    "EOS_ID = verifier_tok.eos_token_id\n",
    "\n",
    "# Load models and move them to the selected device.\n",
    "# .eval() disables dropout etc. for deterministic inference.\n",
    "drafter  = AutoModelForCausalLM.from_pretrained(DRAFTER_ID).to(DEVICE).eval()\n",
    "verifier = AutoModelForCausalLM.from_pretrained(VERIFIER_ID).to(DEVICE).eval()\n",
    "\n",
    "print(\"✅ models ready:\", DRAFTER_ID, \"/\", VERIFIER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d09b8aee-eb5b-42c9-bec6-9e13773fb4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3A) Encode / Decode / Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6d332bf-63b0-45b5-be41-9e8fbd95ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Text I/O utilities =====\n",
    "\n",
    "# Turn a prompt string into model-ready token IDs.\n",
    "# We use the *verifier* tokenizer for consistency throughout the pipeline.\n",
    "def encode_prompt(prompt: str):\n",
    "    return verifier_tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "\n",
    "# Decode token IDs back to text.\n",
    "# Disable HuggingFace's auto \"clean_up_tokenization_spaces\" so we can normalize ourselves.\n",
    "def decode_ids(ids) -> str:\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "# Minimal whitespace normalization:\n",
    "# - Replace NBSP (U+00A0) with a normal space\n",
    "# - Collapse multiple spaces/newlines/tabs into a single space\n",
    "# - Trim leading/trailing spaces\n",
    "def normalize_text(s: str) -> str:\n",
    "    return \" \".join(s.replace(\"\\u00A0\", \" \").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec2fb3be-c098-418e-8d63-f87e96070651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([1, 19])\n",
      "RAW decoded: 'In a distant future, \\xa0  the crew   finds   a signal!  '\n",
      "NORM decoded: 'In a distant future, the crew finds a signal!'\n",
      "len before append: 1\n",
      "len after append: 2\n"
     ]
    }
   ],
   "source": [
    "# ===== Quick tests =====\n",
    "# (Assumes DEVICE, verifier_tok are already defined and models are loaded.)\n",
    "\n",
    "# 1) Encode → Decode → Normalize round trip\n",
    "prompt = \"In a distant future, \" + \"\\u00A0\" + \"  the crew   finds   a signal!  \"\n",
    "ids = encode_prompt(prompt)\n",
    "print(\"input_ids shape:\", ids.shape)\n",
    "\n",
    "decoded_raw = decode_ids(ids)\n",
    "decoded_norm = normalize_text(decoded_raw)\n",
    "\n",
    "print(\"RAW decoded:\", repr(decoded_raw))     # show raw string with possible NBSPs/spaces\n",
    "print(\"NORM decoded:\", repr(decoded_norm))   # cleaned version\n",
    "\n",
    "# 2) Extra sanity: encode a simple prompt and confirm tokens grow after appending a word\n",
    "ids2 = encode_prompt(\"Hello\")\n",
    "print(\"len before append:\", ids2.shape[1])\n",
    "# Simulate appending a token (space + 'world' piece) using tokenizer\n",
    "more = verifier_tok(\" world\", return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]\n",
    "ids2_appended = torch.cat([ids2, more[:, 0:1]], dim=1)  # append just one token for demo\n",
    "print(\"len after append:\", ids2_appended.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c577e21-99d5-4064-9ab4-b332d4ae4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3B) Drafter: clean sampling utilities (filter invisible tokens, add top-p) =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e636aca1-aa5e-4f2a-9ade-04d99d9a89ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible token filter:\n",
    "# - must contain at least one printable, non-whitespace char\n",
    "# - reject the Unicode replacement char '�' (U+FFFD)\n",
    "def _is_visible_token(tid: int, tok) -> bool:\n",
    "    s = tok.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    if not any(ch.isprintable() and not ch.isspace() for ch in s):\n",
    "        return False\n",
    "    if \"\\uFFFD\" in s:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens_clean(ids, k: int, temperature: float = 0.9, top_p: float = 0.95):\n",
    "    \"\"\"\n",
    "    Sample up to K distinct first tokens from the drafter:\n",
    "      - apply temperature\n",
    "      - nucleus (top-p) filtering\n",
    "      - filter out pure whitespace / non-visible byte tokens\n",
    "    Returns a list[int] (length ≤ K).\n",
    "    \"\"\"\n",
    "    logits = get_last_logits(drafter, ids)         # [1, V]\n",
    "    probs  = logits_to_probs(logits, temperature)  # [V]\n",
    "\n",
    "    # nucleus (top-p) pool\n",
    "    sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "    cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "    keep = cumsum <= top_p\n",
    "    keep[0] = True                                  # always keep top-1\n",
    "    pool_ix = sorted_ix[keep].tolist()              # candidate ids\n",
    "\n",
    "    # filter to visible tokens\n",
    "    visible_ix = [int(t) for t in pool_ix if _is_visible_token(int(t), drafter_tok)]\n",
    "    if not visible_ix:\n",
    "        # fallback: top-1 (even if invisible)\n",
    "        return [int(sorted_ix[0].item())]\n",
    "\n",
    "    # renormalize over visible pool\n",
    "    pool_p = probs[visible_ix]\n",
    "    pool_p = pool_p / pool_p.sum()\n",
    "\n",
    "    # sample without replacement\n",
    "    num = min(k, len(visible_ix))\n",
    "    picks = torch.multinomial(pool_p, num_samples=num, replacement=False).tolist()\n",
    "    return [visible_ix[i] for i in picks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f305dd41-7ffd-4c92-9d45-85e31fe117e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next-token logits shape: torch.Size([1, 50257])\n",
      "probs sum (≈1.0): 0.9999999403953552\n",
      "\n",
      "K=4 filtered token IDs: [10185, 40493, 9805, 742]\n",
      "decoded (repr): [\"'!!!'\", \"'『'\", \"'????'\", \"'xt'\"]\n",
      "BPE pieces   : [\"'!!!'\", \"'ãĢİ'\", \"'????'\", \"'xt'\"]\n",
      "is whitespace: [False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# ===== Quick test for section 3B (clean sampler) =====\n",
    "prompt = \"In a distant future, \"\n",
    "ids = encode_prompt(prompt)\n",
    "\n",
    "logits = get_last_logits(drafter, ids)\n",
    "probs  = logits_to_probs(logits, temperature=0.9)\n",
    "print(\"next-token logits shape:\", logits.shape)\n",
    "print(\"probs sum (≈1.0):\", float(probs.sum().item()))\n",
    "\n",
    "K = 4\n",
    "candidates = drafter_sample_first_tokens_clean(ids, k=K, temperature=0.9, top_p=0.95)\n",
    "print(f\"\\nK={K} filtered token IDs:\", candidates)\n",
    "\n",
    "decoded = [drafter_tok.decode([t], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "           for t in candidates]\n",
    "bpe     = [drafter_tok.convert_ids_to_tokens([t])[0] for t in candidates]\n",
    "\n",
    "print(\"decoded (repr):\", [repr(s) for s in decoded])\n",
    "print(\"BPE pieces   :\", [repr(s) for s in bpe])\n",
    "print(\"is whitespace:\", [s.isspace() for s in decoded])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "caad12b5-cc99-4d12-a8e7-8ef4a562b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3C) Drafter: rollout helpers ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6604e23-3742-4122-bbfb-7270028ef684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a single token ID to the current sequence.\n",
    "# - ids: shape [1, T] (batch size 1)\n",
    "# - tok: next token id (int)\n",
    "# Returns a NEW tensor of shape [1, T+1] on the same device as `ids`.\n",
    "def append_token(ids: torch.Tensor, tok: int) -> torch.Tensor:\n",
    "    return torch.cat([ids, torch.tensor([[tok]], device=ids.device)], dim=1)\n",
    "\n",
    "# Get the drafter's greedy next-token choice for the given sequence.\n",
    "# Uses argmax over the last-position logits.\n",
    "# - ids: shape [1, T]\n",
    "# Returns: next token id (int)\n",
    "@torch.inference_mode()\n",
    "def drafter_greedy_next(ids: torch.Tensor) -> int:\n",
    "    logits = get_last_logits(drafter, ids)  # shape: [1, vocab_size]\n",
    "    return int(torch.argmax(logits, dim=-1)[0])\n",
    "\n",
    "# Roll out a branch of length `span`, starting from `first_tok`, using greedy steps.\n",
    "# The returned list includes `first_tok` and (span-1) subsequent greedy tokens.\n",
    "# - ids:       shape [1, T] (context so far)\n",
    "# - first_tok: starting token id for this branch\n",
    "# - span:      total tokens to produce for the branch (>=1)\n",
    "# Returns: list[int] of length `span`\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout_basic(ids: torch.Tensor, first_tok: int, span: int) -> list[int]:\n",
    "    seq: list[int] = [first_tok]\n",
    "    cur = append_token(ids, first_tok)\n",
    "    for _ in range(span - 1):\n",
    "        nxt = drafter_greedy_next(cur)\n",
    "        seq.append(nxt)\n",
    "        cur = append_token(cur, nxt)\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e25b7f9-bd2c-458f-8c79-ba8630e3b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Drafter: multi-branch proposal(K × span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bdc2d70-2f1e-427e-adbf-49fe9a25d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_propose_basic(ids, k: int, span: int, temperature: float = 0.8) -> list[list[int]]:\n",
    "    firsts = drafter_sample_first_tokens_basic(ids, k, temperature)\n",
    "    return [drafter_rollout_basic(ids, t, span) for t in firsts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59f6ab44-051d-425b-aa6a-50a9246628a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3E) Verifier: greedy next-token (split into tiny helpers) ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c5acf6c-d4f6-4226-96a0-2624bf1b89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the verifier's logits at the last (next-token) position.\n",
    "# - ids: shape [1, T]\n",
    "# Returns: tensor of shape [1, vocab_size]\n",
    "@torch.inference_mode()\n",
    "def verifier_last_logits(ids: torch.Tensor) -> torch.Tensor:\n",
    "    return get_last_logits(verifier, ids)\n",
    "\n",
    "# Pick the verifier's greedy next token (argmax over logits).\n",
    "# - ids: shape [1, T]\n",
    "# Returns: next token id (int)\n",
    "@torch.inference_mode()\n",
    "def verifier_greedy_next(ids: torch.Tensor) -> int:\n",
    "    logits = verifier_last_logits(ids)  # [1, V]\n",
    "    return int(torch.argmax(logits, dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4dfa0adf-bc7e-413e-bc4f-4b954afc8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before length: 6\n",
      "next token id: 1849\n",
      "decoded piece (repr): '\\xa0'\n",
      "BPE piece (repr):     'Âł'\n",
      "after length: 7\n",
      "preview: In a distant future,  \n"
     ]
    }
   ],
   "source": [
    "# ===== Quick test for section 3E =====\n",
    "# Assumes: DEVICE, verifier, verifier_tok, encode_prompt(), append_token() are defined.\n",
    "\n",
    "prompt = \"In a distant future, \"\n",
    "ids = encode_prompt(prompt)\n",
    "print(\"before length:\", ids.shape[1])\n",
    "\n",
    "# 1-step greedy with the verifier\n",
    "tid = verifier_greedy_next(ids)\n",
    "print(\"next token id:\", tid)\n",
    "print(\"decoded piece (repr):\", repr(verifier_tok.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)))\n",
    "print(\"BPE piece (repr):    \", repr(verifier_tok.convert_ids_to_tokens([tid])[0]))\n",
    "\n",
    "# Append it and show the new length + a short decode preview\n",
    "ids2 = append_token(ids, tid)\n",
    "print(\"after length:\", ids2.shape[1])\n",
    "\n",
    "preview = verifier_tok.decode(ids2[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(\"preview:\", preview[:120].replace(\"\\n\", \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dbb0434d-b082-4abe-a325-9a125ee5e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3F) Prefix-accept (split into: accept-one / commit-mismatch / loop) ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2a810e6-dae7-4451-a1b8-e8de1c41a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a branch token that matches the verifier's prediction.\n",
    "# - cur_ids: current sequence tensor [1, T]\n",
    "# - tok:     token id to append\n",
    "# Returns: new tensor [1, T+1]\n",
    "def accept_one(cur_ids: torch.Tensor, tok: int) -> torch.Tensor:\n",
    "    return append_token(cur_ids, tok)\n",
    "\n",
    "# On first mismatch, append the verifier's predicted token instead of the branch token.\n",
    "# - pred_tok: verifier-chosen token id\n",
    "def commit_mismatch(cur_ids: torch.Tensor, pred_tok: int) -> torch.Tensor:\n",
    "    return append_token(cur_ids, pred_tok)\n",
    "\n",
    "# Run prefix-accept against a candidate branch:\n",
    "# For each token in branch_tokens:\n",
    "#   - if verifier's next == branch token → accept it\n",
    "#   - else append verifier token and stop\n",
    "# Returns: (new_ids, accepted_tokens_list, mismatched_flag)\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch_basic(context_ids: torch.Tensor, branch_tokens: list[int]):\n",
    "    cur = context_ids.clone()\n",
    "    accepted: list[int] = []\n",
    "    mismatched = False\n",
    "    for t in branch_tokens:\n",
    "        # NOTE: if your helper is named `verifier_next_token`, use that.\n",
    "        # Here we use the 3E helper `verifier_greedy_next`.\n",
    "        pred = verifier_greedy_next(cur)\n",
    "        if pred == t:\n",
    "            accepted.append(t)\n",
    "            cur = accept_one(cur, t)\n",
    "        else:\n",
    "            cur = commit_mismatch(cur, pred)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return cur, accepted, mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "738ff057-6e8e-43f1-b2f1-058a0fc2be49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate branch token IDs: [11839, 11482, 6527]\n",
      "accepted len: 0 | mismatched? True\n",
      "preview: In a distant future,  \n"
     ]
    }
   ],
   "source": [
    "# ===== Quick test for section 3F =====\n",
    "# Assumes: encode_prompt, drafter_sample_first_tokens_* , drafter_rollout_basic are defined.\n",
    "\n",
    "prompt = \"In a distant future, \"\n",
    "ids = encode_prompt(prompt)\n",
    "\n",
    "# Pick a first token (try clean sampler if available; else basic)\n",
    "sampler = globals().get(\"drafter_sample_first_tokens_clean\", globals().get(\"drafter_sample_first_tokens_basic\"))\n",
    "first = sampler(ids, k=1, temperature=0.9)[0]\n",
    "\n",
    "# Draft a short candidate branch (e.g., span=3)\n",
    "branch = drafter_rollout_basic(ids, first_tok=first, span=3)\n",
    "print(\"candidate branch token IDs:\", branch)\n",
    "\n",
    "# Run prefix-accept\n",
    "new_ids, accepted, mism = accept_until_mismatch_basic(ids, branch)\n",
    "print(\"accepted len:\", len(accepted), \"| mismatched?\", mism)\n",
    "print(\"preview:\", decode_ids(new_ids)[:160].replace(\"\\n\", \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e3f0970-f7a9-4dc8-84ab-1f6eb9d5134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3G) Branch scoring & selection (simplest version) ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e96d1ae-71e3-4c83-932f-ca65949ef90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score a branch by how many tokens were prefix-accepted.\n",
    "# Apply a small penalty (-1) if a mismatch occurred.\n",
    "def score_branch_simple(accepted: list[int], mismatched: bool) -> int:\n",
    "    return len(accepted) - (1 if mismatched else 0)\n",
    "\n",
    "# Evaluate a single candidate branch:\n",
    "# - Runs prefix-accept against `branch_tokens`\n",
    "# - Returns the updated ids and the simple score\n",
    "@torch.inference_mode()\n",
    "def evaluate_branch(ids: torch.Tensor, branch_tokens: list[int]):\n",
    "    new_ids, accepted, mism = accept_until_mismatch_basic(ids, branch_tokens)\n",
    "    return new_ids, score_branch_simple(accepted, mism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42c4b37a-ad19-49cd-8817-7cc4bfcc7809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate branch: [933, 12754, 318]\n",
      "score: -1\n",
      "preview: In a distant future,  \n"
     ]
    }
   ],
   "source": [
    "# ===== Quick test for section 3G =====\n",
    "# Assumes: encode_prompt(), drafter_rollout_basic(), accept_until_mismatch_basic(),\n",
    "#          and a sampler (drafter_sample_first_tokens_clean/basic) are defined.\n",
    "\n",
    "prompt = \"In a distant future, \"\n",
    "ids = encode_prompt(prompt)\n",
    "\n",
    "# Pick a sampler (prefer the clean sampler if available)\n",
    "sampler = globals().get(\"drafter_sample_first_tokens_clean\",\n",
    "          globals().get(\"drafter_sample_first_tokens_basic\"))\n",
    "\n",
    "# Draft a small candidate branch (K=1 → take first; span=3 as a demo)\n",
    "first_tok = sampler(ids, k=1, temperature=0.9)[0]\n",
    "branch    = drafter_rollout_basic(ids, first_tok=first_tok, span=3)\n",
    "print(\"candidate branch:\", branch)\n",
    "\n",
    "# Evaluate it\n",
    "cand_ids, score = evaluate_branch(ids, branch)\n",
    "print(\"score:\", score)\n",
    "print(\"preview:\", decode_ids(cand_ids)[:160].replace(\"\\n\", \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8e6795c-95e0-4218-a156-3de5411d7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3H) Medusa 스텝 & 오케스트레이터 (얇게 구성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3762f7cd-3a62-4555-952f-8444759aa8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3H) Medusa step & orchestrator (thin version) =====\n",
    "\n",
    "# One Medusa step:\n",
    "#  - Ask the drafter to propose K branches (each of length `span`)\n",
    "#  - Run prefix-accept on each branch\n",
    "#  - Keep the candidate that scores best (simple length-first scoring)\n",
    "@torch.inference_mode()\n",
    "def medusa_step_basic(ids: torch.Tensor, k_branches: int, span: int, temperature: float = 0.8) -> torch.Tensor:\n",
    "    branches = drafter_propose_basic(ids, k_branches, span, temperature)\n",
    "    best_score, best_ids = -10**9, None\n",
    "    for br in branches:\n",
    "        cand_ids, s = evaluate_branch(ids, br)  # (updated ids, simple score)\n",
    "        if s > best_score:\n",
    "            best_score, best_ids = s, cand_ids\n",
    "    return best_ids\n",
    "\n",
    "# Orchestrator:\n",
    "#  - Repeat Medusa steps until we add ~max_new_tokens\n",
    "#  - Decode and lightly normalize whitespace before returning\n",
    "@torch.inference_mode()\n",
    "def medusa_generate_basic(prompt: str,\n",
    "                          max_new_tokens: int = 40,\n",
    "                          k_branches: int = 4,\n",
    "                          span: int = 3,\n",
    "                          temperature: float = 0.8) -> str:\n",
    "    ids = encode_prompt(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "    steps = math.ceil(max_new_tokens / span)\n",
    "    for _ in range(steps):\n",
    "        ids = medusa_step_basic(ids, k_branches, span, temperature)\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "    return normalize_text(decode_ids(ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8b42624-a002-410c-9823-fe476b11cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Medusa-lite (basic) ===\n",
      "In a distant future, the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. He wants to control the\n"
     ]
    }
   ],
   "source": [
    "# ===== Quick test for section 3H =====\n",
    "# Assumes:\n",
    "#  - encode_prompt(), decode_ids(), normalize_text()\n",
    "#  - drafter_propose_basic(), evaluate_branch()\n",
    "#  - and all earlier sections (3A–3G) are already defined.\n",
    "\n",
    "prompt = \"In a distant future, \"\n",
    "print(\"=== Medusa-lite (basic) ===\")\n",
    "print(medusa_generate_basic(prompt, max_new_tokens=40, k_branches=3, span=3, temperature=0.8)[:200])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
