{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "e46353b7-7078-415f-9040-3f20f26b8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medusa-lite flow : drafter → verifier → multi-branch prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "5ab43540-a679-4dfc-bea9-767a75607adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "4b722306-3387-4100-a24c-ac353774f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Device 선택\n",
    "\n",
    "def pick_device():\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"     # 맥북이면 mps\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"    # GPU 있으면 cuda\n",
    "    return \"cpu\"         # 나머지는 cpu\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"✅ DEVICE =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "ca4d20be-51af-47dc-b8ed-c2d34cf39cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert DEVICE in {\"cpu\", \"cuda\", \"mps\"}\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "dd100859-26a7-428d-b08c-8f99c54e7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "a7f0a711-b21e-4678-93c3-14d51d3c550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ device: mps\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch, random\n",
    "\n",
    "# Device 자동 선택\n",
    "DEVICE = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"✅ device:\", DEVICE)\n",
    "\n",
    "# 재현성(샘플링 안 쓰니까 큰 영향은 없지만 고정)\n",
    "random.seed(42); torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "7d7b0024-611a-4f72-ac17-ba8b68b9b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "87d19e12-3cde-40b6-9910-5a95ecaa3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    DRAFTER_ID: str = \"distilgpt2\"\n",
    "    VERIFIER_ID: str = \"gpt2-medium\"\n",
    "\n",
    "    MAX_NEW_TOKENS: int = 30\n",
    "\n",
    "    TEMPERATURE: float = 0.8   # 🔹 추가\n",
    "    TOP_P: float = 0.9         # 🔹 추가\n",
    "\n",
    "    REPETITION_PENALTY: float = 1.3\n",
    "    NO_REPEAT_NGRAM: int = 5\n",
    "\n",
    "    TOPK_BRANCH: int = 4\n",
    "    DRAFT_SPAN: int = 3\n",
    "\n",
    "    DEVICE: str = DEVICE\n",
    "    DEBUG: bool = False\n",
    "\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "c043eb52-3954-4c4c-9f2a-7e5a53e8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Draft model (Tokenizer-> Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "e16d4bd6-e6eb-4989-b3fe-f3f6c77a1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ models ready: distilgpt2 / gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "drafter_tok  = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "\n",
    "# gpt2 계열은 eos/pad가 비어있는 경우 多 → 보정\n",
    "if verifier_tok.eos_token_id is None:\n",
    "    verifier_tok.eos_token = \"\"\n",
    "if verifier_tok.pad_token_id is None:\n",
    "    verifier_tok.pad_token = verifier_tok.eos_token\n",
    "\n",
    "EOS_ID = verifier_tok.eos_token_id\n",
    "\n",
    "drafter  = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE).eval()\n",
    "verifier = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE).eval()\n",
    "\n",
    "# 캐시 사용 활성화(기본 True 이지만 명시)\n",
    "drafter.config.use_cache  = True\n",
    "verifier.config.use_cache = True\n",
    "\n",
    "print(\"✅ models ready:\", cfg.DRAFTER_ID, \"/\", cfg.VERIFIER_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "aeb72db2-cd74-48aa-a0a3-70d618b1c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ok? True | shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "#Prompt & Context preparation# Prompt & Context\n",
    "prompt = \"In a distant future, a small crew of explorers discovers \"\n",
    "\n",
    "# drafter 토크나이저로 인코딩 + DEVICE 올리기\n",
    "ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "\n",
    "# input_ids만 따로 꺼내기\n",
    "input_ids = ctx[\"input_ids\"]\n",
    "\n",
    "print(\"context ok?\", ctx is not None, \"| shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "8516d202-1746-4430-9347-17506532cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draft 한 토큰 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "a23bc6a6-8263-4f83-8858-1c9d93865d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens_basic(model, ids, k: int, temperature: float = 0.8):\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "    k = min(k, probs.numel())\n",
    "    picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "    return [int(i) for i in picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "1ca643d6-c40b-42a0-abc4-4d98157578d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token str (repr): '\\xa0'\n",
      "gpt2 piece: Âł\n",
      "is space? True\n"
     ]
    }
   ],
   "source": [
    "tid = 1849\n",
    "print(\"token str (repr):\", repr(drafter_tok.decode([tid])))\n",
    "print(\"gpt2 piece:\", drafter_tok.convert_ids_to_tokens([tid])[0])\n",
    "print(\"is space?\", drafter_tok.decode([tid]).isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "2392dfbb-de90-4c85-93cc-324c928f2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#멀티-브랜치 Draft 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "de327be5-fa7d-47cc-8ec8-3cf41de470da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids, k: int, temperature=0.8, top_p=0.9) -> list[int]:\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "\n",
    "    # nucleus(top-p) 샘플링\n",
    "    if top_p is not None:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "        keep = cumsum <= top_p\n",
    "        keep[0] = True\n",
    "        pool_ix = sorted_ix[keep]\n",
    "        pool_p  = probs[pool_ix] / probs[pool_ix].sum()\n",
    "        picks   = torch.multinomial(pool_p, num_samples=min(k, pool_ix.numel()), replacement=False)\n",
    "        return [int(pool_ix[i]) for i in picks]\n",
    "    else:\n",
    "        picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "        return [int(i) for i in picks]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout(ids, first_tok: int, span: int) -> list[int]:\n",
    "    cur = torch.cat([ids, torch.tensor([[first_tok]], device=ids.device)], dim=1)\n",
    "    seq = [first_tok]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        seq.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return seq\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, k: int, span: int, temperature=0.8, top_p=0.9) -> list[list[int]]:\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    branches = [drafter_rollout(ids, f, span) for f in firsts]\n",
    "    return branches\n",
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids, k: int, temperature=0.8, top_p=0.9) -> list[int]:\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "\n",
    "    # nucleus(top-p) 샘플링\n",
    "    if top_p is not None:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "        keep = cumsum <= top_p\n",
    "        keep[0] = True\n",
    "        pool_ix = sorted_ix[keep]\n",
    "        pool_p  = probs[pool_ix] / probs[pool_ix].sum()\n",
    "        picks   = torch.multinomial(pool_p, num_samples=min(k, pool_ix.numel()), replacement=False)\n",
    "        return [int(pool_ix[i]) for i in picks]\n",
    "    else:\n",
    "        picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "        return [int(i) for i in picks]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout(ids, first_tok: int, span: int) -> list[int]:\n",
    "    cur = torch.cat([ids, torch.tensor([[first_tok]], device=ids.device)], dim=1)\n",
    "    seq = [first_tok]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        seq.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return seq\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, k: int, span: int, temperature=0.8, top_p=0.9) -> list[list[int]]:\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    branches = [drafter_rollout(ids, f, span) for f in firsts]\n",
    "    return branches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1184f5-a362-4bd8-9808-440d664fd0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "93eaab97-53d5-4d35-9772-a90b97043a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifier: 한 토큰 예측(greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "a602aa44-d7b8-4cdf-a48d-9c8a37fd274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def verifier_next_token(ids) -> int:\n",
    "    \"\"\"verifier로 다음 토큰 id 하나 예측 (greedy)\"\"\"\n",
    "    logits = verifier(ids).logits[:, -1, :]\n",
    "    return int(torch.argmax(logits, dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "df8e4703-5107-4707-80f6-2dca543b9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_token(tokenizer, tid: int):\n",
    "    \"\"\"토큰 id를 여러 방식으로 표현\"\"\"\n",
    "    s_decode = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    s_token  = tokenizer.convert_ids_to_tokens([tid])[0]\n",
    "    try:\n",
    "        s_fixed = s_token.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        s_fixed = s_token\n",
    "    return {\n",
    "        \"id\": tid,\n",
    "        \"decode_repr\": repr(s_decode),   # 사람이 안 보이는 공백도 확인\n",
    "        \"token_repr\": repr(s_token),     # BPE 토큰 스트링\n",
    "        \"token_fixed\": repr(s_fixed),    # 모지바케 보정 시도\n",
    "        \"codepoints\": [hex(ord(c)) for c in s_decode],\n",
    "        \"bytes\": list(s_decode.encode(\"utf-8\")),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "6abed687-dd54-4c6f-b1dc-895d46f33f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def next_human_token(ids, tokenizer, tries=10):\n",
    "    \"\"\"보이는 토큰이 나올 때까지 최대 tries번 예측\"\"\"\n",
    "    cur = ids.clone()\n",
    "    for _ in range(tries):\n",
    "        tid = verifier_next_token(cur)\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        if any(ch.isprintable() and not ch.isspace() for ch in s):\n",
    "            return tid, s\n",
    "        cur = torch.cat([cur, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "    return tid, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "f30ef52a-82c6-4466-90e6-a8ad6d4c1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 토큰 정보: {'id': 488, 'decode_repr': \"'ich'\", 'token_repr': \"'ich'\", 'token_fixed': \"'ich'\", 'codepoints': ['0x69', '0x63', '0x68'], 'bytes': [105, 99, 104]}\n",
      "다음 보이는 토큰: 488 'ich'\n"
     ]
    }
   ],
   "source": [
    "vid = verifier_next_token(input_ids)\n",
    "info = pretty_token(verifier_tok, vid)\n",
    "\n",
    "print(\"예측 토큰 정보:\", info)\n",
    "\n",
    "t2, s2 = next_human_token(input_ids, verifier_tok)\n",
    "print(\"다음 보이는 토큰:\", t2, repr(s2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "42acd454-c409-4673-8d3b-c63d24b3c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix-Accept (mismatch까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "02cdbb53-4055-490f-80b6-b70f3d967954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch(context_ids, branch_tokens:List[int]) -> Tuple[torch.Tensor, List[int], bool]:\n",
    "    ids = context_ids.clone()\n",
    "    accepted = []\n",
    "    mismatched = False\n",
    "    for tid in branch_tokens:\n",
    "        pred = verifier_next_token(ids)\n",
    "        if pred == tid:\n",
    "            ids = torch.cat([ids, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "            accepted.append(tid)\n",
    "        else:\n",
    "            ids = torch.cat([ids, torch.tensor([[pred]], device=ids.device)], dim=1)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return ids, accepted, mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "45b684a3-ec85-4092-ab4f-88c9a384cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted len: 0 | mismatched? True\n",
      "새 길이: 13 | 추가된 토큰 수: 1\n"
     ]
    }
   ],
   "source": [
    "# 방금 만든 브랜치들 중 첫 번째를 검사해보기\n",
    "new_ids, accepted, mism = accept_until_mismatch(input_ids, b[0])\n",
    "print('accepted len:', len(accepted), '| mismatched?', mism)\n",
    "print('새 길이:', new_ids.shape[1], '| 추가된 토큰 수:', new_ids.shape[1] - input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "b938a293-6969-4edb-8784-0e1b34b473a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator (medusa_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "510d8429-152b-42b2-ab3d-46c2994a0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "ee1be8b8-5047-4ee2-adde-26409e63722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch 점수 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "c8cea79e-4e6c-474b-895c-e745b078be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def score_branch(accepted, mismatched):\n",
    "    # prefix-accept된 토큰 수 - mismatch 패널티\n",
    "    return len(accepted) - (1 if mismatched else 0)\n",
    "\n",
    "# ✔️ 체크\n",
    "print(score_branch([1,2,3], False))  # 3\n",
    "print(score_branch([1,2], True))     # 1 (2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "f6414314-abf1-484f-9a9a-d782b3773b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt → 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "757a2021-07af-452b-b469-d550e46e48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def encode_prompt(prompt: str):\n",
    "    ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    return ctx[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "512fc442-7b76-4fb3-9ffd-c618d859642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids.shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# ✔️ 체크\n",
    "ids = encode_prompt(\"In a distant future, \")\n",
    "print(\"ids.shape:\", ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "9c5a09ad-485a-4887-9fd8-c623132712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한 스텝 수행(multi-branch→검증→최고 점수 채택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "c9036549-1494-49c6-b2cb-0c55ea2ce21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_step(ids, topk_branch: int, draft_span: int, temperature: float):\n",
    "    # ↑ more diverse branches: stronger sampling\n",
    "    branches = drafter_propose(\n",
    "        ids,\n",
    "        topk_branch,\n",
    "        draft_span,\n",
    "        temperature=max(0.9, float(temperature)),  # ensure ≥ 0.9\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    best_score = -10**9\n",
    "    best_ids = None\n",
    "    for br in branches:\n",
    "        new_ids, accepted, mism = accept_until_mismatch(ids, br)\n",
    "        s = score_branch(accepted, mism)\n",
    "        if s > best_score:\n",
    "            best_score, best_ids = s, new_ids\n",
    "\n",
    "    return best_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "689ee0dd-6059-4bcd-9556-fdf4a4b5d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 6 → after: 8\n"
     ]
    }
   ],
   "source": [
    "ids2 = medusa_step(ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(\"before:\", ids.shape[1], \"→ after:\", ids2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "21a7e22b-51dd-4218-82f7-20555827dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "21accee7-8bde-4019-a992-5255ca3a5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_generate(prompt:str,\n",
    "                    max_new_tokens:int=None,\n",
    "                    topk_branch:int=None,\n",
    "                    draft_span:int=None,\n",
    "                    temperature:float=None) -> str:\n",
    "    if max_new_tokens is None: max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "    if topk_branch   is None: topk_branch   = cfg.TOPK_BRANCH\n",
    "    if draft_span    is None: draft_span    = cfg.DRAFT_SPAN\n",
    "    if temperature   is None: temperature   = cfg.TEMPERATURE\n",
    "\n",
    "    ids = encode_prompt(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "\n",
    "    steps = math.ceil(max_new_tokens / draft_span)\n",
    "    for _ in range(steps):\n",
    "        ids = medusa_step(ids, topk_branch, draft_span, temperature)\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "    return drafter_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "78601636-7a26-4a4b-a2be-0f1cbd210e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He\n"
     ]
    }
   ],
   "source": [
    "out = medusa_generate(\"In a distant future, \", 40)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "9a00ab2b-8e08-4ae9-8277-13002c95529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13) Greedy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "d3ed7155-d9e8-47b4-a337-4e64f12a4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int = None) -> str:\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    # verifier 기준으로 생성 (비교군)\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = verifier(ids).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])  # greedy\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "200e4cc7-141e-4a4f-b3be-7925962cff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants to control the world's resources so that he can rule the world. \n"
     ]
    }
   ],
   "source": [
    "txt = greedy_generate(\"In a distant future, \", 40)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "7b6f4292-a915-4184-8f0b-b279ccffaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) A/B 속도·텍스트 비교 셀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "f924eb0c-f5c1-4ac3-a144-e3904c3ffc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ greedy: 4.241 s\n",
      "⏱ medusa: 7.441 s\n",
      "\n",
      "--- greedy ---\n",
      " In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants to control the world's resources so that he can rule the world.  He wants to control the world's resources so that he can rule the world.  He wants to control the world's resources so that he can rule the world.  He wants to\n",
      "\n",
      "--- medusa ---\n",
      " In a distant future,  the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources.  He wants to control the world's resources so that he can rule the world.  He wants to control the world's resources so that he can rule the world.  He wants to control\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def time_it(fn, *args, **kwargs):\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    return out, time.perf_counter() - t0\n",
    "\n",
    "g_txt, g_t = time_it(greedy_generate, \"In a distant future, \", 80)\n",
    "m_txt, m_t = time_it(medusa_generate, \"In a distant future, \", 80)\n",
    "\n",
    "print(\"⏱ greedy:\", round(g_t, 3), \"s\")\n",
    "print(\"⏱ medusa:\", round(m_t, 3), \"s\")\n",
    "print(\"\\n--- greedy ---\\n\", g_txt[:400])\n",
    "print(\"\\n--- medusa ---\\n\", m_txt[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66a17-0016-4606-a4dc-03c8dae9450f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc0d46-c787-4048-b06c-a0ee7057924b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
