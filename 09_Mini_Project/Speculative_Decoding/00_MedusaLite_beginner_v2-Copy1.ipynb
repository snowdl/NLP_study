{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "e46353b7-7078-415f-9040-3f20f26b8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medusa-lite flow : drafter â†’ verifier â†’ multi-branch prefix-accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "5ab43540-a679-4dfc-bea9-767a75607adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "4b722306-3387-4100-a24c-ac353774f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DEVICE = mps\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Device ì„ íƒ\n",
    "\n",
    "def pick_device():\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"     # ë§¥ë¶ì´ë©´ mps\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"    # GPU ìˆìœ¼ë©´ cuda\n",
    "    return \"cpu\"         # ë‚˜ë¨¸ì§€ëŠ” cpu\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"âœ… DEVICE =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "ca4d20be-51af-47dc-b8ed-c2d34cf39cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert DEVICE in {\"cpu\", \"cuda\", \"mps\"}\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "dd100859-26a7-428d-b08c-8f99c54e7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed ê³ ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "a7f0a711-b21e-4678-93c3-14d51d3c550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… device: mps\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch, random\n",
    "\n",
    "# Device ìë™ ì„ íƒ\n",
    "DEVICE = (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "          else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"âœ… device:\", DEVICE)\n",
    "\n",
    "# ì¬í˜„ì„±(ìƒ˜í”Œë§ ì•ˆ ì“°ë‹ˆê¹Œ í° ì˜í–¥ì€ ì—†ì§€ë§Œ ê³ ì •)\n",
    "random.seed(42); torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "7d7b0024-611a-4f72-ac17-ba8b68b9b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "87d19e12-3cde-40b6-9910-5a95ecaa3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    DRAFTER_ID: str = \"distilgpt2\"\n",
    "    VERIFIER_ID: str = \"gpt2-medium\"\n",
    "\n",
    "    MAX_NEW_TOKENS: int = 30\n",
    "\n",
    "    TEMPERATURE: float = 0.8   # ğŸ”¹ ì¶”ê°€\n",
    "    TOP_P: float = 0.9         # ğŸ”¹ ì¶”ê°€\n",
    "\n",
    "    REPETITION_PENALTY: float = 1.3\n",
    "    NO_REPEAT_NGRAM: int = 5\n",
    "\n",
    "    TOPK_BRANCH: int = 4\n",
    "    DRAFT_SPAN: int = 3\n",
    "\n",
    "    DEVICE: str = DEVICE\n",
    "    DEBUG: bool = False\n",
    "\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "c043eb52-3954-4c4c-9f2a-7e5a53e8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Draft model (Tokenizer-> Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "e16d4bd6-e6eb-4989-b3fe-f3f6c77a1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… models ready: distilgpt2 / gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "drafter_tok  = AutoTokenizer.from_pretrained(cfg.DRAFTER_ID)\n",
    "verifier_tok = AutoTokenizer.from_pretrained(cfg.VERIFIER_ID)\n",
    "\n",
    "# gpt2 ê³„ì—´ì€ eos/padê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° å¤š â†’ ë³´ì •\n",
    "if verifier_tok.eos_token_id is None:\n",
    "    verifier_tok.eos_token = \"\"\n",
    "if verifier_tok.pad_token_id is None:\n",
    "    verifier_tok.pad_token = verifier_tok.eos_token\n",
    "\n",
    "EOS_ID = verifier_tok.eos_token_id\n",
    "\n",
    "drafter  = AutoModelForCausalLM.from_pretrained(cfg.DRAFTER_ID).to(cfg.DEVICE).eval()\n",
    "verifier = AutoModelForCausalLM.from_pretrained(cfg.VERIFIER_ID).to(cfg.DEVICE).eval()\n",
    "\n",
    "# ìºì‹œ ì‚¬ìš© í™œì„±í™”(ê¸°ë³¸ True ì´ì§€ë§Œ ëª…ì‹œ)\n",
    "drafter.config.use_cache  = True\n",
    "verifier.config.use_cache = True\n",
    "\n",
    "print(\"âœ… models ready:\", cfg.DRAFTER_ID, \"/\", cfg.VERIFIER_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "aeb72db2-cd74-48aa-a0a3-70d618b1c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ok? True | shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "#Prompt & Context preparation# Prompt & Context\n",
    "prompt = \"In a distant future, a small crew of explorers discovers \"\n",
    "\n",
    "# drafter í† í¬ë‚˜ì´ì €ë¡œ ì¸ì½”ë”© + DEVICE ì˜¬ë¦¬ê¸°\n",
    "ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "\n",
    "# input_idsë§Œ ë”°ë¡œ êº¼ë‚´ê¸°\n",
    "input_ids = ctx[\"input_ids\"]\n",
    "\n",
    "print(\"context ok?\", ctx is not None, \"| shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "8516d202-1746-4430-9347-17506532cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draft í•œ í† í° ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "a23bc6a6-8263-4f83-8858-1c9d93865d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens_basic(model, ids, k: int, temperature: float = 0.8):\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "    k = min(k, probs.numel())\n",
    "    picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "    return [int(i) for i in picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "1ca643d6-c40b-42a0-abc4-4d98157578d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token str (repr): '\\xa0'\n",
      "gpt2 piece: Ã‚Å‚\n",
      "is space? True\n"
     ]
    }
   ],
   "source": [
    "tid = 1849\n",
    "print(\"token str (repr):\", repr(drafter_tok.decode([tid])))\n",
    "print(\"gpt2 piece:\", drafter_tok.convert_ids_to_tokens([tid])[0])\n",
    "print(\"is space?\", drafter_tok.decode([tid]).isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "2392dfbb-de90-4c85-93cc-324c928f2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë©€í‹°-ë¸Œëœì¹˜ Draft í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "de327be5-fa7d-47cc-8ec8-3cf41de470da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids, k: int, temperature=0.8, top_p=0.9) -> list[int]:\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "\n",
    "    # nucleus(top-p) ìƒ˜í”Œë§\n",
    "    if top_p is not None:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "        keep = cumsum <= top_p\n",
    "        keep[0] = True\n",
    "        pool_ix = sorted_ix[keep]\n",
    "        pool_p  = probs[pool_ix] / probs[pool_ix].sum()\n",
    "        picks   = torch.multinomial(pool_p, num_samples=min(k, pool_ix.numel()), replacement=False)\n",
    "        return [int(pool_ix[i]) for i in picks]\n",
    "    else:\n",
    "        picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "        return [int(i) for i in picks]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout(ids, first_tok: int, span: int) -> list[int]:\n",
    "    cur = torch.cat([ids, torch.tensor([[first_tok]], device=ids.device)], dim=1)\n",
    "    seq = [first_tok]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        seq.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return seq\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, k: int, span: int, temperature=0.8, top_p=0.9) -> list[list[int]]:\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    branches = [drafter_rollout(ids, f, span) for f in firsts]\n",
    "    return branches\n",
    "@torch.inference_mode()\n",
    "def drafter_sample_first_tokens(model, ids, k: int, temperature=0.8, top_p=0.9) -> list[int]:\n",
    "    logits = model(ids).logits[:, -1, :]\n",
    "    probs  = torch.softmax(logits / max(temperature, 1e-6), dim=-1)[0]\n",
    "\n",
    "    # nucleus(top-p) ìƒ˜í”Œë§\n",
    "    if top_p is not None:\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "        keep = cumsum <= top_p\n",
    "        keep[0] = True\n",
    "        pool_ix = sorted_ix[keep]\n",
    "        pool_p  = probs[pool_ix] / probs[pool_ix].sum()\n",
    "        picks   = torch.multinomial(pool_p, num_samples=min(k, pool_ix.numel()), replacement=False)\n",
    "        return [int(pool_ix[i]) for i in picks]\n",
    "    else:\n",
    "        picks = torch.multinomial(probs, num_samples=k, replacement=False)\n",
    "        return [int(i) for i in picks]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_rollout(ids, first_tok: int, span: int) -> list[int]:\n",
    "    cur = torch.cat([ids, torch.tensor([[first_tok]], device=ids.device)], dim=1)\n",
    "    seq = [first_tok]\n",
    "    for _ in range(span - 1):\n",
    "        logits = drafter(cur).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])\n",
    "        seq.append(nxt)\n",
    "        cur = torch.cat([cur, torch.tensor([[nxt]], device=cur.device)], dim=1)\n",
    "    return seq\n",
    "\n",
    "@torch.inference_mode()\n",
    "def drafter_propose(ids, k: int, span: int, temperature=0.8, top_p=0.9) -> list[list[int]]:\n",
    "    firsts = drafter_sample_first_tokens(drafter, ids, k, temperature, top_p)\n",
    "    branches = [drafter_rollout(ids, f, span) for f in firsts]\n",
    "    return branches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1184f5-a362-4bd8-9808-440d664fd0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "93eaab97-53d5-4d35-9772-a90b97043a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifier: í•œ í† í° ì˜ˆì¸¡(greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "a602aa44-d7b8-4cdf-a48d-9c8a37fd274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def verifier_next_token(ids) -> int:\n",
    "    \"\"\"verifierë¡œ ë‹¤ìŒ í† í° id í•˜ë‚˜ ì˜ˆì¸¡ (greedy)\"\"\"\n",
    "    logits = verifier(ids).logits[:, -1, :]\n",
    "    return int(torch.argmax(logits, dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "df8e4703-5107-4707-80f6-2dca543b9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_token(tokenizer, tid: int):\n",
    "    \"\"\"í† í° idë¥¼ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ í‘œí˜„\"\"\"\n",
    "    s_decode = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    s_token  = tokenizer.convert_ids_to_tokens([tid])[0]\n",
    "    try:\n",
    "        s_fixed = s_token.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        s_fixed = s_token\n",
    "    return {\n",
    "        \"id\": tid,\n",
    "        \"decode_repr\": repr(s_decode),   # ì‚¬ëŒì´ ì•ˆ ë³´ì´ëŠ” ê³µë°±ë„ í™•ì¸\n",
    "        \"token_repr\": repr(s_token),     # BPE í† í° ìŠ¤íŠ¸ë§\n",
    "        \"token_fixed\": repr(s_fixed),    # ëª¨ì§€ë°”ì¼€ ë³´ì • ì‹œë„\n",
    "        \"codepoints\": [hex(ord(c)) for c in s_decode],\n",
    "        \"bytes\": list(s_decode.encode(\"utf-8\")),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "6abed687-dd54-4c6f-b1dc-895d46f33f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def next_human_token(ids, tokenizer, tries=10):\n",
    "    \"\"\"ë³´ì´ëŠ” í† í°ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ìµœëŒ€ triesë²ˆ ì˜ˆì¸¡\"\"\"\n",
    "    cur = ids.clone()\n",
    "    for _ in range(tries):\n",
    "        tid = verifier_next_token(cur)\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        if any(ch.isprintable() and not ch.isspace() for ch in s):\n",
    "            return tid, s\n",
    "        cur = torch.cat([cur, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "    return tid, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "f30ef52a-82c6-4466-90e6-a8ad6d4c1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ˆì¸¡ í† í° ì •ë³´: {'id': 488, 'decode_repr': \"'ich'\", 'token_repr': \"'ich'\", 'token_fixed': \"'ich'\", 'codepoints': ['0x69', '0x63', '0x68'], 'bytes': [105, 99, 104]}\n",
      "ë‹¤ìŒ ë³´ì´ëŠ” í† í°: 488 'ich'\n"
     ]
    }
   ],
   "source": [
    "vid = verifier_next_token(input_ids)\n",
    "info = pretty_token(verifier_tok, vid)\n",
    "\n",
    "print(\"ì˜ˆì¸¡ í† í° ì •ë³´:\", info)\n",
    "\n",
    "t2, s2 = next_human_token(input_ids, verifier_tok)\n",
    "print(\"ë‹¤ìŒ ë³´ì´ëŠ” í† í°:\", t2, repr(s2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "42acd454-c409-4673-8d3b-c63d24b3c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix-Accept (mismatchê¹Œì§€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "02cdbb53-4055-490f-80b6-b70f3d967954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "@torch.inference_mode()\n",
    "def accept_until_mismatch(context_ids, branch_tokens:List[int]) -> Tuple[torch.Tensor, List[int], bool]:\n",
    "    ids = context_ids.clone()\n",
    "    accepted = []\n",
    "    mismatched = False\n",
    "    for tid in branch_tokens:\n",
    "        pred = verifier_next_token(ids)\n",
    "        if pred == tid:\n",
    "            ids = torch.cat([ids, torch.tensor([[tid]], device=ids.device)], dim=1)\n",
    "            accepted.append(tid)\n",
    "        else:\n",
    "            ids = torch.cat([ids, torch.tensor([[pred]], device=ids.device)], dim=1)\n",
    "            mismatched = True\n",
    "            break\n",
    "    return ids, accepted, mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "45b684a3-ec85-4092-ab4f-88c9a384cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted len: 0 | mismatched? True\n",
      "ìƒˆ ê¸¸ì´: 13 | ì¶”ê°€ëœ í† í° ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "# ë°©ê¸ˆ ë§Œë“  ë¸Œëœì¹˜ë“¤ ì¤‘ ì²« ë²ˆì§¸ë¥¼ ê²€ì‚¬í•´ë³´ê¸°\n",
    "new_ids, accepted, mism = accept_until_mismatch(input_ids, b[0])\n",
    "print('accepted len:', len(accepted), '| mismatched?', mism)\n",
    "print('ìƒˆ ê¸¸ì´:', new_ids.shape[1], '| ì¶”ê°€ëœ í† í° ìˆ˜:', new_ids.shape[1] - input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "b938a293-6969-4edb-8784-0e1b34b473a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator (medusa_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "510d8429-152b-42b2-ab3d-46c2994a0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "ee1be8b8-5047-4ee2-adde-26409e63722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch ì ìˆ˜ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "c8cea79e-4e6c-474b-895c-e745b078be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def score_branch(accepted, mismatched):\n",
    "    # prefix-acceptëœ í† í° ìˆ˜ - mismatch íŒ¨ë„í‹°\n",
    "    return len(accepted) - (1 if mismatched else 0)\n",
    "\n",
    "# âœ”ï¸ ì²´í¬\n",
    "print(score_branch([1,2,3], False))  # 3\n",
    "print(score_branch([1,2], True))     # 1 (2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "f6414314-abf1-484f-9a9a-d782b3773b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt â†’ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "757a2021-07af-452b-b469-d550e46e48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def encode_prompt(prompt: str):\n",
    "    ctx = drafter_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    return ctx[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "512fc442-7b76-4fb3-9ffd-c618d859642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids.shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# âœ”ï¸ ì²´í¬\n",
    "ids = encode_prompt(\"In a distant future, \")\n",
    "print(\"ids.shape:\", ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "9c5a09ad-485a-4887-9fd8-c623132712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í•œ ìŠ¤í… ìˆ˜í–‰(multi-branchâ†’ê²€ì¦â†’ìµœê³  ì ìˆ˜ ì±„íƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "c9036549-1494-49c6-b2cb-0c55ea2ce21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_step(ids, topk_branch: int, draft_span: int, temperature: float):\n",
    "    # â†‘ more diverse branches: stronger sampling\n",
    "    branches = drafter_propose(\n",
    "        ids,\n",
    "        topk_branch,\n",
    "        draft_span,\n",
    "        temperature=max(0.9, float(temperature)),  # ensure â‰¥ 0.9\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    best_score = -10**9\n",
    "    best_ids = None\n",
    "    for br in branches:\n",
    "        new_ids, accepted, mism = accept_until_mismatch(ids, br)\n",
    "        s = score_branch(accepted, mism)\n",
    "        if s > best_score:\n",
    "            best_score, best_ids = s, new_ids\n",
    "\n",
    "    return best_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "689ee0dd-6059-4bcd-9556-fdf4a4b5d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 6 â†’ after: 8\n"
     ]
    }
   ],
   "source": [
    "ids2 = medusa_step(ids, cfg.TOPK_BRANCH, cfg.DRAFT_SPAN, cfg.TEMPERATURE)\n",
    "print(\"before:\", ids.shape[1], \"â†’ after:\", ids2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "21a7e22b-51dd-4218-82f7-20555827dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "21accee7-8bde-4019-a992-5255ca3a5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def medusa_generate(prompt:str,\n",
    "                    max_new_tokens:int=None,\n",
    "                    topk_branch:int=None,\n",
    "                    draft_span:int=None,\n",
    "                    temperature:float=None) -> str:\n",
    "    if max_new_tokens is None: max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "    if topk_branch   is None: topk_branch   = cfg.TOPK_BRANCH\n",
    "    if draft_span    is None: draft_span    = cfg.DRAFT_SPAN\n",
    "    if temperature   is None: temperature   = cfg.TEMPERATURE\n",
    "\n",
    "    ids = encode_prompt(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "\n",
    "    steps = math.ceil(max_new_tokens / draft_span)\n",
    "    for _ in range(steps):\n",
    "        ids = medusa_step(ids, topk_branch, draft_span, temperature)\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "\n",
    "    return drafter_tok.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "78601636-7a26-4a4b-a2be-0f1cbd210e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, Â the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. Â He\n"
     ]
    }
   ],
   "source": [
    "out = medusa_generate(\"In a distant future, \", 40)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "9a00ab2b-8e08-4ae9-8277-13002c95529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13) Greedy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "d3ed7155-d9e8-47b4-a337-4e64f12a4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str, max_new_tokens: int = None) -> str:\n",
    "    if max_new_tokens is None:\n",
    "        max_new_tokens = cfg.MAX_NEW_TOKENS\n",
    "\n",
    "    # verifier ê¸°ì¤€ìœ¼ë¡œ ìƒì„± (ë¹„êµêµ°)\n",
    "    ctx = verifier_tok(prompt, return_tensors=\"pt\").to(cfg.DEVICE)\n",
    "    ids = ctx[\"input_ids\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = verifier(ids).logits[:, -1, :]\n",
    "        nxt = int(torch.argmax(logits, dim=-1)[0])  # greedy\n",
    "        ids = torch.cat([ids, torch.tensor([[nxt]], device=ids.device)], dim=1)\n",
    "\n",
    "    return verifier_tok.decode(ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "200e4cc7-141e-4a4f-b3be-7925962cff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, Â the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. Â He wants to control the world's resources so that he can rule the world. \n"
     ]
    }
   ],
   "source": [
    "txt = greedy_generate(\"In a distant future, \", 40)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "7b6f4292-a915-4184-8f0b-b279ccffaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) A/B ì†ë„Â·í…ìŠ¤íŠ¸ ë¹„êµ ì…€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "f924eb0c-f5c1-4ac3-a144-e3904c3ffc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â± greedy: 4.241 s\n",
      "â± medusa: 7.441 s\n",
      "\n",
      "--- greedy ---\n",
      " In a distant future, Â the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. Â He wants to control the world's resources so that he can rule the world. Â He wants to control the world's resources so that he can rule the world. Â He wants to control the world's resources so that he can rule the world. Â He wants to\n",
      "\n",
      "--- medusa ---\n",
      " In a distant future, Â the world is ruled by a dictator who is obsessed with the idea of controlling the world's resources. Â He wants to control the world's resources so that he can rule the world. Â He wants to control the world's resources so that he can rule the world. Â He wants to control\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def time_it(fn, *args, **kwargs):\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    return out, time.perf_counter() - t0\n",
    "\n",
    "g_txt, g_t = time_it(greedy_generate, \"In a distant future, \", 80)\n",
    "m_txt, m_t = time_it(medusa_generate, \"In a distant future, \", 80)\n",
    "\n",
    "print(\"â± greedy:\", round(g_t, 3), \"s\")\n",
    "print(\"â± medusa:\", round(m_t, 3), \"s\")\n",
    "print(\"\\n--- greedy ---\\n\", g_txt[:400])\n",
    "print(\"\\n--- medusa ---\\n\", m_txt[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66a17-0016-4606-a4dc-03c8dae9450f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc0d46-c787-4048-b06c-a0ee7057924b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
