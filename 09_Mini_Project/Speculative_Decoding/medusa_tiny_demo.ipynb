{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7099a2de-64da-4133-986e-fae2712b408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFlow\\n\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Flow\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "208e2560-3cfc-4862-abdd-bae82e524638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Minimal setup: imports, device, and seed (modular)\n",
    "# ============================================================\n",
    "import math\n",
    "import random\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f128c4d-30a6-4176-9032-715431f3d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ device: mps\n"
     ]
    }
   ],
   "source": [
    "# ---------- 0.1 Device ----------\n",
    "def pick_device() -> str:\n",
    "    \"\"\"Pick best available device: MPS (Apple) → CUDA → CPU.\"\"\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = pick_device()\n",
    "print(\"✅ device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "130b7230-263c-455a-94fc-20e0058061df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   WHY DO WE SET A SEED?\\n   - Our code has two decoding paths:\\n       (1) Greedy decoding (deterministic): picks argmax every step -> NO randomness involved.\\n           Seeding is NOT required for pure greedy baselines.\\n       (2) Medusa-like path (sampling): uses torch.multinomial(...) to sample tokens\\n           with temperature/top-p -> this IS stochastic.\\n           Seeding makes the sampled outputs reproducible across runs.\\n\\n   FLOW / HOW TO USE:\\n   - Keep seeding OFF when you run greedy-only baselines to show they are deterministic by nature.\\n   - Turn seeding ON right before you call any function that samples tokens\\n     (e.g., propose_branch / medusa_tiny) to get repeatable experiments.\\n   - Optionally turn it OFF again after the experiment if you want variability in later runs.\\n\\n   EXAMPLES:\\n   - Deterministic greedy baseline:\\n       set_seed(None)                  # turn off seeding\\n       out_g = greedy_generate(...)\\n\\n   - Reproducible sampling run:\\n       set_seed(42)                    # fix RNG state\\n       out_m = medusa_tiny(...)        # sampling -> same output every time with the same seed\\n       set_seed(None)                  # (optional) restore randomness for later calls\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    "    WHY DO WE SET A SEED?\n",
    "    - Our code has two decoding paths:\n",
    "        (1) Greedy decoding (deterministic): picks argmax every step -> NO randomness involved.\n",
    "            Seeding is NOT required for pure greedy baselines.\n",
    "        (2) Medusa-like path (sampling): uses torch.multinomial(...) to sample tokens\n",
    "            with temperature/top-p -> this IS stochastic.\n",
    "            Seeding makes the sampled outputs reproducible across runs.\n",
    "\n",
    "    FLOW / HOW TO USE:\n",
    "    - Keep seeding OFF when you run greedy-only baselines to show they are deterministic by nature.\n",
    "    - Turn seeding ON right before you call any function that samples tokens\n",
    "      (e.g., propose_branch / medusa_tiny) to get repeatable experiments.\n",
    "    - Optionally turn it OFF again after the experiment if you want variability in later runs.\n",
    "\n",
    "    EXAMPLES:\n",
    "    - Deterministic greedy baseline:\n",
    "        set_seed(None)                  # turn off seeding\n",
    "        out_g = greedy_generate(...)\n",
    "\n",
    "    - Reproducible sampling run:\n",
    "        set_seed(42)                    # fix RNG state\n",
    "        out_m = medusa_tiny(...)        # sampling -> same output every time with the same seed\n",
    "        set_seed(None)                  # (optional) restore randomness for later calls\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ee454d7-d55b-44b5-b9ff-32af27d7111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: Optional[int] = 42) -> None:\n",
    "    if seed is None:\n",
    "        return  # no seeding -> each sampling run can differ\n",
    "    import random\n",
    "    random.seed(seed)            # Python RNG\n",
    "    torch.manual_seed(seed)      # PyTorch CPU RNG\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)  # PyTorch CUDA RNG for all GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2167ec5-6cdf-4e40-b609-d70efaffa6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "out1 = medusa_tiny(\"Long ago, \", max_new_tokens=30)\n",
    "set_seed(42)\n",
    "out2 = medusa_tiny(\"Long ago, \", max_new_tokens=30)\n",
    "print(out1 == out2)  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1ee9677-e2ee-4ce8-a7d7-4c7c8c4713df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_tokenizer: wraps Hugging Face’s built-in AutoTokenizer.from_pretrained(...) and then patches special tokens for GPT-2–family models (which often ship without explicit EOS/PAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0538df98-4134-45e5-8a44-f16fa76eb94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_id: str) -> AutoTokenizer:\n",
    "    #uses Hugging Face's official API to loead the correct tokenizer efor model_id/distilgpt2\n",
    "    \"\"\"\n",
    "    Load tokenizer and patch EOS/PAD for GPT-2-like models/ GPT2 uses BPE tokenizer\n",
    "    (they often lack explicit eos/pad tokens).\n",
    "    \"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    \"\"\"\n",
    "    In GPT-2 models, it’s common for eos_token_id to be unset.\n",
    "We declare the special token that corresponds to the empty string (\"\")—typically ID 50256 in classic GPT-2—as the EOS to populate eos_token_id.\n",
    "Why? Because eos_token_id is needed to terminate generation and to mask EOS in the early steps (to prevent immediate early termination), among other controls.\n",
    "    \"\"\"\n",
    "    if tok.eos_token_id is None:\n",
    "        tok.eos_token = \"\"         # End-Of-Sequence token text (common for GPT-2)\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dce1875b-76af-4a73-8cea-f5de00d17db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id: str, device: str) -> AutoModelForCausalLM:\n",
    "    \"\"\"Load causal LM on device with KV-cache enabled.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device).eval()\n",
    "    model.config.use_cache = True\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_ID = \"distilgpt2\"  # small & fast for demos\n",
    "tok = load_tokenizer(MODEL_ID)\n",
    "model = load_model(MODEL_ID, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "960c596c-7c1b-418b-85b9-8b7468eec35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1.1 Special IDs ----------\n",
    "def get_special_ids(tokenizer: AutoTokenizer) -> Tuple[Optional[int], Optional[int]]:\n",
    "    \"\"\"Return (EOS_ID, PAD_ID) from tokenizer.\"\"\"\n",
    "    return tokenizer.eos_token_id, tokenizer.pad_token_id\n",
    "\n",
    "EOS_ID, PAD_ID = get_special_ids(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15b1a13f-3daa-4343-a26b-5b5781825526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS_ID=50256, PAD_ID=50256, #BAN_IDS=22\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1-Extra) Ban-list builders (fight NBSP/whitespace loops)\n",
    "# ============================================================\n",
    "def get_nbsp_token_ids(tokenizer: AutoTokenizer) -> List[int]:\n",
    "    \"\"\"\n",
    "    Return ALL token IDs that represent NBSP (U+00A0).\n",
    "    In byte-level BPE it can be 1+ IDs.\n",
    "    \"\"\"\n",
    "    return tokenizer.encode(\"\\u00A0\", add_special_tokens=False)\n",
    " \n",
    "def build_whitespace_banlist(tokenizer: AutoTokenizer) -> List[int]:\n",
    "    \"\"\"\n",
    "    Build a list of token IDs that decode to whitespace-only strings,\n",
    "    EXCLUDING a normal space ' ' (keep regular spaces).\n",
    "    \"\"\"\n",
    "    ban = []\n",
    "    for tid in range(tokenizer.vocab_size):\n",
    "        s = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        if s and s.isspace() and s != \" \":\n",
    "            ban.append(tid)\n",
    "    return ban\n",
    "\n",
    "# Choose your strategy:\n",
    "#   A) Only NBSP            → safer/targeted\n",
    "#   B) All whitespace-only  → stronger (recommended)\n",
    "BAN_IDS = torch.tensor(build_whitespace_banlist(tok), device=DEVICE, dtype=torch.long)\n",
    "print(f\"EOS_ID={EOS_ID}, PAD_ID={PAD_ID}, #BAN_IDS={BAN_IDS.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6744c967-e3aa-46e9-9aa0-f5845ff90aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#BAN_IDS: 22\n",
      "Sample banned IDs: [197, 198, 199, 200, 201, 216, 217, 218, 219, 628]\n",
      "197 '\\t' isspace? True\n",
      "198 '\\n' isspace? True\n",
      "199 '\\x0b' isspace? True\n",
      "200 '\\x0c' isspace? True\n",
      "201 '\\r' isspace? True\n"
     ]
    }
   ],
   "source": [
    "print(\"#BAN_IDS:\", BAN_IDS.numel())\n",
    "# 상위 몇 개 확인\n",
    "print(\"Sample banned IDs:\", BAN_IDS[:10].tolist())\n",
    "\n",
    "# 실제로 디코딩해 확인\n",
    "for tid in BAN_IDS[:5].tolist():\n",
    "    s = tok.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    print(tid, repr(s), \"isspace?\", s.isspace())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a39e643-d89b-4dcf-8f5b-1f41f07ff3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Text I/O helpers (encode/decode/append + normalization)\n",
    "# ============================================================\n",
    "def normalize_nbsp_to_space(text: str) -> str:\n",
    "    \"\"\"Replace NBSP (U+00A0) with a regular space for cleaner display.\"\"\"\n",
    "    return text.replace(\"\\u00A0\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa127801-c8ec-46f5-ae99-b15858f1fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(prompt: str) -> torch.Tensor:\n",
    "    \"\"\"Text → token IDs on DEVICE. Shape: [1, T].\"\"\"\n",
    "    return tok(prompt, return_tensors=\"pt\").to(DEVICE)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "60f17493-a890-479c-92c9-c0f43f8ee16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: torch.Tensor) -> str:\n",
    "    \"\"\"Token IDs → text (with NBSP normalized).\"\"\"\n",
    "    txt = tok.decode(ids[0], skip_special_tokens=True)\n",
    "    return normalize_nbsp_to_space(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eee5f291-4128-4bdc-82f2-2c6270c2fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_token(ids: torch.Tensor, tid: int) -> torch.Tensor:\n",
    "    \"\"\"Append single token ID to [1, T] sequence (preserve dtype/device).\"\"\"\n",
    "    t = torch.tensor([[tid]], dtype=ids.dtype, device=ids.device)\n",
    "    return torch.cat([ids, t], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "28b9261c-2f1d-4339-b346-1675500300bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Logits utilities (last-step + masking)\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def last_step_logits(ids: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return logits for the last position. Shape: [V].\"\"\"\n",
    "    return model(ids).logits[:, -1, :][0]\n",
    "    \"\"\"\n",
    "    1) model(ids)\n",
    "    Runs a forward pass of the language model on the token IDs ids (shape typically [B, T]).\n",
    "    2) .logits\n",
    "    Returns the raw scores (logits) for each position and each vocabulary token.\n",
    "    Shape: [B, T, V]\n",
    "    B: batch size\n",
    "    T: sequence length\n",
    "    V: vocabulary size (number of next-token candidates)\n",
    "    3) [:, -1, :]\n",
    "    Selects:\n",
    "    all batch items (:),\n",
    "    the last time step (-1),\n",
    "    all vocabulary dimensions (:).\n",
    "    Result shape: [B, V] — per-batch “next-token” logit vectors.\n",
    "    4) [0]\n",
    "Takes the first batch item only.\n",
    "Final shape: [V] — the next-token logits vector for batch item 0.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55cd13ae-8e75-4ae9-8a4f-527d18b65349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids.shape = (1, 6)\n",
      "vec.shape = (50257,)\n"
     ]
    }
   ],
   "source": [
    "# 1) Encode a prompt → ids: [1, T]\n",
    "prompt = \"In a distant future, \"\n",
    "inps = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "ids = inps[\"input_ids\"]  # shape: [1, T]\n",
    "\n",
    "# 2) Take last-step logits for the first (and only) batch item\n",
    "vec = model(**inps).logits[:, -1, :][0]   # == model(ids).logits[:, -1, :][0]\n",
    "print(\"ids.shape =\", tuple(ids.shape))     # expect: (1, T)\n",
    "print(\"vec.shape =\", tuple(vec.shape))     # expect: (V,)  i.e., [V]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "deb047af-414f-47de-bff5-498fb57ee58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In-place: forbid EOS by setting its logit to -inf.\\nMake probability zero: If you set the EOS logit to −∞ before softmax, the EOS probability becomes exactly 0 after softmax, so both sampling and greedy can never select it.\\nPrevent early termination: GPT-2 models often pick EOS in the very first steps and stop immediately. By banning EOS only for the first N steps (e.g., ban_eos_steps=8), you force the model to produce at least some tokens.\\nControlled behavior: Using a ban flag lets you apply this only when desired; later you can lift the ban to allow normal termination via EOS.\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In-place: forbid EOS by setting its logit to -inf.\n",
    "Make probability zero: If you set the EOS logit to −∞ before softmax, the EOS probability becomes exactly 0 after softmax, so both sampling and greedy can never select it.\n",
    "Prevent early termination: GPT-2 models often pick EOS in the very first steps and stop immediately. By banning EOS only for the first N steps (e.g., ban_eos_steps=8), you force the model to produce at least some tokens.\n",
    "Controlled behavior: Using a ban flag lets you apply this only when desired; later you can lift the ban to allow normal termination via EOS.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c860ce53-9cd4-4259-9bcc-a1a65095e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_eos_(logits: torch.Tensor, ban: bool) -> None:\n",
    "    \"\"\"In-place: forbid EOS by setting its logit to -inf.\"\"\"\n",
    "    if ban and EOS_ID is not None:\n",
    "        logits[EOS_ID] = float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c1e0fc7-e077-41e3-8437-e3e6775466ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_bad_tokens_(logits: torch.Tensor, ban_ids: Optional[torch.Tensor]) -> None:\n",
    "    \"\"\"In-place: forbid any token in ban_ids by setting logits to -inf.\"\"\"\n",
    "    if ban_ids is not None and ban_ids.numel() > 0:\n",
    "        logits[ban_ids] = float(\"-inf\")\n",
    "\n",
    "def mask_all_(logits: torch.Tensor, *, ban_eos: bool = False, ban_ids: Optional[torch.Tensor] = None) -> None:\n",
    "    \"\"\"In-place: apply both EOS mask and ban-list mask.\"\"\"\n",
    "    mask_eos_(logits, ban=ban_eos)\n",
    "    mask_bad_tokens_(logits, ban_ids=ban_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f003ea2c-4f69-4f10-856c-0db26554489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hugging Face’s repetition_penalty is a technique that lowers the scores of tokens that have already appeared in the context, reducing repeated words/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72c0844c-2ffe-4ae5-a925-0b9d1dfc1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) Repetition controls (logits post-processing)\n",
    "# ============================================================\n",
    "def apply_repetition_penalty_(logits: torch.Tensor,\n",
    "                              ids: torch.Tensor,\n",
    "                              penalty: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    In-place repetition penalty (Hugging Face style):\n",
    "      - if a token appeared in the context:\n",
    "           logit > 0 → logit /= penalty\n",
    "           logit < 0 → logit *= penalty\n",
    "    \"\"\"\n",
    "    if penalty is None or float(penalty) == 1.0:\n",
    "        return\n",
    "    seen = torch.unique(ids)  # [N_seen]\n",
    "    for tid in seen.tolist():\n",
    "        t = logits[tid]\n",
    "        logits[tid] = t / penalty if t > 0 else t * penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "313fe40a-f29c-43f3-8aed-711679345b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _banned_tokens_for_no_repeat(ids: torch.Tensor, n: int) -> List[int]:\n",
    "    \"\"\"Return tokens that would create a repeated n-gram if chosen next.\"\"\"\n",
    "    if not n or n <= 0:\n",
    "        return []\n",
    "    seq = ids[0].tolist()\n",
    "    if len(seq) < n:\n",
    "        return []\n",
    "    table = {}\n",
    "    for i in range(len(seq) - n + 1):\n",
    "        prefix = tuple(seq[i:i + n - 1])\n",
    "        nx = seq[i + n - 1]\n",
    "        table.setdefault(prefix, set()).add(nx)\n",
    "    cur_prefix = tuple(seq[-(n - 1):])\n",
    "    return list(table.get(cur_prefix, set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "29cc7ef3-a80f-4604-9b66-3d6138cddf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_no_repeat_ngram_(logits: torch.Tensor, ids: torch.Tensor, n: int = 0) -> None:\n",
    "    \"\"\"In-place: ban tokens that would form a repeated n-gram.\"\"\"\n",
    "    if not n or n <= 0:\n",
    "        return\n",
    "    banned = _banned_tokens_for_no_repeat(ids, n)\n",
    "    if banned:\n",
    "        logits[torch.tensor(banned, device=logits.device, dtype=torch.long)] = float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f453a07a-97bb-42a7-96dd-43601b9d6063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#greedy decoding =greedy decoding picks and appends the highest-probability token (i.e., the argmax) at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "197878e3-2f7d-43af-8dc2-d9c4d35f396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Pick the next token greedily (argmax) *after* applying safety masks and repetition controls.\\n\\n    Args:\\n        ids: Tensor of shape [1, T]. The current token sequence (same device/dtype as the model).\\n    Keyword Args:\\n        ban_eos: If True, forbid EOS for this step (helps prevent early termination in the first N steps).\\n        ban_ids: Tensor of token IDs to always forbid (e.g., NBSP/newline-only tokens). Can be None.\\n        repetition_penalty: Soft penalty (>1.0) applied to tokens that already appeared in `ids`.\\n                            Typical range: 1.1–1.3. 1.0 disables the effect.\\n        no_repeat_ngram_size: Hard constraint that prevents repeating n-grams (e.g., 4 or 5).\\n\\n    Returns:\\n        int: The ID of the selected next token (greedy argmax after all adjustments).\\n\\n    Notes:\\n        - This function assumes helper utilities exist:\\n            last_step_logits(ids) -> [V]\\n            mask_all_(logits, ban_eos, ban_ids)   # in-place: sets logits for banned tokens to -inf\\n            apply_repetition_penalty_(logits, ids, penalty)  # in-place soft downweight\\n            apply_no_repeat_ngram_(logits, ids, n)           # in-place hard ban for repeated n-grams\\n        - We clone the logits because we modify them in-place before argmax.\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Pick the next token greedily (argmax) *after* applying safety masks and repetition controls.\n",
    "\n",
    "    Args:\n",
    "        ids: Tensor of shape [1, T]. The current token sequence (same device/dtype as the model).\n",
    "    Keyword Args:\n",
    "        ban_eos: If True, forbid EOS for this step (helps prevent early termination in the first N steps).\n",
    "        ban_ids: Tensor of token IDs to always forbid (e.g., NBSP/newline-only tokens). Can be None.\n",
    "        repetition_penalty: Soft penalty (>1.0) applied to tokens that already appeared in `ids`.\n",
    "                            Typical range: 1.1–1.3. 1.0 disables the effect.\n",
    "        no_repeat_ngram_size: Hard constraint that prevents repeating n-grams (e.g., 4 or 5).\n",
    "\n",
    "    Returns:\n",
    "        int: The ID of the selected next token (greedy argmax after all adjustments).\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes helper utilities exist:\n",
    "            last_step_logits(ids) -> [V]\n",
    "            mask_all_(logits, ban_eos, ban_ids)   # in-place: sets logits for banned tokens to -inf\n",
    "            apply_repetition_penalty_(logits, ids, penalty)  # in-place soft downweight\n",
    "            apply_no_repeat_ngram_(logits, ids, n)           # in-place hard ban for repeated n-grams\n",
    "        - We clone the logits because we modify them in-place before argmax.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8601b7f7-b93f-4467-881d-34569e9b8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Greedy decoding (baseline, masks + repetition controls)\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def greedy_next(ids: torch.Tensor,\n",
    "                *,\n",
    "                ban_eos: bool = False,\n",
    "                ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                repetition_penalty: float = 1.2,\n",
    "                no_repeat_ngram_size: int = 4) -> int:\n",
    "    # 1) Get logits for the last position. Shape: [V].\n",
    "    #    Clone so we can safely mutate (mask/penalize) without affecting upstream tensors.\n",
    "    logits = last_step_logits(ids).clone()\n",
    "\n",
    "    # 2) Apply safety masks:\n",
    "    #    - ban_eos: optionally forbid EOS this step (logit = -inf)\n",
    "    #    - ban_ids: forbid whitespace-only or any custom disallowed tokens\n",
    "    mask_all_(logits, ban_eos=ban_eos, ban_ids=ban_ids)\n",
    "\n",
    "    # 3) Softly reduce the likelihood of tokens that already appeared in the context.\n",
    "    apply_repetition_penalty_(logits, ids, penalty=repetition_penalty)\n",
    "\n",
    "    # 4) Hard-prevent forming a repeated n-gram by banning the offending next tokens (logit = -inf).\n",
    "    apply_no_repeat_ngram_(logits, ids, n=no_repeat_ngram_size)\n",
    "\n",
    "    # 5) Greedy selection: pick the token with the highest (adjusted) logit.\n",
    "    return int(torch.argmax(logits).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dfe2475e-537f-486c-9165-bdf1735d244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def greedy_generate(prompt: str,\n",
    "                    max_new_tokens: int = 40,\n",
    "                    *,\n",
    "                    ban_eos_steps: int = 8,\n",
    "                    ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                    repetition_penalty: float = 1.2,\n",
    "                    no_repeat_ngram_size: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Greedy decoding with early EOS ban + repetition controls.\n",
    "    - For the first `ban_eos_steps`, EOS is forbidden to avoid immediate stop.\n",
    "    \"\"\"\n",
    " # 1) Encode the prompt to token IDs on the correct device.\n",
    "    ids = encode(prompt)\n",
    "\n",
    "    # 2) Iteratively append up to `max_new_tokens`.\n",
    "    for t in range(max_new_tokens):\n",
    "        # 2a) Pick next token greedily after applying masks + repetition controls.\n",
    "        nxt = greedy_next(ids,\n",
    "                          ban_eos=(t < ban_eos_steps),\n",
    "                          ban_ids=ban_ids,\n",
    "                          repetition_penalty=repetition_penalty,\n",
    "                          no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "\n",
    "        # 2b) If EOS is selected *after* the ban window, stop generation.\n",
    "        if EOS_ID is not None and nxt == EOS_ID and t >= ban_eos_steps:\n",
    "            break\n",
    "\n",
    "        # 2c) If EOS is selected *during* the ban window, skip it and continue.\n",
    "        if EOS_ID is not None and nxt == EOS_ID:\n",
    "            continue\n",
    "\n",
    "        # 2d) Otherwise, append the chosen token to the sequence.\n",
    "        ids = append_token(ids, nxt)\n",
    "\n",
    "    # 3) Decode token IDs back to text (handles NBSP normalization / skips specials).\n",
    "    return decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800428e7-cde1-4821-a4e5-56842e85acba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e64e92c-b69a-46bd-910c-d60140827001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Sampling building blocks (temperature + top-p nucleus)\n",
    "# ============================================================\n",
    "def softmax_with_temperature(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "    \"\"\"Apply temperature scaling + softmax.\"\"\"\n",
    "    t = max(float(temperature), 1e-6)  #Convert temperature to a float and clamp it to a minimum of 1e-6.\n",
    "    return torch.softmax(logits / t, dim=-1)\n",
    "    #Temperature scaling: scale the logits by 1/t and apply softmax, converting them into a probability distribution (sums to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ed2bc579-8842-447a-be61-c99a64b1ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_filter_indices(probs: torch.Tensor, top_p: Optional[float]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return indices inside the nucleus (top-p) set.\n",
    "    Always keep at least the top-1 token.\n",
    "    \"\"\"\n",
    "    V = probs.numel() # vocabulary size\n",
    "    \n",
    "    # No filtering: keep all indices [0..V-1]\n",
    "    if top_p is None or top_p >= 1:\n",
    "        return torch.arange(V, device=probs.device)\n",
    "    # Degenerate case: keep only the single most probable token\n",
    "    if top_p <= 0:\n",
    "        # Sort tokens by probability (descending)\n",
    "        sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "        return sorted_ix[:1]\n",
    "\n",
    "    # Compute cumulative sum to find the smallest prefix whose mass ≤ top_p \n",
    "    sorted_p, sorted_ix = torch.sort(probs, descending=True)\n",
    "    csum = torch.cumsum(sorted_p, dim=0)\n",
    "    \n",
    "    # Keep all tokens up to (and including) the point where cumprob ≤ top_p\n",
    "    keep = csum <= top_p\n",
    "    # Safety: always keep the top-1 token, even if top_p is very small\n",
    "    keep[0] = True\n",
    "    return sorted_ix[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4b7f8d2e-a566-42e9-99ec-6e13bdbad880",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample_next_token(ids: torch.Tensor,\n",
    "                      *,\n",
    "                      temperature: float = 0.9,\n",
    "                      top_p: float = 0.95,\n",
    "                      ban_eos: bool = False,\n",
    "                      ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                      repetition_penalty: float = 1.2,\n",
    "                      no_repeat_ngram_size: int = 4) -> int:\n",
    "    \"\"\"\n",
    "    Sample ONE next token after applying masks and repetition controls.\n",
    "\n",
    "    Args:\n",
    "        ids: Tensor of shape [1, T]. Current token sequence on the model's device.\n",
    "\n",
    "    Keyword Args:\n",
    "        temperature: Temperature for softmax smoothing. Lower = sharper; higher = flatter.\n",
    "        top_p: Nucleus (cumulative) probability threshold. Keep only the smallest prefix\n",
    "               of tokens whose cumulative prob ≤ top_p (always keep at least top-1).\n",
    "        ban_eos: If True, forbid EOS this step (prevents premature stopping).\n",
    "        ban_ids: Tensor of token IDs to always forbid (e.g., whitespace-only tokens). Can be None.\n",
    "        repetition_penalty: Soft penalty (>1.0) for tokens that already appeared in `ids`.\n",
    "        no_repeat_ngram_size: Hard ban on forming a repeated n-gram of this size.\n",
    "\n",
    "    Returns:\n",
    "        int: The sampled next token ID.\n",
    "    \"\"\"\n",
    "    # 1) Get last-step logits [V] and clone for in-place edits.\n",
    "    logits = last_step_logits(ids).clone()\n",
    "\n",
    "    # 2) Safety masks: EOS (optional) + custom banned tokens (e.g., NBSP/newline-only).\n",
    "    mask_all_(logits, ban_eos=ban_eos, ban_ids=ban_ids)\n",
    "\n",
    "    # 3) Repetition controls: soft downweight for seen tokens; hard n-gram ban.\n",
    "    apply_repetition_penalty_(logits, ids, penalty=repetition_penalty)\n",
    "    apply_no_repeat_ngram_(logits, ids, n=no_repeat_ngram_size)\n",
    "\n",
    "    # 4) Convert to probabilities with temperature scaling.\n",
    "    probs = softmax_with_temperature(logits, temperature)\n",
    "\n",
    "    # 5) Nucleus (top-p) filtering: keep only high-mass prefix; always keep top-1.\n",
    "    pool_ix = top_p_filter_indices(probs, top_p)\n",
    "    pool = probs[pool_ix]\n",
    "\n",
    "    # 6) Renormalize (guard against numerical underflow → fallback to top-1).\n",
    "    mass = float(pool.sum().item())\n",
    "    if mass <= 0.0:\n",
    "        # Degenerate case: fall back to the most probable token in the nucleus.\n",
    "        pick_local = 0\n",
    "    else:\n",
    "        pool = pool / mass\n",
    "        # 7) Stochastic choice from the filtered, renormalized distribution.\n",
    "        pick_local = int(torch.multinomial(pool, num_samples=1, replacement=False)[0].item())\n",
    "\n",
    "    # 8) Map local index back to the original vocab index and return.\n",
    "    return int(pool_ix[pick_local].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6cb90857-3ce7-4893-bcdd-f1e2596a5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7) Propose a short branch (span tokens) via sampling\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def propose_branch(ids: torch.Tensor,\n",
    "                   *,\n",
    "                   span: int = 3,\n",
    "                   temperature: float = 0.9,\n",
    "                   top_p: float = 0.95,\n",
    "                   ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                   ban_eos_first_n: int = 2,\n",
    "                   repetition_penalty: float = 1.2,\n",
    "                   no_repeat_ngram_size: int = 4) -> List[int]:\n",
    "    \"\"\"\n",
    "    Propose a short branch of length `span` by iterative sampling.\n",
    "    - Ban EOS for the first `ban_eos_first_n` samples to avoid immediate stop.\n",
    "    \"\"\"\n",
    "    cur = ids.clone()\n",
    "    branch: List[int] = []\n",
    "    for i in range(span):\n",
    "        ban = (i < ban_eos_first_n)\n",
    "        t = sample_next_token(cur,\n",
    "                              temperature=temperature, top_p=top_p,\n",
    "                              ban_eos=ban, ban_ids=ban_ids,\n",
    "                              repetition_penalty=repetition_penalty,\n",
    "                              no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "        branch.append(t)\n",
    "        cur = append_token(cur, t)\n",
    "    return branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba53e1f-9a0a-43ab-8162-ba738f97d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) Prefix-accept-once (compare sampled vs greedy)\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def prefix_accept_once(ids: torch.Tensor,\n",
    "                       branch: List[int],\n",
    "                       *,\n",
    "                       ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                       ban_eos_during_accept: bool = False,\n",
    "                       repetition_penalty: float = 1.2,\n",
    "                       no_repeat_ngram_size: int = 4) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Accept sampled tokens while they match the greedy prediction.\n",
    "    On first mismatch, accept greedy token and STOP.\n",
    "    Returns: (updated_ids, accepted_count)\n",
    "    \"\"\"\n",
    "    cur = ids.clone()\n",
    "    accepted = 0\n",
    "    for i, t in enumerate(branch):\n",
    "        g = greedy_next(cur,\n",
    "                        ban_eos=(ban_eos_during_accept and i == 0),\n",
    "                        ban_ids=ban_ids,\n",
    "                        repetition_penalty=repetition_penalty,\n",
    "                        no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "        if g == t:\n",
    "            cur = append_token(cur, t)\n",
    "            accepted += 1\n",
    "        else:\n",
    "            cur = append_token(cur, g)\n",
    "            break\n",
    "    return cur, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59fec6b2-b7c3-4037-be3d-58eb4edab222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9) Ultra-simple Medusa-like loop (one branch per step)\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def medusa_tiny(prompt: str,\n",
    "                *,\n",
    "                max_new_tokens: int = 30,\n",
    "                span: int = 3,\n",
    "                temperature: float = 0.9,\n",
    "                top_p: float = 0.95,\n",
    "                ban_eos_steps: int = 8,\n",
    "                ban_ids: Optional[torch.Tensor] = BAN_IDS,\n",
    "                repetition_penalty: float = 1.2,\n",
    "                no_repeat_ngram_size: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Minimal Medusa-like generation:\n",
    "      loop:\n",
    "        (a) propose a small branch of `span` tokens (sampling)\n",
    "        (b) prefix-accept against greedy (1st mismatch → greedy, stop)\n",
    "      stop when new tokens reach `max_new_tokens`\n",
    "    \"\"\"\n",
    "    ids = encode(prompt)\n",
    "    start_len = ids.shape[1]\n",
    "    steps = math.ceil(max_new_tokens / max(1, span))\n",
    "    remaining_ban = max(0, ban_eos_steps)\n",
    "\n",
    "    for step in range(steps):\n",
    "        branch = propose_branch(ids,\n",
    "                                span=span, temperature=temperature, top_p=top_p,\n",
    "                                ban_ids=ban_ids, ban_eos_first_n=min(span, remaining_ban),\n",
    "                                repetition_penalty=repetition_penalty,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "        ids, _ = prefix_accept_once(ids, branch,\n",
    "                                    ban_ids=ban_ids,\n",
    "                                    ban_eos_during_accept=(step == 0 and remaining_ban > 0),\n",
    "                                    repetition_penalty=repetition_penalty,\n",
    "                                    no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "        if ids.shape[1] - start_len >= max_new_tokens:\n",
    "            break\n",
    "        remaining_ban = max(0, remaining_ban - span)\n",
    "\n",
    "    return decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c1e0234-a131-47aa-837f-e28afc3e2c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Greedy (baseline) ===\n",
      "PROMPT: 'In a distant future, '\n",
      "'In a distant future, vernacular is the most common language in English. It has been used for centuries to describe many of our everyday lives and we are often referred to as'\n",
      "------------------------------------------------------------\n",
      "PROMPT: 'Long ago, '\n",
      "'Long ago, iced tea was a popular drink in the United States. It is now widely used as an alternative to traditional Chinese medicine and has been shown to be effective'\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Medusa-tiny (span=3) ===\n",
      "PROMPT: 'In a distant future, '\n",
      "'In a distant future, vernacular is the most common language in English. It has been used for centuries'\n",
      "------------------------------------------------------------\n",
      "PROMPT: 'Long ago, '\n",
      "'Long ago, iced tea was a popular drink in the United States. It is'\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10) Quick smoke tests (repr shows spaces clearly)\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    prompts = [\n",
    "        \"In a distant future, \",\n",
    "        \"Long ago, \",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n=== Greedy (baseline) ===\")\n",
    "    for p in prompts:\n",
    "        print(\"PROMPT:\", repr(p))\n",
    "        print(repr(greedy_generate(p,\n",
    "                                   max_new_tokens=30,\n",
    "                                   ban_eos_steps=8,\n",
    "                                   repetition_penalty=1.2,\n",
    "                                   no_repeat_ngram_size=4)))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(\"\\n=== Medusa-tiny (span=3) ===\")\n",
    "    for p in prompts:\n",
    "        print(\"PROMPT:\", repr(p))\n",
    "        print(repr(medusa_tiny(p,\n",
    "                               max_new_tokens=30,\n",
    "                               span=3,\n",
    "                               temperature=0.9,\n",
    "                               top_p=0.95,\n",
    "                               ban_eos_steps=8,\n",
    "                               repetition_penalty=1.2,\n",
    "                               no_repeat_ngram_size=4)))\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b2c8e-4472-4cfe-82d1-e41c4cb94de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
