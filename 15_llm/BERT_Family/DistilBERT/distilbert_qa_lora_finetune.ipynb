{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bd26d60a-7ca2-4335-a183-2dabc2348705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (4.53.3)\n",
      "Requirement already satisfied: filelock in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a6d424b5-4a38-434a-9c76-977c01e46b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.53.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, sentence-transformers, trl\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "40319342-a8cc-4699-bffb-c9944a506645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.training_args\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "19f47e9b-0854-4ca4-a2f9-225d668a941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data use SQuAD v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "98956601-1297-4c9d-b1fc-ae7ec155b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "796bfa89-d7b5-4ea0-8cea-8f92fe6c7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing (prepare_features_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "96b55477-0c79-43f7-a6d7-987b7b90c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c867b7ee-fa97-4009-86c4-5b190f6e1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inputs(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the input examples for question answering.\n",
    "    \n",
    "    Args:\n",
    "        examples (dict): A batch of examples containing 'question' and 'context' keys.\n",
    "        \n",
    "    Returns:\n",
    "        tokenized_examples (BatchEncoding): Tokenized inputs with overflow handling.\n",
    "        sample_mapping (list[int]): Maps tokenized chunks back to original examples.\n",
    "        offset_mapping (list[list[tuple[int, int]]]): Character start/end positions for each token.\n",
    "    \"\"\"\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",      # Truncate only the context (second sequence=context)\n",
    "        max_length=384,                # Max sequence length\n",
    "        stride=128,                   # Overlap tokens for long contexts\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,  # Provide character offsets for tokens\n",
    "        padding=\"max_length\"          # Pad to max_length\n",
    "    )\n",
    "    \n",
    "    # Extract auxiliary info for mapping tokenized chunks back to original examples\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    \n",
    "    return tokenized_examples, sample_mapping, offset_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "aa1e0af4-e42a-4f05-9911-9ce8576014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ac82c052-2292-4ee0-93e7-742017351f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    \"id\": [\"1\"],\n",
    "    \"question\": [\"When did Beyoncé become popular?\"],\n",
    "    \"context\": [\"Beyoncé Giselle Knowles-Carter was born in 1981 and became famous in the late 1990s as part of Destiny's Child.\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3d2174ef-db59-492a-9647-68046a1f6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized, sample_mapping, offset_mapping = tokenize_inputs(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5f6f9e8c-baf0-483e-bd56-d12a0dea24a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      "[101, 2043, 2106, 20773, 2468, 2759, 1029, 102, 20773, 21025]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(\"Input IDs:\")\n",
    "pprint(tokenized[\"input_ids\"][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "21536997-ab93-4f96-aef0-2480f5f77e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Mapping:\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample Mapping:\")\n",
    "pprint(sample_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "85136871-457d-4ba9-aa97-2a5d489ff771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Offset Mapping:\n",
      "[(0, 0),\n",
      " (0, 4),\n",
      " (5, 8),\n",
      " (9, 16),\n",
      " (17, 23),\n",
      " (24, 31),\n",
      " (31, 32),\n",
      " (0, 0),\n",
      " (0, 7),\n",
      " (8, 10)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOffset Mapping:\")\n",
    "pprint(offset_mapping[0][:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "59a47452-1f2d-4423-9f7d-f3c6f55d22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded Tokens:\n",
      "[CLS] when did beyonce become popular? [SEP] beyonce giselle knowles - carter was born in 1981 and became famous in the late 1990s as part of destiny ' s child. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDecoded Tokens:\")\n",
    "print(tokenizer.decode(tokenized[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4d136f68-a891-48d5-8fc0-d3705bd188f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def prepare_features_with_labels(examples) identifies the start and positions of the answer within the tokenized inuts and assigns them as labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "e726261e-834e-41af-8920-746efce41064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_with_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the input examples and computes the start and end token positions \n",
    "    of the answer within the context, suitable for training a QA model.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A batch of examples containing 'question', 'context', and 'answers'.\n",
    "\n",
    "    Returns:\n",
    "        tokenized_examples (dict): Tokenized input with added 'start_positions' and 'end_positions' for training.\n",
    "    \"\"\"\n",
    "    # Tokenize examples using sliding window and truncating only the context (second sequence)\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Extract mappings to link back to the original examples\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    #offsets : information for the current tokenized feature\n",
    "    #Each token is mapped to its corresponding character span in the original context\n",
    "    #i = the index of the current tokenized feature\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i] #The list of token IDs for the current tokenized feature.\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id) #The position of the [CLS] token\n",
    "\n",
    "        # sequence_ids: 0 = question, 1 = context, None = special tokens (CLS, SEP, etc.)\n",
    "        \"\"\"\n",
    "        sequence_ids is a list that indicates which part of the input each token comes from.\n",
    "        0 means the token belongs to the question\n",
    "        1 means the token belongs to the context\n",
    "        None means the token is a special token (e.g., [CLS], [SEP], [PAD]).\n",
    "        \"\"\"\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:  \n",
    "        # When the question has no answers (unanswerable question)\n",
    "        # Since there is no answer, set the start and end positions to the [CLS] token index\n",
    "        # This helps the model learn to predict \"no answer\" by pointing to the [CLS] token\n",
    "            start_positions.append(cls_index)  # Start position set to CLS token index\n",
    "            end_positions.append(cls_index)    # End position also set to CLS token index\n",
    "        else:\n",
    "        # When there is an answer, get the character start index of the answer text in the context\n",
    "            start_char = answers[\"answer_start\"][0]             # Character index where answer starts\n",
    "            end_char = start_char + len(answers[\"text\"][0])     # Character index where answer ends (start + length of answer)\n",
    "\n",
    "            # Find the start of the context tokens\n",
    "            token_start_index = 0\n",
    "            # Iterate until we find the first token that belongs to the context\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # Find the end of the context tokens\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                # Move to the next token index\n",
    "                token_end_index -= 1\n",
    "                # After the loop, token_start_index points to the first token of the context sequence\n",
    "\n",
    "            # If the answer is outside the span of context tokens, set positions to CLS\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                # Find the start token index corresponding to the answer\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "\n",
    "                # Find the end token index corresponding to the answer\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "\n",
    "    # Attach start and end positions to tokenized examples\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f89a433d-f8cc-4eb0-ac83-6c3eb6e544cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0:\n",
      "  [CLS]: None\n",
      "  when: 0\n",
      "  did: 0\n",
      "  beyonce: 0\n",
      "  start: 0\n",
      "  becoming: 0\n",
      "  popular: 0\n",
      "  ?: 0\n",
      "  [SEP]: None\n",
      "  beyonce: 1\n",
      "  gi: 1\n",
      "  ##selle: 1\n",
      "  knowles: 1\n",
      "  -: 1\n",
      "  carter: 1\n",
      "  (: 1\n",
      "  /: 1\n",
      "  bi: 1\n",
      "  ##ː: 1\n",
      "  ##ˈ: 1\n",
      "  ##j: 1\n",
      "  ##ɒ: 1\n",
      "  ##nse: 1\n",
      "  ##ɪ: 1\n",
      "  /: 1\n",
      "  bee: 1\n",
      "  -: 1\n",
      "  yo: 1\n",
      "  ##n: 1\n",
      "  -: 1\n",
      "  say: 1\n",
      "  ): 1\n",
      "  (: 1\n",
      "  born: 1\n",
      "  september: 1\n",
      "  4: 1\n",
      "  ,: 1\n",
      "  1981: 1\n",
      "  ): 1\n",
      "  is: 1\n",
      "  an: 1\n",
      "  american: 1\n",
      "  singer: 1\n",
      "  ,: 1\n",
      "  songwriter: 1\n",
      "  ,: 1\n",
      "  record: 1\n",
      "  producer: 1\n",
      "  and: 1\n",
      "  actress: 1\n",
      "  .: 1\n",
      "  born: 1\n",
      "  and: 1\n",
      "  raised: 1\n",
      "  in: 1\n",
      "  houston: 1\n",
      "  ,: 1\n",
      "  texas: 1\n",
      "  ,: 1\n",
      "  she: 1\n",
      "  performed: 1\n",
      "  in: 1\n",
      "  various: 1\n",
      "  singing: 1\n",
      "  and: 1\n",
      "  dancing: 1\n",
      "  competitions: 1\n",
      "  as: 1\n",
      "  a: 1\n",
      "  child: 1\n",
      "  ,: 1\n",
      "  and: 1\n",
      "  rose: 1\n",
      "  to: 1\n",
      "  fame: 1\n",
      "  in: 1\n",
      "  the: 1\n",
      "  late: 1\n",
      "  1990s: 1\n",
      "  as: 1\n",
      "  lead: 1\n",
      "  singer: 1\n",
      "  of: 1\n",
      "  r: 1\n",
      "  &: 1\n",
      "  b: 1\n",
      "  girl: 1\n",
      "  -: 1\n",
      "  group: 1\n",
      "  destiny: 1\n",
      "  ': 1\n",
      "  s: 1\n",
      "  child: 1\n",
      "  .: 1\n",
      "  managed: 1\n",
      "  by: 1\n",
      "  her: 1\n",
      "  father: 1\n",
      "  ,: 1\n",
      "  mathew: 1\n",
      "  knowles: 1\n",
      "  ,: 1\n",
      "  the: 1\n",
      "  group: 1\n",
      "  became: 1\n",
      "  one: 1\n",
      "  of: 1\n",
      "  the: 1\n",
      "  world: 1\n",
      "  ': 1\n",
      "  s: 1\n",
      "  best: 1\n",
      "  -: 1\n",
      "  selling: 1\n",
      "  girl: 1\n",
      "  groups: 1\n",
      "  of: 1\n",
      "  all: 1\n",
      "  time: 1\n",
      "  .: 1\n",
      "  their: 1\n",
      "  hiatus: 1\n",
      "  saw: 1\n",
      "  the: 1\n",
      "  release: 1\n",
      "  of: 1\n",
      "  beyonce: 1\n",
      "  ': 1\n",
      "  s: 1\n",
      "  debut: 1\n",
      "  album: 1\n",
      "  ,: 1\n",
      "  dangerously: 1\n",
      "  in: 1\n",
      "  love: 1\n",
      "  (: 1\n",
      "  2003: 1\n",
      "  ): 1\n",
      "  ,: 1\n",
      "  which: 1\n",
      "  established: 1\n",
      "  her: 1\n",
      "  as: 1\n",
      "  a: 1\n",
      "  solo: 1\n",
      "  artist: 1\n",
      "  worldwide: 1\n",
      "  ,: 1\n",
      "  earned: 1\n",
      "  five: 1\n",
      "  grammy: 1\n",
      "  awards: 1\n",
      "  and: 1\n",
      "  featured: 1\n",
      "  the: 1\n",
      "  billboard: 1\n",
      "  hot: 1\n",
      "  100: 1\n",
      "  number: 1\n",
      "  -: 1\n",
      "  one: 1\n",
      "  singles: 1\n",
      "  \": 1\n",
      "  crazy: 1\n",
      "  in: 1\n",
      "  love: 1\n",
      "  \": 1\n",
      "  and: 1\n",
      "  \": 1\n",
      "  baby: 1\n",
      "  boy: 1\n",
      "  \": 1\n",
      "  .: 1\n",
      "  [SEP]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "  [PAD]: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_examples[\"input_ids\"][i])\n",
    "    seq_ids = tokenized_examples.sequence_ids(i)\n",
    "    print(f\"Feature {i}:\")\n",
    "    for token, seq_id in zip(tokens, seq_ids):\n",
    "        print(f\"  {token}: {seq_id}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f9473609-b1f2-4ff2-8a2d-4b4668d13682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset = raw_dataset[\"train\"].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "81b0c395-542a-4947-8736-75e4d7456364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "56f7b904-578c-423c-b672-9775ba683eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples, sample_mapping, offset_mapping = tokenize_inputs(subset)\n",
    "processed_data = add_token_labels(tokenized_examples, sample_mapping, offset_mapping, subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d56f2392-6488-4a00-b1d9-6bad8efccae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids (first sample): [101, 2043, 2106, 20773, 2707, 3352, 2759, 1029, 102, 20773, 21025, 19358, 22815, 1011, 5708, 1006, 1013, 12170, 23432, 29715, 3501, 29678, 12325, 29685, 1013, 10506, 1011, 10930, 2078, 1011, 2360, 1007, 1006, 2141, 2244, 1018, 1010, 3261, 1007, 2003, 2019, 2137, 3220, 1010, 6009, 1010, 2501, 3135, 1998, 3883, 1012, 2141, 1998, 2992, 1999, 5395, 1010, 3146, 1010, 2016, 2864, 1999, 2536, 4823, 1998, 5613, 6479, 2004, 1037, 2775, 1010, 1998, 3123, 2000, 4476, 1999, 1996, 2397, 4134, 2004, 2599, 3220, 1997, 1054, 1004, 1038, 2611, 1011, 2177, 10461, 1005, 1055, 2775, 1012, 3266, 2011, 2014, 2269, 1010, 25436, 22815, 1010, 1996, 2177, 2150, 2028, 1997, 1996, 2088, 1005, 1055, 2190, 1011, 4855, 2611, 2967, 1997, 2035, 2051, 1012, 2037, 14221, 2387, 1996, 2713, 1997, 20773, 1005, 1055, 2834, 2201, 1010, 20754, 1999, 2293, 1006, 2494, 1007, 1010, 2029, 2511, 2014, 2004, 1037, 3948, 3063, 4969, 1010, 3687, 2274, 8922, 2982, 1998, 2956, 1996, 4908, 2980, 2531, 2193, 1011, 2028, 3895, 1000, 4689, 1999, 2293, 1000, 1998, 1000, 3336, 2879, 1000, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start_position: 75\n",
      "end_position: 78\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids (first sample):\", processed_data[\"input_ids\"][0])\n",
    "print(\"start_position:\", processed_data[\"start_positions\"][0])\n",
    "print(\"end_position:\", processed_data[\"end_positions\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c4d81f9c-bfbb-44aa-abc2-0fb0a9cc2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8904602d-304d-483e-b0bc-0c69fa381447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee739f58b0b418e9f17b48dd4f97de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acb0a6c141940baa273cc446bf03c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = raw_dataset.map(\n",
    "    function=prepare_features_with_labels,  # Apply the function that tokenizes inputs and sets start/end labels\n",
    "    batched=True,                          # Process the dataset in batches for efficiency\n",
    "    remove_columns=raw_dataset[\"train\"].column_names  # Remove original columns after processing to keep only tokenized features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "887e7bc6-870c-44c4-86bc-60e9e5e30a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3d827e26-2ea7-4545-a4f7-f637dbfcc0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# 1. Set the model name or identifier\n",
    "model_id = \"distilbert-base-uncased\"\n",
    "\n",
    "# 2. Load the tokenizer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 3. Load the pre-trained Question Answering model\n",
    "base_model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e072acf5-0c37-42c5-8a43-00fba27ff769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "36d841f7-9fd2-46c9-b5ad-ae37d1ecc6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# 1. Load the pre-trained DistilBERT model for Question Answering\n",
    "base_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# 2. Configure LoRA (Low-Rank Adaptation)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # Modules in DistilBERT where LoRA will be applied\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"QUESTION_ANS\"     # Task type set as a string for version compatibility\n",
    ")\n",
    "\n",
    "# 3. Create a PEFT model by applying LoRA to the base model\n",
    "model = get_peft_model(base_model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "4ba25a21-8ead-4416-a19b-9d22105fce11",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4012217190.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[274]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(list(TaskType))from transformers import TrainingArguments\u001b[39m\n                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from peft import TaskType\n",
    "print(list(TaskType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ef472576-eac0-4bd2-8472-59852c4e412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "43bc261e-496f-46ba-9a47-a2dbc9cfca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_qa\",                # Directory to save model checkpoints and outputs\n",
    "    per_device_train_batch_size=8,         # Batch size for training on each device (GPU/CPU)\n",
    "    per_device_eval_batch_size=8,          # Batch size for evaluation on each device\n",
    "    num_train_epochs=2,                    # Number of training epochs\n",
    "    logging_steps=50,                      # Log training info every 50 steps\n",
    "    do_eval=True,                         # Enable evaluation during training\n",
    "    eval_steps=500,                       # Evaluate the model every 500 steps\n",
    "    save_steps=500,                       # Save a checkpoint every 500 steps\n",
    "    save_total_limit=2,                   # Maximum number of checkpoints to keep\n",
    "    remove_unused_columns=False           # Keep all columns from the dataset (avoid removing unused columns)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b06653c0-6667-4f91-a295-fd52e76ec60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148,994 || all params: 66,513,412 || trainable%: 0.2240\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6f158b86-bd67-49fc-b4c7-bfbd0c46cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "46820ebf-d5a0-4e8d-b4e9-ebacba5c57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/xtl4b0cx1cs9zrr9n5y814_h0000gn/T/ipykernel_40390/1363889537.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.500200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=5.69317431640625, metrics={'train_runtime': 66.3123, 'train_samples_per_second': 30.16, 'train_steps_per_second': 3.77, 'total_flos': 196666214400000.0, 'train_loss': 5.69317431640625, 'epoch': 2.0})"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "\n",
    "# Trainer 객체 정의\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].select(range(1000)),\n",
    "    eval_dataset=tokenized_dataset[\"validation\"].select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b68c0f35-c79d-4e12-a1ff-6293516872da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2043, 2106, 20773, 2707, 3352, 2759, 1029, 102, 20773, 21025, 19358, 22815, 1011, 5708, 1006, 1013, 12170, 23432, 29715, 3501, 29678, 12325, 29685, 1013, 10506, 1011, 10930, 2078, 1011, 2360, 1007, 1006, 2141, 2244, 1018, 1010, 3261, 1007, 2003, 2019, 2137, 3220, 1010, 6009, 1010, 2501, 3135, 1998, 3883, 1012, 2141, 1998, 2992, 1999, 5395, 1010, 3146, 1010, 2016, 2864, 1999, 2536, 4823, 1998, 5613, 6479, 2004, 1037, 2775, 1010, 1998, 3123, 2000, 4476, 1999, 1996, 2397, 4134, 2004, 2599, 3220, 1997, 1054, 1004, 1038, 2611, 1011, 2177, 10461, 1005, 1055, 2775, 1012, 3266, 2011, 2014, 2269, 1010, 25436, 22815, 1010, 1996, 2177, 2150, 2028, 1997, 1996, 2088, 1005, 1055, 2190, 1011, 4855, 2611, 2967, 1997, 2035, 2051, 1012, 2037, 14221, 2387, 1996, 2713, 1997, 20773, 1005, 1055, 2834, 2201, 1010, 20754, 1999, 2293, 1006, 2494, 1007, 1010, 2029, 2511, 2014, 2004, 1037, 3948, 3063, 4969, 1010, 3687, 2274, 8922, 2982, 1998, 2956, 1996, 4908, 2980, 2531, 2193, 1011, 2028, 3895, 1000, 4689, 1999, 2293, 1000, 1998, 1000, 3336, 2879, 1000, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'start_positions': 75, 'end_positions': 78}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6518dff6-d2c8-482f-ba7c-d7040a801153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8f586710-db2a-47c5-9d98-68b2bc71b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.581022262573242, 'eval_runtime': 7.7393, 'eval_samples_per_second': 64.605, 'eval_steps_per_second': 8.14, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "eb380cf1-d8cc-41d8-ab05-73386db0abc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 예측값 리스트, 정답 리스트 (둘 다 텍스트)\n",
    "predictions = [\"in the late 1990s\", \"another answer\", \"some answer\"]\n",
    "references = [\"in the late 1990s\", \"correct answer\", \"some answer\"]\n",
    "\n",
    "# 정확도 계산\n",
    "acc = accuracy_score(references, predictions)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "cff6fe98-587c-4892-92a8-5038c0fcfac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.6667\n",
      "F1 Score: 0.8333\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"텍스트 정규화: 소문자, 구두점 제거, 공백 정리 등\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_text(a_gold) == normalize_text(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = normalize_text(a_gold).split()\n",
    "    pred_toks = normalize_text(a_pred).split()\n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_toks)\n",
    "    recall = num_same / len(gold_toks)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# 예시 리스트\n",
    "predictions = [\"in the late 1990s\", \"another answer\", \"some answer\"]\n",
    "references = [\"in the late 1990s\", \"correct answer\", \"some answer\"]\n",
    "\n",
    "em_scores = [compute_exact(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "f1_scores = [compute_f1(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "\n",
    "print(f\"Exact Match: {sum(em_scores)/len(em_scores):.4f}\")\n",
    "print(f\"F1 Score: {sum(f1_scores)/len(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512e199-383a-4499-a02a-d587bd99b19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pyenv)",
   "language": "python",
   "name": "pyenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
