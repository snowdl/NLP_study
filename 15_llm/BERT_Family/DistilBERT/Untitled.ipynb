{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4702988f-7b7d-4f5f-b840-c17ae4a83d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (4.53.3)\n",
      "Requirement already satisfied: datasets in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: peft in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.15.2)\n",
      "Requirement already satisfied: accelerate in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.14)\n",
      "Requirement already satisfied: psutil in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a250ab95-911c-4e72-83c0-477e6864c386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProject goal :To fine-tune a lightweight transformer model (distilbert-base-uncased) for extractive question answering using the LoRA (Low-Rank Adaptation) technique, with the aim of optimizing efficiency and reducing computational overhead during model adaptation.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Project goal :To fine-tune a lightweight transformer model (distilbert-base-uncased) for extractive question answering using the LoRA (Low-Rank Adaptation) technique, with the aim of optimizing efficiency and reducing computational overhead during model adaptation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63f71f1-8544-4127-9ac8-f4e308aac278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key  objective\n",
    "#Apply LoRA to enable parameter-efficient fine-tuning.\n",
    "#Train a lightweight QA model with minimal resources.\n",
    "#Maintain performance while reducing memory and storage requirements.\n",
    "#Maintain performance while reducing memory and storage requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ee2c49-298c-4c9b-b13a-6c350ad2f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading (SQuAD v1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af203c88-5582-4e75-98e4-65202e5c54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import required libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, default_data_collator\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a4b3d2-c49d-49e4-a0d3-1e0d91bcf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load SQuAD dataset (question answering dataset\n",
    "#Standford Question Answering Dataset\n",
    "raw_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753eec59-66fe-4b1f-8a49-fd8acdf17298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e616be5-b4fa-4937-be4e-2eead9d299f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3. Load tokenizer and pre-trained DistilBERT QA model\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "model_id = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14afd80a-03c4-4fc1-83e1-0a59574382f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lora configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf527ed2-4200-44e3-bc74-ef2ee2fa63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # LoRA rank\n",
    "    lora_alpha=16,                # LoRA scaling factor\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # Target modules in DistilBERT to apply LoRA\n",
    "    lora_dropout=0.1,             # Dropout probability for LoRA layers\n",
    "    task_type=\"QUESTION_ANS\"      # Task type: Question Answering\n",
    ")  # Close the LoraConfig constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6b71a6-0008-41b9-980e-c996fccd11a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# Wrap the pre-trained model with LoRA configuration\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e61b77f3-aef7-45a1-b267-bcce79af0d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTokenize the question and context to create input data,\\nGenerate labels (start_positions, end_positions) that indicate the token indices where the answer text appears within the context.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare_features_with_labels\n",
    "\"\"\"\n",
    "Tokenize the question and context to create input data,\n",
    "Generate labels (start_positions, end_positions) that indicate the token indices where the answer text appears within the context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d6458e-2680-4383-ba3d-cd729c6846f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCreate two functions \\n   1) tokenize_examples(examples) : Tokenizes the question and context\\n    and returns the tokenized outputs along with:\\n        -sample_mapping: mapping from each tokenized chunk back to the original example\\n        -offset_mapping: character-level positions of each token in the original context\\n\\n    2)add_answer_positions(...)\\n    Converts the answer's character-level start and end positions\\n    into token-level start and end indices,\\n    and adds them to the tokenized data as start_positions and end_positions.\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization and mapping\n",
    "\"\"\"\n",
    "Create two functions \n",
    "   1) tokenize_examples(examples) : Tokenizes the question and context\n",
    "    and returns the tokenized outputs along with:\n",
    "        -sample_mapping: mapping from each tokenized chunk back to the original example\n",
    "        -offset_mapping: character-level positions of each token in the original context\n",
    "\n",
    "    2)add_answer_positions(...)\n",
    "    Converts the answer's character-level start and end positions\n",
    "    into token-level start and end indices,\n",
    "    and adds them to the tokenized data as start_positions and end_positions.\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c1fdb7c-bd51-4d37-9867-e8a263943ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_examples(examples):\n",
    "    # Tokenize the \"question\" and \"context\" fields from the examples\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],         # List of questions\n",
    "        examples[\"context\"],          # Corresponding list of contexts\n",
    "        truncation=\"only_second\",     # Only truncate the context if it's too long\n",
    "        max_length=384,               # Limit total token length to 384\n",
    "        stride=128,                   # Overlap between chunks to avoid cutting off answers\n",
    "        return_overflowing_tokens=True,  # Return multiple chunks if context is too long\n",
    "        return_offsets_mapping=True,     # Return mapping of tokens to original character positions\n",
    "        padding=\"max_length\"             # Pad sequences to max length\n",
    "    )\n",
    "\n",
    "    # Extract the mapping from each tokenized chunk to its original example index\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # Extract the offset mapping (token-to-character span info)\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Return the tokenized input plus sample/offset mapping for later label alignment\n",
    "    return tokenized_examples, sample_mapping, offset_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5058bb72-6a24-4922-8e0a-f205db5ac582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_answer_positions(tokenized_examples, sample_mapping, offset_mapping, examples):\n",
    "    # Initialize lists to store start and end token indices for each example\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Iterate through each tokenized chunk\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]  # Get token IDs for this chunk\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)  # Get the index of the [CLS] token\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)  # Sequence IDs: 0=question, 1=context\n",
    "        sample_index = sample_mapping[i]  # Map this chunk back to the original example\n",
    "        answers = examples[\"answers\"][sample_index]  # Get the answer(s) for this example\n",
    "\n",
    "        # If there is no answer (impossible question)\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)  # Use CLS index as placeholder\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            # Get character-level start/end of the first answer\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Find the first token index that belongs to the context\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # Find the last token index that belongs to the context\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Check if the answer is fully within the current chunk\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                # If not fully contained, use CLS token as default\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                # Narrow down the token span to exact start position\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "\n",
    "                # Narrow down the token span to exact end position\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "\n",
    "    # Add the computed positions to the tokenized examples\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "\n",
    "    # Return tokenized examples with labels attached\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd10616-f9aa-4b34-b7fd-0dd8d32db971",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m tokenized_examples, sample_mapping, offset_mapping = tokenize_examples(raw_dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2. Add start/end position labels to the tokenized examples\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m processed_data = \u001b[43madd_answer_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36madd_answer_positions\u001b[39m\u001b[34m(tokenized_examples, sample_mapping, offset_mapping, examples)\u001b[39m\n\u001b[32m     11\u001b[39m sequence_ids = tokenized_examples.sequence_ids(i)  \u001b[38;5;66;03m# Sequence IDs: 0=question, 1=context\u001b[39;00m\n\u001b[32m     12\u001b[39m sample_index = sample_mapping[i]  \u001b[38;5;66;03m# Map this chunk back to the original example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m answers = \u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manswers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[sample_index]  \u001b[38;5;66;03m# Get the answer(s) for this example\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# If there is no answer (impossible question)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(answers[\u001b[33m\"\u001b[39m\u001b[33manswer_start\u001b[39m\u001b[33m\"\u001b[39m]) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:2762\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2761\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/datasets/formatting/formatting.py:653\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    651\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/datasets/formatting/formatting.py:408\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_row(pa_table)\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_batch(pa_table)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/datasets/formatting/formatting.py:459\u001b[39m, in \u001b[36mPythonFormatter.format_column\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     column = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     column = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_column(column, pa_table.column_names[\u001b[32m0\u001b[39m])\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/datasets/formatting/formatting.py:146\u001b[39m, in \u001b[36mPythonArrowExtractor.extract_column\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pyarrow/table.pxi:1366\u001b[39m, in \u001b[36mpyarrow.lib.ChunkedArray.to_pylist\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pyarrow/array.pxi:1663\u001b[39m, in \u001b[36mpyarrow.lib.Array.to_pylist\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pyarrow/scalar.pxi:834\u001b[39m, in \u001b[36mpyarrow.lib.StructScalar.as_py\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/_collections_abc.py:786\u001b[39m, in \u001b[36mMapping.keys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mD.keys() -> a set-like object providing a view on D\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms keys\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m KeysView(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Tokenize the training dataset and get mappings\n",
    "tokenized_examples, sample_mapping, offset_mapping = tokenize_examples(raw_dataset[\"train\"])\n",
    "\n",
    "# 2. Add start/end position labels to the tokenized examples\n",
    "processed_data = add_answer_positions(tokenized_examples, sample_mapping, offset_mapping, raw_dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814aecc7-9947-4801-8801-7b6f5a7fa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation data\n",
    "val_tokenized, val_sample_map, val_offset_map = tokenize_examples(raw_dataset[\"validation\"])\n",
    "val_processed = add_answer_positions(val_tokenized, val_sample_map, val_offset_map, raw_dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa2607-0887-416e-b56c-725cc23f78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba57363-ba97-4ab2-82b3-f0d7e8d9b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the entire dataset\n",
    "# - Tokenizes questions and contexts with proper truncation and padding\n",
    "# - Maps character-level answer positions to token-level start and end positions\n",
    "# - Handles overlapping chunks for long contexts\n",
    "# - Removes original columns to keep only processed features for training\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_features_with_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6a0fd-f427-4155-8152-4e9c83c370e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Set training arguments such as output directory, batch sizes, epochs, and logging details\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_qa\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ab367-63e1-4694-ab35-e3f7b8fda861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"].select(range(1000)),\n",
    "    eval_dataset=tokenized[\"validation\"].select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()  # 학습 시작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ddee0-e755-4a35-8086-b8f295dad39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca08575-3af2-4d1c-aaf3-43ecb2a33bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./lora_qa_model\")\n",
    "tokenizer.save_pretrained(\"./lora_qa_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a97187-9c59-43fa-9dd8-1b9085a7b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b2835-5608-4221-a4bb-0c0d88b796d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9558e-e39c-4a48-80f8-203c90cb68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction/Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d13050-229c-4877-81c5-993f526cb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "result = qa_pipeline(question=\"What is LoRA?\", context=\"LoRA is ...\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64431f20-96f1-463e-aedc-ea549a448169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pyenv)",
   "language": "python",
   "name": "pyenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
