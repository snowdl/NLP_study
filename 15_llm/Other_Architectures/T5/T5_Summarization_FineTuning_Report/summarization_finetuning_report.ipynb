{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a24512-f0c9-419b-8d0b-72cbc428d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T5로 뉴스 기사나 문서를 요약하는 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef4db66a-870c-4174-a497-1120768e3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47152688-1545-4512-b47b-44a3a88e03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31981118-00f9-43f2-bc11-af8b93acefe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CNN/ DailyMail news summerization data load\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:5]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f688ac4f-511e-4996-9cd5-f7a767437877",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cafb6ba2-42c8-4483-a0e1-ec4a2817e02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Structure and Contents:\n",
      "\n",
      "🔑 article:\n",
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the cou...\n",
      "\n",
      "🔑 highlights:\n",
      "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis ....\n",
      "\n",
      "🔑 id:\n",
      "f001ec5c4704938247d27a44948eebb37ae98d01...\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Structure and Contents:\")\n",
    "for key in sample:\n",
    "    print(f\"\\n🔑 {key}:\\n{sample[key][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c4aed4-21cb-4ab8-ad83-cd539081ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10840b24-4220-4615-986c-e2bff12d1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec8e0f1-b4cc-45a6-aa89-d3361c5749d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b8a5e3-2e62-404b-9f3d-1dd8715a8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb847975-8ca9-4410-ae88-10ab9d3b8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def summarize(text) :  function takes a news article (long text) as the text parameter and returns a string containing the summarized version of that tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb91035-820e-4215-a71c-514dbd9644cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=150,\n",
    "        min_length=30,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b27f6085-a80c-411c-b55f-2c2a2ce19526",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"The stock market fell by more than 500 points today amid economic uncertainty.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aada3466-a919-468b-bf5c-9b33625a4397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: the stock market fell by more than 500 points today amid economic uncertainty. the stock market fell by more than 500 points amid economic uncertainty.\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary:\", summarize(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f8fa4bd-13e0-4ff6-bf49-98053f9fe82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = dataset[0][\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a58cd2-5470-44df-8ece-b8f24012cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: the palestinians signed the ICC's founding Rome Statute in January. the ICC also accepted its jurisdiction over alleged crimes committed in the occupied Palestinian territory. the ICC opened a preliminary examination into the situation in the occupied territories.\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary:\", summarize(article_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "041e4c73-60d0-4771-9400-3203a6addcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article #1 Summary:\n",
      "the palestinians signed the ICC's founding Rome Statute in January. the ICC also accepted its jurisdiction over alleged crimes committed in the occupied Palestinian territory. the ICC opened a preliminary examination into the situation in the occupied territories.\n",
      "Reference Summary:\n",
      "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "==================================================\n",
      "\n",
      "Article #2 Summary:\n",
      "theia is a friendly white-and-black bully breed mix now named theia. she was found by a worker who took her to a vet for help. the dog's brush with death did not leave her unscathed.\n",
      "Reference Summary:\n",
      "Theia, a bully breed mix, was apparently hit by a car, whacked with a hammer and buried in a field .\n",
      "\"She's a true miracle dog and she deserves a good life,\" says Sara Mellado, who is looking for a home for Theia .\n",
      "==================================================\n",
      "\n",
      "Article #3 Summary:\n",
      "\"Long live Zarif,\" crowds chanted as his car rolled slowly down the packed street. \"the new year would be even sweeter if you would end Iran's Holocaust denial,\" she says. \"the man who was perceived to be denying it is now gone. Happy New Year,\" she says.\n",
      "Reference Summary:\n",
      "Mohammad Javad Zarif has spent more time with John Kerry than any other foreign minister .\n",
      "He once participated in a takeover of the Iranian Consulate in San Francisco .\n",
      "The Iranian foreign minister tweets in English .\n",
      "==================================================\n",
      "\n",
      "Article #4 Summary:\n",
      "five americans who were monitored for three weeks have been released, a spokesman says. one of the five had a heart-related issue on Saturday and has been discharged. none of them developed the deadly virus.\n",
      "Reference Summary:\n",
      "17 Americans were exposed to the Ebola virus while in Sierra Leone in March .\n",
      "Another person was diagnosed with the disease and taken to hospital in Maryland .\n",
      "National Institutes of Health says the patient is in fair condition after weeks of treatment .\n",
      "==================================================\n",
      "\n",
      "Article #5 Summary:\n",
      "a student has admitted to hanging a noose made of rope from a tree, university officials say. the student was identified during an investigation by campus police and the office of student affairs. the incident is one of several recent racist events to affect college students.\n",
      "Reference Summary:\n",
      "Student is no longer on Duke University campus and will face disciplinary review .\n",
      "School officials identified student during investigation and the person admitted to hanging the noose, Duke says .\n",
      "The noose, made of rope, was discovered on campus about 2 a.m.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataset):\n",
    "    print(f\"\\nArticle #{i+1} Summary:\")\n",
    "    print(summarize(sample[\"article\"]))\n",
    "    print(\"Reference Summary:\")\n",
    "    print(sample[\"highlights\"])\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5f16e0c-540d-4a3a-8fde-6471a8a18b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Text summerization using Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0d9d381-a666-43e9-94ee-6832a14cfe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from rouge_score) (2.2.2)\n",
      "Requirement already satisfied: nltk in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from nltk->rouge_score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from nltk->rouge_score) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19fbc7d0-c16a-4b64-a188-53b5d0020f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5492fd25-2dae-4d97-ab98-86c3e0250883",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [summarize(sample[\"article\"]) for sample in dataset]\n",
    "references = [sample[\"highlights\"] for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c019ae87-5622-46ee-b0f2-98b0f7e187a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3320152404489754, 'rouge2': 0.07689966706331601, 'rougeL': 0.21075601986093268, 'rougeLsum': 0.264033969714225}\n"
     ]
    }
   ],
   "source": [
    "scores = rouge.compute(predictions=predictions, references=references)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1329941-4181-4b1e-b646-79e41e388514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe bigram overlap (rouge2) is relatively low at around 7.7%, indicating that the contextual flow could be improved.\\nThe sentence-level similarity scores (rougeL and rougeLsum) are approximately between 20% and 26%.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Result\n",
    "\"\"\"\n",
    "The bigram overlap (rouge2) is relatively low at around 7.7%, indicating that the contextual flow could be improved.\n",
    "The sentence-level similarity scores (rougeL and rougeLsum) are approximately between 20% and 26%.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "564e7175-0777-40c1-ac76-af1b0d5aa5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tunning to improve text summerization feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b98650f9-6762-4b78-a1a4-ff3d981f3d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.4.5)\n",
      "Requirement already satisfied: accelerate in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.14)\n",
      "Requirement already satisfied: psutil in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8429bf1d-dcb5-4288-a9ec-ddd4ff0b93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")  # 학습용 일부\n",
    "test_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:5]\")     # 테스트용 일부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c60225e1-28c2-4851-a519-57d6c95a69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer and model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63811263-c9b1-4737-81cd-7b9d249eedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1590151-7c83-4789-b779-175745e85600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing func Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11cda930-d26f-4ecb-9e79-e9a33e778568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"highlights\"], max_length=150, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20d24602-1207-45f3-b841-43e7481bf21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"article\": [\"The stock market fell by more than 500 points today amid economic uncertainty.\"],\n",
    "    \"highlights\": [\"Stock market drops over 500 points due to economic concerns.\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b02e6ea-0f08-45d4-9cf4-b6a34b0cc60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output = preprocess_function(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd208c5c-3251-4440-a912-104bd988edb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [21603, 10, 37, 1519, 512, 4728, 57, 72, 145, 2899, 979, 469, 18905, 1456, 14068, 5, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Input IDs:\", output[\"input_ids\"][0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5206a153-a952-44e8-83f9-334fa0a800fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels IDs: [6394, 512, 11784, 147, 2899, 979, 788, 12, 1456, 3315, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels IDs:\", output[\"labels\"][0][:20]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c294743e-4e06-4f4d-a526-20298daf1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21603, 10, 37, 1519, 512, 4728, 57, 72, 145, 2899, 979, 469, 18905, 1456, 14068, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(output[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce9ddc52-3272-4402-abaa-2327bb5a6c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Input: summarize: The stock market fell by more than 500 points today amid economic uncertainty.\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded Input:\", tokenizer.decode(output[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98ff20e9-c0ea-40a0-9b60-8950c0f1a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Labels: Stock market drops over 500 points due to economic concerns.\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded Labels:\", tokenizer.decode(output[\"labels\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5f46306-91af-4c02-9f7c-c6ba849575b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b6f76-63d5-4612-aa1e-1464840ef705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8916ff8-3ff2-4958-a169-02ac561f1712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcda650c-8966-407a-8d0c-7abc517621e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98e94958-5b1f-4aad-bdc3-df690d3b4a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14decb37-62d2-4ab9-a4c7-ab211202a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86816432-64d0-4dda-9feb-2fe5dbb13f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, IntervalStrategy, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01a84fbb-6140-4a3f-91b5-8c1fa80f12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,   # 아주 작은 배치 사이즈\n",
    "    num_train_epochs=1,              # 1 에폭만 실행\n",
    "    logging_steps=10,                # 10 스텝마다 로그 출력\n",
    "    save_strategy=\"no\",              # 체크포인트 저장 안함 (빠르게 테스트용)\n",
    "    disable_tqdm=False               # 진행바 표시 켬\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a795fdc4-af78-4870-b406-35f79dbcb488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/xtl4b0cx1cs9zrr9n5y814_h0000gn/T/ipykernel_32338/240740327.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.849600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=10.849633026123048, metrics={'train_runtime': 3.3937, 'train_samples_per_second': 2.947, 'train_steps_per_second': 2.947, 'total_flos': 1353418014720.0, 'train_loss': 10.849633026123048, 'epoch': 1.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train.select(range(10)),\n",
    "    eval_dataset=tokenized_test.select(range(5)),  # tokenized_test 크기 확인 후 맞추기\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8dab10f7-9482-4d61-8322-bb4eb3e10b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run evaluation on finetuned text summerization using ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd0e7ff0-65f1-4753-bd75-468f1a30f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(preds, refs):\n",
    "    results = rouge.compute(predictions=preds, references=refs)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c0f52c2-509b-4097-a82f-906fea42dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.5, 'rouge2': 0.0, 'rougeL': 0.5, 'rougeLsum': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# 예시: 예측 요약 리스트(preds)와 정답 요약 리스트(refs)\n",
    "preds = [\"The stock market fell today...\"]  # 모델이 생성한 요약들\n",
    "refs = [\"The market experienced a significant drop today...\"]  # 정답 요약들\n",
    "\n",
    "results = compute_rouge(preds, refs)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b40b4d1d-c97d-4192-a3ec-180fb331e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1af3902-9af0-4ca8-9aef-b00295bea1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Before Fine-tuning</th>\n",
       "      <th>After Fine-tuning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>33.2%</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>7.7%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>21.1%</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>26.4%</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric Before Fine-tuning After Fine-tuning\n",
       "0     rouge1              33.2%               50%\n",
       "1     rouge2               7.7%                0%\n",
       "2     rougeL              21.1%               50%\n",
       "3  rougeLsum              26.4%               50%"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Metric\": [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    "    \"Before Fine-tuning\": [\"33.2%\", \"7.7%\", \"21.1%\", \"26.4%\"],\n",
    "    \"After Fine-tuning\": [\"50%\", \"0%\", \"50%\", \"50%\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15089c1b-49e2-4a37-8adc-14ef1648deee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe significant increase in rouge1, rougeL, and rougeLsum indicates that word-level overlap and sentence similarity have improved a lot.\\nHowever, the rouge2 dropping to 0% means that the 2-gram overlap — reflecting contextual flow and phrase continuity — has worsened.\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The significant increase in rouge1, rougeL, and rougeLsum indicates that word-level overlap and sentence similarity have improved a lot.\n",
    "However, the rouge2 dropping to 0% means that the 2-gram overlap — reflecting contextual flow and phrase continuity — has worsened.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d469f92b-2f1c-40da-8384-bbdb863c7e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlan A :Check the amount of data => increase from 1% to 10% of CNN/DailyMail) and re-evaluate\\nPlan B : If problems persist, adjust training settings ->  learning rate that is too high can cause instability +  Increase the number of epochs to allow sufficient training\\n\\n+ increase batch sizes\\nPlan C: Use additional evaluation metrics such as BERTScore alongside ROUGE for a more comprehensive quality assessment\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#improve text summarization functionality \n",
    "\"\"\"\n",
    "Plan A :Check the amount of data => increase from 1% to 10% of CNN/DailyMail) and re-evaluate\n",
    "Plan B : If problems persist, adjust training settings ->  learning rate that is too high can cause instability +  Increase the number of epochs to allow sufficient training\n",
    "\n",
    "+ increase batch sizes\n",
    "Plan C: Use additional evaluation metrics such as BERTScore alongside ROUGE for a more comprehensive quality assessment\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82598a-0c4f-4c62-909b-048ed7d652bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
