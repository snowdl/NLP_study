{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2663fdbc-a232-4bc2-8910-8d37ea9503ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "llm = OpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1cd7264c-f75e-48f0-9ba2-c56b987c71fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9b011cd1-2421-4f13-a0ff-8b0756397e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDF Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9bb0fc59-4dec-4128-be60-6089de03a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/jessicahong/Desktop/textclustering.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "be1109d6-edbb-4a01-a69a-85721026f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the documents as a list of LangChain Document objects\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1632e3a8-722c-425b-bb3d-aada8606e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peer-reviewed version of this paper is published in the International Journal of Cognitive Compu\n"
     ]
    }
   ],
   "source": [
    "#Preview the first 100 characters of the first page\n",
    "print(documents[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "979003ba-b033-4835-86c1-ec0143da18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "39f08b57-a23f-4291-b8c5-1561cf0f6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the text splitter\n",
    "# chunk_size: Maximum number of characters per chunk\n",
    "# chunk_overlap: Number of overlapping characters between chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dc92e394-174c-4b65-9518-077994e971f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the loaded documents into smaller chunks\n",
    "# Input should be a list of Document objects\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f564e454-10d6-40f2-9cda-66dfcc10467f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 71\n"
     ]
    }
   ],
   "source": [
    "#Print how many chunks were created\n",
    "print(f\"Number of text chunks: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "527e7a10-bf72-4bb0-82eb-4a37af3bcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "137e1d5e-732a-4b0f-8210-f3a8ebcc7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "310fdc6a-bf89-4792-9152-87f8ee18e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "20a03a6a-a9ca-44af-9eea-8dbe642b6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fbbbcdd4-1a93-4d09-9dda-039a0812125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RetrievalQA chain using OpenAI LLM and FAISS retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(openai_api_key=api_key),  \n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fa388ebd-db17-4182-8904-346ef7f8566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main contributions of this paper are the identification of optimal combinations of embeddings and clustering algorithms in text clustering tasks, as well as the evaluation of the effectiveness of LLM embeddings and the impact of model size and dimensionality reduction on clustering performance. \n"
     ]
    }
   ],
   "source": [
    "#Run your query against the RAG system\n",
    "query = \"What are the main contributions of this paper?\"\n",
    "result = qa_chain.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0bb626ec-3049-4dbd-91b6-d47b8889e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e0dff0ff-861a-4a56-bea0-6366405f75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "326638af-471f-42d5-9755-598014ea1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_text : cleans input text by removing page numbers, collapsing extra blank lines, and stripping unwanted line characters\n",
    "def clean_text(text):\n",
    "    # Remove page numbers, e.g., \"Page 1 of 10\"\n",
    "    text = re.sub(r\"Page \\d+ of \\d+\", \"\", text)\n",
    "    # Replace multiple newlines with a single newline\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)\n",
    "    # Remove special characters except word characters, whitespace, and basic punctuation\n",
    "    text = re.sub(r\"[^\\w\\s.,?!]\", \"\", text)\n",
    "    # Trim leading and trailing whitespace\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bcaf1090-de5a-4177-8e8d-c092b417d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "61b1e667-e199-485e-91f1-e3a149f4c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loops through each document object in the documents list\n",
    "for doc in documents:\n",
    "    # Clean the text content of each document using the clean_text function\n",
    "    doc.page_content = clean_text(doc.page_content)\n",
    "    #applies clean_text func to the page_content attribute of each document to remove unwanted characters, numbers, and extra blank lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b835ad74-f5fc-42e3-8bf9-b113d8b231a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text splitting\n",
    "# Splits long texts into smaller chunks, each about 1000 characters long\n",
    "# chunk_overlap=100 means each chunk overlaps the previous chunk by 100 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)  # Split the cleaned documents into smaller chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f3c1c959-5d1e-428f-b876-eceed301c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAISS VECTORSTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "29c7683e-d2a1-4b03-924a-89bd7a52d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the FAISS vector store to local disk\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "adf87030-08c5-43f3-9435-bcf8738d578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set allow_dangerous_deserialization=True ONLY IF you trust the data source\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bf80fddf-b69d-4095-a3b2-9e6f0507de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate embedding object with your OpenAI API key\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "04ccd1f1-7550-44a9-be6c-a4c85a9f55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d8ef75eb-909e-40bb-9b35-237e7cddd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI LLM with your API key\n",
    "llm = OpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3fcb122d-3851-4468-9b8b-a4d39fe659f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RetrievalQA chain with the vectorstore retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2500e28c-8a01-4954-83ef-1efb032e407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a query against your PDF documents\n",
    "query = \"What are the main contributions of this paper?\"\n",
    "result = qa_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "168cdf11-65d1-4c65-bd80-dc2ef58827f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main contributions of this paper are:\n",
      "1. Testing and identifying optimal combinations of embeddings and clustering algorithms for text clustering tasks.\n",
      "2. Evaluating the effectiveness of embeddings derived from LLMs compared to traditional embedding techniques.\n",
      "3. Assessing the impact of model size and dimensionality reduction through summarisation techniques on clustering performance.\n",
      "4. Highlighting the need to balance detailed text representation with computational feasibility in text clustering tasks.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "034b50f0-56b1-4a44-819a-20beac0e662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d7917da6-735d-4f46-a1c9-3e8f1865f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f671bf4a-e2ac-4b5a-b51d-744ac35da421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f52221a0-cc68-4ac6-b8de-b52cc3350b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length of the embedding vector\n",
    "dimension = len(embeddings.embed_query(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "93720d08-246e-430d-942f-80ef1953594c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1b94e98e-edb6-41d7-a10a-9987e0fe1d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat is IVF : used to speed up vector similarity search by partitioning the entire vector space into multiple clusters (also called centroids), and assigning each vector to its nearest cluster.\\nwhy use IVF : Searching through all vectors in a large dataset is computationally expensive and slow. IVF helps by limiting the search to only the most relevant clusters, dramatically improving search speed.\\nHow it work? \\nThe vector space is first partitioned into n clusters using k-means clustering. This step requires calling index.train(vectors).\\nEach vector is assigned to its closest centroid (cluster center).\\nDuring search, instead of scanning all clusters, only a few relevant clusters are searched, making the process much faster.\\n'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IVF(Inverted File index)\n",
    "\"\"\"\n",
    "What is IVF : used to speed up vector similarity search by partitioning the entire vector space into multiple clusters (also called centroids), and assigning each vector to its nearest cluster.\n",
    "why use IVF : Searching through all vectors in a large dataset is computationally expensive and slow. IVF helps by limiting the search to only the most relevant clusters, dramatically improving search speed.\n",
    "How it work? \n",
    "The vector space is first partitioned into n clusters using k-means clustering. This step requires calling index.train(vectors).\n",
    "Each vector is assigned to its closest centroid (cluster center).\n",
    "During search, instead of scanning all clusters, only a few relevant clusters are searched, making the process much faster.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a00ab539-bd94-40ee-b11e-3cda67a125d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IVF index setup\n",
    "nlist = 10  # num of cluster\n",
    "quantizer = faiss.IndexFlatL2(dimension)  # # Flat index for L2 distance (used as the coarse quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e456ad5b-c769-4732-bd0c-0b48093df8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IVF flat index (coarse quantizer + flat index within each cluster)\n",
    "# - quantizer: used to assign vectors to clusters\n",
    "# - dimension: dimensionality of the embedding vectors\n",
    "# - nlist: number of clusters (coarse centroids)\n",
    "# - faiss.METRIC_L2: use L2 (Euclidean) distance for similarity search\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d8df0f43-8f41-4cb8-b80c-242b59aa223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve vectors from the existing FAISS index inside the vectorstore\n",
    "# reconstruct_n(start, count) returns 'count' vectors starting from index 'start'\n",
    "vectors = vectorstore.index.reconstruct_n(0, vectorstore.index.ntotal)  # (ntotal, dimension) ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "65531372-9293-470d-9db9-edd329cd705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (70, 1536)\n"
     ]
    }
   ],
   "source": [
    "print(type(vectors), vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2479af23-7e33-4983-84c3-619888220b33",
   "metadata": {},
   "outputs": [],
   "source": [
    " # vectors가 ndarray임을 확인했으면 별도 변환 불필요\n",
    "# 만약 리스트라면 numpy array로 변환\n",
    "if not isinstance(vectors, np.ndarray):\n",
    "    vectors = np.array(vectors).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ce8c0f1c-b340-4cbf-8319-b20ff355c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is not trained. Training now...\n",
      "Vectors added successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 70 points to 10 centroids: please provide at least 390 training points\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 학습 확인 후 학습\n",
    "if not index.is_trained:\n",
    "    print(\"Index is not trained. Training now...\")\n",
    "    index.train(vectors)   # 인덱스가 학습되지 않았으면 벡터로 학습함\n",
    "index.add(vectors)        # 학습된 인덱스에 벡터 추가\n",
    "print(\"Vectors added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ad71e-08f2-41d0-8bab-cf4a653630bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
