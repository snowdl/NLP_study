{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f14afa99-df31-449a-ab24-f12e18fd069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.72)\n",
      "Requirement already satisfied: langchain-openai in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (1.97.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d650ac10-ffda-4226-b493-5b4394e5554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: langchain in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: pymupdf in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n",
    "!pip install langchain pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ba6785d-f2f0-425e-8e59-560be0ccea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.28)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (1.97.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "195161fb-e1e4-4b0c-93f6-720190b71e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45f2e99b-a31b-4411-97e1-238b037e8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/jessicahong/Desktop/textclustering.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "326a9d21-f754-43c7-9ca9-eb7041c6293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92da5a03-97f2-4a1c-87ad-542b7c20a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peer-reviewed version of this paper is published in the International Journal of Cognitive Compu\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5d577c9-133f-4f59-b7df-21d425e5b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc1e2e50-2928-411c-a7bd-069253a5701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "289aa321-47a4-4b35-9603-e18444c08fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd1f5568-bd94-411c-abdd-cca0f4b8698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"\".join([doc.page_content for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b2f785e-ee51-4d0c-988d-cc763e7f2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8717ac36-e6ee-4932-8895-3b0682d3eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1개의 텍스트 청크(chunk)가 생성되었습니다.\n",
      "The peer-reviewed version of this paper is published in the International Journal of Cognitive Computing in Engineering at\n",
      "https://doi.org/10.1016/j.ijcce.2024.11.004. This version is typeset by the authors and differs only in pagination\n",
      "and typographical detail.\n",
      "Text Clustering with Large Language Model\n",
      "Embeddings\n",
      "Alina Petukhovaa,∗, Jo˜ao P. Matos-Carvalhoa,b, Nuno Fachadaa,b\n",
      "aCOPELABS, Lus´ofona University, Campo Grande, 376, Lisbon, 1700-921, Portugal\n",
      "bCenter of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent\n",
      "Systems (LASI), Caparica, 2829-516, Portugal\n",
      "Abstract\n",
      "Text clustering is an important method for organising the increasing volume\n",
      "of digital content, aiding in the structuring and discovery of hidden patterns\n",
      "in uncategorised data. The effectiveness of text clustering largely depends\n",
      "on the selection of textual embeddings and clustering algorithms.\n",
      "This\n",
      "study argues that recent advancements in large language models (LLMs)\n",
      "have the potential to enhance this task. The research investigates how dif-\n",
      "ferent textual embeddings, particularly those utilised in LLMs, and various\n",
      "clustering algorithms influence the clustering of text datasets. A series of\n",
      "experiments were conducted to evaluate the impact of embeddings on clus-\n",
      "tering results, the role of dimensionality reduction through summarisation,\n",
      "and the adjustment of model size. The findings indicate that LLM embed-\n",
      "dings are superior at capturing subtleties in structured language. OpenAI’s\n",
      "GPT-3.5 Turbo model yields better results in three out of five clustering\n",
      "metrics across most tested datasets. Most LLM embeddings show improve-\n",
      "ments in cluster purity and provide a more informative silhouette score,\n",
      "reflecting a refined structural understanding of text data compared to tra-\n",
      "ditional methods. Among the more lightweight models, BERT demonstrates\n",
      "leading performance. Additionally, it was observed that increasing model\n",
      "dimensionality and employing summarisation techniques do not consistently\n",
      "enhance clustering efficiency, suggesting that these strategies require careful\n",
      "consideration for practical application. These results highlight a complex\n",
      "balance between the need for refined text representation and computational\n",
      "feasibility in text clustering applications. This study extends traditional\n",
      "text clustering frameworks by integrating embeddings from LLMs, offering\n",
      "improved methodologies and suggesting new avenues for future research in\n",
      "various types of textual analysis.\n",
      "Keywords:\n",
      "text clustering, large language models, text summarisation\n",
      "2000 MSC: 68T50, 62H30\n",
      "1\n",
      "arXiv:2403.15112v5  [cs.CL]  2 Dec 20241. Introduction\n",
      "Text clustering has attracted considerable interest in text analysis due\n",
      "to its potential to reveal hidden structures in large volumes of unstruc-\n",
      "tured textual data. With the exponential growth of digital text content\n",
      "generated through platforms such as social media, online news outlets, and\n",
      "academic publications, the ability to organise and analyse these data has\n",
      "become increasingly critical. Text clustering can organise large volumes of\n",
      "unstructured data into meaningful categories, facilitating efficient informa-\n",
      "tion retrieval and insightful thematic analysis across various domains, such\n",
      "as customer feedback, academic research, and social media content.\n",
      "Text clustering serves as a preliminary step in various text analysis tasks,\n",
      "including topic modelling, trend analysis, and sentiment analysis. By group-\n",
      "ing similar texts, subsequent analyses can proceed with enhanced accuracy\n",
      "and relevance, focusing on more homogeneous data sets with specific char-\n",
      "acteristics or themes. This process improves the precision of the analyses\n",
      "and ensures that the insights derived are more targeted and applicable to\n",
      "the context in which the data were generated.\n",
      "As an analytical task, text clustering involves grouping text documents\n",
      "into clusters such that texts within the same cluster are more similar to\n",
      "each other than to those in different clusters. This process relies on the\n",
      "principle that text documents can be mathematically represented as vec-\n",
      "tors in a high-dimensional space, known as embeddings, where dimensions\n",
      "correspond to various features extracted from the documents, such as word\n",
      "frequency or context. Clustering algorithms use measures of proximity or\n",
      "resemblance to group documents that exhibit close correspondence in the\n",
      "feature space. This approach enables the identification of natural groupings\n",
      "within the data, facilitating more effective organisation and analysis of large\n",
      "text corpora.\n",
      "The research presented in this article aims to contribute to the do-\n",
      "main of text clustering by testing and identifying optimal combinations\n",
      "of embeddings—including those used in recently released large language\n",
      "models (LLMs)—and clustering algorithms that maximise clustering per-\n",
      "formance across various datasets. The primary objective of this paper is\n",
      "∗Corresponding author\n",
      "Email addresses: alina.petukhova@ulusofona.pt (Alina Petukhova),\n",
      "joao.matos.carvalho@ulusofona.pt (Jo˜ao P. Matos-Carvalho),\n",
      "nuno.fachada@ulusofona.pt (Nuno Fachada)\n",
      "2to determine if embeddings derived from LLMs outperform traditional em-\n",
      "bedding techniques, such as Term Frequency-Inverse Document Frequency\n",
      "(TF-IDF). Additionally, experiments are conducted to evaluate the impact\n",
      "of model size and dimensionality reduction through summarisation tech-\n",
      "niques on clustering performance.\n",
      "Results indicate that LLM embeddings are highly effective at captur-\n",
      "ing the structured aspects of language, with BERT demonstrating superior\n",
      "performance among lightweight models.\n",
      "Moreover, increasing model di-\n",
      "mensionality and employing summarisation techniques do not consistently\n",
      "enhance clustering efficiency, suggesting that these strategies require careful\n",
      "evaluation for practical application. These findings underscore the need to\n",
      "balance detailed text representation with computational feasibility in text\n",
      "clustering tasks.\n",
      "This paper is organised as follows. In Section 2, advancements in textual\n",
      "embeddings are described, and classical text clustering algorithms used in\n",
      "this domain are briefly mentioned. Section 3 outlines the main steps and\n",
      "components of this study, including dataset selection, data preprocessing,\n",
      "embeddings, and clustering algorithm configurations used to assess clus-\n",
      "tering quality.\n",
      "Section 4 presents the results of our study and provides\n",
      "a discussion of these findings. Limitations encountered during the study,\n",
      "along with recommendations for overcoming them, are acknowledged in Sec-\n",
      "tion 5. Finally, Section 6 synthesises the main conclusions of this research\n",
      "and suggests future developments.\n",
      "2. Background\n",
      "2.1. Text Embeddings\n",
      "The field of text representation in natural language processing (NLP) has\n",
      "undergone an impressive transformation over the past few decades. From\n",
      "simple representations to highly sophisticated embeddings, advancements in\n",
      "this domain have significantly improved the ability of machines to process\n",
      "and understand human language with increasing accuracy.\n",
      "One of the earliest methods of text representation that laid the ground-\n",
      "work for subsequent advances was Term Frequency-Inverse Document Fre-\n",
      "quency (TF-IDF). This method quantifies the importance of a word within a\n",
      "document relative to a corpus by accounting for term frequency and inverse\n",
      "document frequency [1]. While TF-IDF effectively highlights the relevance\n",
      "of words, it treats each term as independent and fails to capture word con-\n",
      "text and semantic meaning [2].\n",
      "3Word embeddings, such as those produced by Word2Vec [3] and GloVe [4],\n",
      "marked a significant advancement by generating dense vector representa-\n",
      "tions of words based on their contexts. These models leveraged surrounding\n",
      "words and large corpora to learn word relationships, successfully capturing\n",
      "a range of semantic and syntactic similarities. Despite their effectiveness in\n",
      "capturing semantic regularities, these models still provided a single static\n",
      "vector per word, which posed limitations in handling polysemous words—\n",
      "words with multiple meanings.\n",
      "The arrival of BERT (Bidirectional Encoder Representations from Trans-\n",
      "formers) initiated a new phase of embedding sophistication [5]. To generate\n",
      "contextual embeddings, BERT employs a bidirectional transformer archi-\n",
      "tecture, pre-trained on a massive corpus. This allows for a deeper under-\n",
      "standing of word relationships by considering the full context of a word in a\n",
      "sentence in both directions. BERT revolutionised tasks like text clustering\n",
      "by providing richer semantic representations.\n",
      "Today, LLMs like OpenAI’s GPT are at the forefront of generating state-\n",
      "of-the-art embeddings [6]. LLMs extend the capabilities of previous models\n",
      "by providing an unprecedented depth and breadth of knowledge encoded in\n",
      "word and sentence-level embeddings. These models are trained on extensive\n",
      "datasets to capture a broad spectrum of human language variations and\n",
      "generate embeddings that reflect a comprehensive understanding of contexts\n",
      "and concepts.\n",
      "The progression from TF-IDF to sophisticated LLM embeddings rep-\n",
      "resents a significant advancement towards more contextually aware text\n",
      "representation in NLP. This evolution continues to propel the field forward,\n",
      "expanding the possibilities for applications such as text clustering, sentiment\n",
      "analysis, and beyond. In the context of text clustering methodologies, there\n",
      "exists a considerable research gap that underscores the need for a compre-\n",
      "hensive evaluation of LLM embeddings against traditional techniques such\n",
      "as TF-IDF.\n",
      "2.2. Text Clustering Algorithms\n",
      "Text clustering involves grouping a set of texts such that texts in the\n",
      "same group (referred to as a cluster) are more similar to each other than\n",
      "to those in different clusters. This section provides an overview of classic\n",
      "clustering algorithms widely used for clustering textual data.\n",
      "K-means is perhaps the most well-known and commonly used clustering\n",
      "algorithm due to its simplicity and efficiency. It partitions the dataset into k\n",
      "clusters by minimising the within-cluster sum of squares, i.e., variance. Each\n",
      "4cluster is represented by the mean of its points, known as the centroid [7].\n",
      "K-means is particularly effective for large datasets but depends heavily on\n",
      "the initialisation of centroids and the value of k, which must be known a\n",
      "priori.\n",
      "Agglomerative hierarchical clustering (AHC) builds nested clusters by\n",
      "merging them successively. This bottom-up approach starts with each text\n",
      "as a separate cluster and combines clusters based on a linkage criterion,\n",
      "such as the minimum or maximum distance between cluster pairs [8]. AHC\n",
      "is versatile and allows for discovering hierarchies in data, but it can be\n",
      "computationally expensive for large datasets.\n",
      "Spectral clustering techniques use the eigenvalues of a similarity matrix\n",
      "to reduce dimensionality before applying a clustering algorithm such as k-\n",
      "means. It is particularly adept at identifying clusters that are not necessarily\n",
      "spherical, as k-means assumes. Spectral clustering can also handle noise and\n",
      "outliers effectively [9]. However, its computational cost can be high due to\n",
      "the eigenvalue decomposition involved.\n",
      "Fuzzy c-means (FuzzyCM) is a clustering method that allows data points\n",
      "to belong to more than one cluster. This method minimises the objective\n",
      "function with respect to membership and centroid position, providing soft\n",
      "clustering and handling overlapping clusters [10]. FuzzyCM is useful when\n",
      "the boundaries between clusters are not clearly defined, although it is com-\n",
      "putationally more intensive than k-means.\n",
      "In addition to these classic approaches, recent years have seen the rise\n",
      "of alternative methods for text clustering that leverage the strengths of\n",
      "modern embeddings and consider the unique properties of textual data.\n",
      "These include using deep learning models, particularly those based on au-\n",
      "toencoders, to learn meaningful low-dimensional representations ideal for\n",
      "clustering [11, 12].\n",
      "Ensemble clustering approaches, where multiple clustering algorithms\n",
      "are combined to improve the robustness and quality of the results, have\n",
      "also been gaining attention. These methods benefit from the diversity of\n",
      "the individual algorithms and can sometimes overcome the limitations of\n",
      "any single method [13].\n",
      "The field of text clustering is rapidly expanding, and keeping up with\n",
      "the latest findings is crucial for advancing the state of the art. Recent pa-\n",
      "pers, such as those exploring the use of transformer-based embeddings for\n",
      "clustering [14] and integrating external knowledge bases into the clustering\n",
      "process [15], represent just the tip of the iceberg in this area of research.\n",
      "5Additionally, very recent work by Keraghel et al. [16] offers a preliminary\n",
      "discussion on the potential of using LLM embeddings for text clustering.\n",
      "The authors analysed embeddings from BLOOMZ, Mistral, Llama-2, and\n",
      "OpenAI using five clustering algorithms.\n",
      "This paper extends that work\n",
      "by focusing on different datasets, testing additional LLM embeddings, and\n",
      "comparing results using classical TF-IDF as a baseline. Furthermore, this\n",
      "study experiments with summarisation as a dimensionality reduction tech-\n",
      "nique and evaluates the impact of model size on clustering results.\n",
      "3. Methods\n",
      "This research evaluates the effectiveness of various embedding represen-\n",
      "tations in enhancing the performance of text clustering algorithms, aiming\n",
      "to identify the most informative embeddings for clustering tasks. To achieve\n",
      "this, we systematically experimented with multiple datasets, embeddings,\n",
      "and clustering methods, and performed an in-depth analysis of clustering\n",
      "results using different evaluation metrics. The following steps were under-\n",
      "taken during the course of this study:\n",
      "1. Selection of datasets, ensuring the robustness of our findings across\n",
      "different types of textual data.\n",
      "2. Preprocessing of datasets, including the removal of miscellaneous char-\n",
      "acters such as emails and HTML tags.\n",
      "3. Utilisation of various embedding computations, including LLM-related\n",
      "ones, to retrieve numerical text representations.\n",
      "4. Application of several clustering algorithms commonly used in text\n",
      "clustering.\n",
      "5. Comparison of clustering results using different external and internal\n",
      "validation metrics.\n",
      "The following subsections provide a comprehensive description of each\n",
      "of these steps, further detailing the employed methodologies.\n",
      "3.1. Datasets\n",
      "We selected five datasets to cover a variety of text clustering challenges.\n",
      "Table 1 shows these datasets and their characteristics. The CSTR abstracts\n",
      "dataset [17] is a corpus of 299 scientific abstracts from the Centre for Speech\n",
      "Technology Research. The homogeneous and domain-specific nature of the\n",
      "6CSTR dataset allowed us to investigate the effectiveness of clustering tech-\n",
      "niques in discerning fine-grained topic distinctions in scholarly text related\n",
      "to categories such as artificial intelligence, theory, systems, and robotics.\n",
      "The SyskillWebert dataset [18], which includes user ratings for web\n",
      "pages, enabled exploratory clustering to analyse information used in rec-\n",
      "ommendation systems. The 20Newsgroups dataset [19] is a famous collec-\n",
      "tion of approximately 19,000 news documents partitioned across 20 different\n",
      "classes. With its broad assortment of topics and noisy, unstructured text,\n",
      "this dataset provided a realistic scenario for evaluating the robustness of\n",
      "clustering algorithms under less-than-ideal conditions.\n",
      "To complement these, we added the MN-DS dataset [20], a diverse com-\n",
      "pilation of multimedia news articles, which provides the opportunity to ex-\n",
      "plore the effectiveness of clustering algorithms in handling multi-categorical\n",
      "data. The dataset is organised hierarchically in two levels; therefore, ex-\n",
      "periments were performed independently for each level. Additionally, we\n",
      "included the Reuters dataset [21], a benchmark dataset widely used in text\n",
      "mining and information retrieval research, which contains stories from the\n",
      "Reuters news agency. To enhance the comprehensiveness of our evaluation,\n",
      "we selected documents assigned to single classes, reducing the dataset from\n",
      "10,369 to 8,654 text articles. This allowed us to assess the clustering al-\n",
      "gorithms’ ability to accurately group similar documents according to their\n",
      "predefined categories.\n",
      "Table 1: Tested datasets and their characteristics. ‘Size’ represents the number of docu-\n",
      "ments in the dataset, while ‘No. classes’ is the number of categories.\n",
      "Dataset\n",
      "Size\n",
      "No. classes\n",
      "Ref.\n",
      "CSTR\n",
      "299\n",
      "4\n",
      "[17]\n",
      "SyskillWebert\n",
      "333\n",
      "4\n",
      "[18]\n",
      "20Newsgroups dataset\n",
      "18,846\n",
      "20\n",
      "[19]\n",
      "MN-DS dataset level 1\n",
      "10,917\n",
      "17\n",
      "[20]\n",
      "MN-DS dataset level 2\n",
      "10,917\n",
      "109\n",
      "[20]\n",
      "Reuters\n",
      "8,654\n",
      "65\n",
      "[21]\n",
      "3.2. Preprocessing\n",
      "The motivation for preprocessing text data is to minimise noise and\n",
      "highlight key patterns, thereby improving the efficiency and accuracy of\n",
      "7clustering algorithms [22].\n",
      "For all datasets, a series of preprocessing steps were taken to ensure the\n",
      "quality and uniformity of the input data. The initial step involved removing\n",
      "miscellaneous items such as irrelevant metadata, HTML tags, and any extra-\n",
      "neous content that might skew the analysis. Continuing with this method-\n",
      "ology, we systematically eliminated invalid non-Latin characters from the\n",
      "dataset. These characters could arise from multilingual data sources or arte-\n",
      "facts of data collection, such as encoding errors. Given that the employed\n",
      "workflow is optimised for Latin-based languages, retaining such characters\n",
      "could increase the dimensionality of the feature space, thereby undermin-\n",
      "ing clustering performance, both in terms of computational efficiency and\n",
      "interpretability of the results.\n",
      "3.3. Text Embeddings\n",
      "For textual data representation, we compared classical embeddings [5,\n",
      "23] and state-of-the-art LLM embeddings.\n",
      "Traditional TF-IDF vectors\n",
      "served as a baseline, providing a sparse but interpretable representation\n",
      "based on word importance within the analysed dataset. BERT embeddings\n",
      "were extracted from the BERT model, which was trained as a transformer-\n",
      "based bidirectional encoder on BookCorpus [24] and English Wikipedia [25].\n",
      "These embeddings were utilised to achieve deep contextual understanding,\n",
      "capturing the semantic variations across the corpus.\n",
      "We also employed “text-embedding-ada-002” [26] embeddings, as they\n",
      "demonstrated the best results among OpenAI’s embeddings on larger datasets\n",
      "for tasks such as text search, code search, and sentence similarity. Addi-\n",
      "tionally, other LLM embeddings were extracted for Falcon [27] and LLaMA-\n",
      "2-chat [28] models, included for their respective advancements in perfor-\n",
      "mance and efficiency.\n",
      "Falcon embeddings were trained on a hybrid cor-\n",
      "pus consisting of both text documents and code, while the LLaMA-2-chat\n",
      "embeddings—built on the foundation of the LLaMA-2 model using an opti-\n",
      "mised auto-regressive transformer—underwent targeted fine-tuning for dia-\n",
      "logue and question-and-answer tasks.\n",
      "BERT, Falcon, and LLaMA-2-chat embeddings were obtained from Hug-\n",
      "ging Face’s transformers library, a platform providing state-of-the-art mod-\n",
      "els in accessible pipelines [29]. Table 2 describes the exact embeddings and\n",
      "configurations used.\n",
      "8Table 2: Tested embeddings and their configuration. For TF-IDF, parameters min df and\n",
      "max df are used to exclude terms that have a document frequency strictly lower than or\n",
      "higher than the specified thresholds, respectively, during the vocabulary creation process;\n",
      "if real-valued, these parameters represent a proportion of documents; if integer-valued,\n",
      "they represent absolute document counts. In turn, max features limits the vocabulary to\n",
      "only the top max features terms, ordered by term frequency across the corpus.\n",
      "Embeddings\n",
      "Configuration\n",
      "TF-IDF\n",
      "min df = 5, max df = 0.95, max features = 8000\n",
      "BERT\n",
      "huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
      "OpenAI\n",
      "huggingface.co/Xenova/text-embedding-ada-002\n",
      "Falcon\n",
      "huggingface.co/tiiuae/falcon-7b\n",
      "LLaMA-2\n",
      "huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
      "3.4. Clustering Algorithms\n",
      "The selected clustering algorithms address the diverse nature of text\n",
      "data, which often contains complex patterns requiring robust methods for\n",
      "effective grouping. Standard k-means clustering was used for its simplicity\n",
      "and efficiency in dealing with large datasets. K-means++ was chosen as\n",
      "an enhanced variant of k-means, with careful initialisation to improve con-\n",
      "vergence and cluster quality [30]. AHC was utilised for its ability to reveal\n",
      "nested structures within the data. Compared to k-means, which assigns\n",
      "each data point to a single cluster, FuzzyCM provides a probabilistic mem-\n",
      "bership approach, accommodating polysemy and subtle semantic differences\n",
      "typical in text data. Finally, Spectral clustering was selected for its effec-\n",
      "tiveness in identifying clusters based on their data-induced graph structure,\n",
      "and it is particularly adept at discovering clusters with non-convex shapes.\n",
      "To map the newly formed clusters to the original labels of the dataset,\n",
      "we calculated the Euclidean distance between the centroid of each derived\n",
      "cluster and the centroid of each ground truth cluster, assigning the closest\n",
      "ones. The Euclidean distance is a preferable metric for this process because\n",
      "it accurately reflects distances in multidimensional space, ensuring precise\n",
      "mapping. This method allows the use of external evaluation metrics such\n",
      "as the F1-score. In our work, we did not apply Euclidean distance at the\n",
      "word level, which can be problematic. Instead, the process computes an\n",
      "aggregation for each identified cluster, enabling a more reliable association\n",
      "with the existing labels.\n",
      "9The selected algorithms and their respective parameters are listed in\n",
      "Table 3. The scikit-learn library [31] provided the implementations for all\n",
      "algorithms except for FuzzyCM, which was utilised from the scikit-fuzzy\n",
      "package [32].\n",
      "Table 3: Tested clustering algorithms and respective parameters. For k-means and k-\n",
      "means++, init refers to the method used for the initial selection of cluster centroids, ninit\n",
      "specifies the number of times the k-means algorithm is run with different centroid seeds,\n",
      "while seed defines the random number for centroid initialisation. For AHC, metric defines\n",
      "the metric used to compute the linkage, while linkage specifies the linkage criterion that\n",
      "determines the distance measurement used between sets of observations. For FuzzyCM,\n",
      "init is the initial fuzzy c-partitioned matrix—if set to None, the matrix will be randomly\n",
      "initialised—m is the degree of fuzziness, error defines the stopping criterion, and maxiter\n",
      "specifies the maximum number of iterations allowed. For Spectral clustering, assign labels\n",
      "determines the strategy used to assign labels in the embedding space and seed is a pseudo-\n",
      "random number seed for initialising the locally optimal block preconditioned conjugate\n",
      "gradient eigenvectors decomposition. For all algorithms, the number of clusters was set\n",
      "to match the number of classes present in the dataset.\n",
      "Algorithm\n",
      "Parameters\n",
      "k-means\n",
      "init = random, ninit = 10, seed = 0\n",
      "k-means++\n",
      "init = k-means++, ninit = 1, seed = 0\n",
      "AHC\n",
      "metric = euclidean, linkage = ward\n",
      "FuzzyCM\n",
      "init = None, m = 2, error = 0.005, maxiter = 1000\n",
      "Spectral\n",
      "assign labels = discretize, seed = 10\n",
      "3.5. Evaluation Metrics\n",
      "To comprehensively evaluate the quality of different embeddings and al-\n",
      "gorithm combinations, we used a diverse set of metrics. For external valida-\n",
      "tion, since the original labels were available, we used the weighted F1-score\n",
      "(F1S) [33], the Adjusted Rand Index (ARI) [34], and the Homogeneity score\n",
      "(HS) [35]. The F1-score was computed to balance precision and recall in\n",
      "the presence of class imbalance. ARI was used to assess clustering outcomes\n",
      "while correcting for chance grouping, and HS was used to evaluate the degree\n",
      "to which each cluster is composed of data points primarily from one class.\n",
      "For internal validation, we employed the Silhouette Score (SS) [36] and the\n",
      "Calinski-Harabasz Index (CHI) [37], evaluating cluster coherence and sepa-\n",
      "ration without requiring ground truth. This multifaceted approach ensures\n",
      "a robust assessment, capturing both the alignment with known labels and\n",
      "10the intrinsic structure of the generated clusters. These metrics collectively\n",
      "provide a balanced view of performance, accounting for datasets with vary-\n",
      "ing characteristics and sizes. The metrics and their corresponding formulas\n",
      "are presented in Table 4.\n",
      "Table 4: Metrics used to assess clustering results, their type (external or internal), and\n",
      "their respective formulas. For the F1-score (F1S), C represents the number of classes,\n",
      "wi is the weight assigned to the i-th class, which is typically the proportion of that\n",
      "class within the dataset, and F1,i represents the F1-score computed for the i-th class.\n",
      "For the Adjusted Rand Index (ARI), RI stands for Rand Index, Expected RI refers to\n",
      "the expected value of the Rand Index under random label assignment (calculated using\n",
      "the contingency table marginals), and Max RI is the maximum possible value of the\n",
      "Rand Index. For the Homogeneity Score (HS), H(C|K) is the conditional entropy of\n",
      "the class distribution given the predicted cluster assignments, and H(C) is the entropy\n",
      "of the class distribution. For the Silhouette Score (SS), N represents the total number\n",
      "of data points in the dataset, and s(i) is the silhouette score for a single data point i,\n",
      "defined as s(i) =\n",
      "b(i)−a(i)\n",
      "max{a(i),b(i)}, where a(i) is the average distance from the i-th data\n",
      "point to other points in the same cluster, and b(i) is the minimum mean distance from\n",
      "the i-th data point to points in a different cluster, minimised over all clusters. For the\n",
      "Calinski-Harabasz Index (CHI), Tr(Bk) is the trace of the between-group dispersion\n",
      "matrix that measures the between-cluster dispersion, Tr(Wk) is the trace of the within-\n",
      "cluster dispersion matrix, which quantifies the within-cluster dispersion, N refers to the\n",
      "number of data points, and k indicates the number of clusters. The optimal value for all\n",
      "these metrics is the maximum.\n",
      "Metric\n",
      "Type\n",
      "Formula\n",
      "F1S\n",
      "External\n",
      "C\n",
      "X\n",
      "i=1\n",
      "wiF1,i\n",
      "ARI\n",
      "External\n",
      "RI −Expected RI\n",
      "Max RI −Expected RI\n",
      "HS\n",
      "External\n",
      "1 −H(C|K)\n",
      "H(C)\n",
      "SS\n",
      "Internal\n",
      "1\n",
      "N\n",
      "PN\n",
      "i=1 s(i)\n",
      "CHI\n",
      "Internal\n",
      "Tr(Bk)\n",
      "Tr(Wk) × N−k\n",
      "k−1\n",
      "3.6. Additional Experiments\n",
      "This section describes additional experiments in which we performed\n",
      "text summarisation (3.6.1) and tested LLM embeddings obtained from larger\n",
      "models (i.e., models with more parameters) prior to clustering (3.6.2). The\n",
      "11purpose of these experiments is to investigate whether such representations\n",
      "can improve the discriminability of features within text clusters.\n",
      "3.6.1. Summarisation\n",
      "This experiment aims to evaluate summarisation as a tool for dimension-\n",
      "ality reduction in text clustering by creating compact representations of the\n",
      "texts that encapsulate their semantic core without losing context. These ex-\n",
      "periments are hypothesised to streamline the clustering process, potentially\n",
      "leading to more coherent and interpretable clusters, even in large and com-\n",
      "plex datasets. This involves adding a summarisation step after preprocess-\n",
      "ing and before the retrieval of embeddings. The models used in summari-\n",
      "sation are described in Table 5. As an alternative approach to LLM-based\n",
      "models, we used the BERT-large-uncased summarisation model [5] imple-\n",
      "mented by the BERT summariser [38] to assess potential improvements in\n",
      "clustering achieved by utilising a lower-dimensionality model.\n",
      "Table 5: Overview of text summarisation models used in this study, each employing a\n",
      "transformer-based architecture. ‘Max Tokens’ is the maximum input sequence length.\n",
      "BERT is a bidirectional encoder-only model trained on BookCorpus, a dataset consisting\n",
      "of 11,038 unpublished books and English Wikipedia (excluding lists, tables, and headers);\n",
      "OpenAI is a GPT-3-based decoder-only model built with multi-head attention blocks,\n",
      "trained on an extended dataset with reinforcement learning from human feedback; Falcon\n",
      "is a causal decoder-only model with a multi-query attention mechanism trained on 1,500B\n",
      "tokens of RefinedWeb enhanced with curated corpora; LLaMA-2-chat is a LLaMA-2-\n",
      "based auto-regressive decoder-only model trained on a 1,000B token dataset collected by\n",
      "Meta, enriched with supervised fine-tuning, reinforcement learning from human feedback,\n",
      "and the Ghost Attention mechanism.\n",
      "Embeddings\n",
      "Summarisation Model\n",
      "Max Tokens\n",
      "BERT\n",
      "bert-large-uncased [5]\n",
      "512\n",
      "OpenAI\n",
      "gpt-3.5-turbo [39]\n",
      "4096\n",
      "Falcon\n",
      "falcon-7b [40, 27]\n",
      "2048\n",
      "LLaMA-2-chat\n",
      "Llama-2-7b-chat-hf [41, 28]\n",
      "4096\n",
      "For the LLaMA-2 and Falcon models, we used the Hugging Face trans-\n",
      "formers library with the parameters described in Table 6.\n",
      "12Table 6:\n",
      "Parameters used for summarisation with LLaMA-2 and Falcon models:\n",
      "temperature represents the value to modulate probabilities of the next token, max length\n",
      "defines the maximum length of the sequence to be generated, do sample is a parameter\n",
      "that determines whether to use sampling rather than greedy decoding, top k restricts the\n",
      "selection to the top k tokens with the highest probabilities during top-k filtering, and\n",
      "num return sequences is the number of independently computed returned sequences for\n",
      "each element in the batch.\n",
      "Parameter name\n",
      "Value\n",
      "temperature\n",
      "0\n",
      "max length\n",
      "800\n",
      "do sample\n",
      "True\n",
      "top k\n",
      "10\n",
      "num return sequences\n",
      "1\n",
      "The following zero-shot prompt was used for generating the summarised\n",
      "text with LLMs:\n",
      "Write a concise summary of the text. Return your responses\n",
      "with maximum 5 sentences that cover the key points of the text.\n",
      "{text}\n",
      "SUMMARY:\n",
      "3.6.2. Increasing Model Dimension\n",
      "The original publications on LLMs underline the performance increase\n",
      "with larger model sizes in tasks such as common sense reasoning, question\n",
      "answering, and code tasks [27, 28]. This experiment evaluates embeddings\n",
      "from various LLM sizes to analyse the impact of higher-dimensional models\n",
      "on the performance of clustering algorithms, aiming to determine if they\n",
      "enhance cluster cohesion and separation. For this purpose, we used embed-\n",
      "dings obtained from models presented in Table 7.\n",
      "To visualise the different embeddings and capture their intrinsic struc-\n",
      "tures, Principal Component Analysis (PCA) and t-Distributed Stochastic\n",
      "Neighbor Embedding (t-SNE) [42] were employed. Initially, PCA was ap-\n",
      "plied for preliminary dimensionality reduction while preserving variance.\n",
      "Subsequently, t-SNE was used to project the data into a lower-dimensional\n",
      "space, emphasising local disparities between embeddings. This sequential\n",
      "13application of PCA and t-SNE allows us to capture both global and lo-\n",
      "cal structures within the embeddings, providing a richer visualisation than\n",
      "using PCA alone.\n",
      "Table 7: Embeddings and corresponding models used in the dimensionality experiments.\n",
      "Here, ‘Size’ represents the number of parameters denoted in billions (bp), and ‘Tokens’\n",
      "refers to the embedding token size in trillions (trl).\n",
      "Model\n",
      "Model Reference\n",
      "Size\n",
      "(bp)\n",
      "Tokens\n",
      "(trl)\n",
      "Falcon-7b\n",
      "huggingface.co/tiiuae/falcon-7b\n",
      "7\n",
      "1.5\n",
      "Falcon-40b\n",
      "huggingface.co/tiiuae/falcon-40b\n",
      "40\n",
      "1\n",
      "LLaMA-2-7b\n",
      "huggingface.co/meta-llama/Llama-\n",
      "2-7b-chat-hf\n",
      "7\n",
      "2\n",
      "LLaMA-2-13b\n",
      "huggingface.co/meta-llama/Llama-\n",
      "2-13b-chat-hf\n",
      "13\n",
      "2\n",
      "4. Results and Discussion\n",
      "Table 8 presents the clustering metrics for the “best” algorithm for the\n",
      "tested combinations of dataset, embedding, and clustering algorithm. By\n",
      "“best”, we mean the algorithm with the highest F1-score value. The com-\n",
      "plete results are available as supplementary material1.\n",
      "Results demonstrate that OpenAI embeddings generally yield superior\n",
      "clustering performance on structured, formal texts compared to other meth-\n",
      "ods based on most metrics (column ‘Total’ in Table 8). The combination of\n",
      "the k-means algorithm and OpenAI’s embeddings yielded the highest val-\n",
      "ues of ARI, F1-score, and HS in most experiments. This may be attributed\n",
      "to OpenAI’s embeddings being trained on a diverse array of Internet text,\n",
      "rendering them highly effective at capturing the diversity of language struc-\n",
      "tures.\n",
      "Low values of SS and CHI for the same algorithm could indicate that,\n",
      "while clusters are homogeneous and aligned closely with the ground truth\n",
      "labels (suggesting good class separation and cluster purity), they may not\n",
      "1https://doi.org/10.5281/zenodo.10844657\n",
      "14be well-separated or compact as evaluated in a geometric space. This dis-\n",
      "crepancy can arise in cases where data has a high-dimensional or complex\n",
      "structure that external measures such as ARI, F1S, and HS capture effec-\n",
      "tively. However, when projecting into a lower-dimensional space for SS and\n",
      "CHI, the clusters appear to overlap or vary widely in size, leading to a\n",
      "lower score for these spatial coherence metrics. This highlights a challenge\n",
      "in text clustering: achieving both high cluster purity alongside tight cluster\n",
      "geometry.\n",
      "Interestingly, for the case of the Reuters dataset (DS6), we observed\n",
      "that FuzzyCM consistently outperformed other clustering algorithms when\n",
      "paired with different embeddings. The potential reason behind this result\n",
      "may be related to the flexibility of fuzzy clustering, which allows data points\n",
      "to belong to multiple clusters with varying degrees of membership, making\n",
      "it particularly suitable for datasets with a large number of classes such as\n",
      "the Reuters dataset.\n",
      "In the domain of open-source models, namely Falcon, LLaMA-2, and\n",
      "BERT, the latter emerged as the frontrunner. Given that BERT is designed\n",
      "to understand context and potentially due to the model’s lower dimension-\n",
      "ality, these embeddings demonstrate good effectiveness in text clustering.\n",
      "In the comparative analysis of open-source LLM embeddings, Falcon-7b\n",
      "outperformed LLaMA-2-7b across most datasets, demonstrating improved\n",
      "cluster quality and distinctiveness. This superiority may be attributed to\n",
      "Falcon-7b embeddings’ ability to better capture salient linguistic features\n",
      "and semantic relationships within the texts since these embeddings were\n",
      "trained on a mixed corpus of text and code, as opposed to the LLaMA-2\n",
      "embeddings, which are specialised for dialogues and Q&A contexts. Ad-\n",
      "ditionally, the CHI metric—measuring the dispersion ratio between and\n",
      "within clusters—is higher for Falcon-7b embeddings, suggesting that clus-\n",
      "ters are well-separated and dense.\n",
      "Experiments with the MN-DS dataset, featuring a hierarchical label\n",
      "structure, indicate that clustering at a higher, more abstract label level (17\n",
      "classes) produces better class separation. However, the F1-score is higher\n",
      "when clustering is assessed at the more specific level (109 classes). This\n",
      "likely indicates better precision and recall for individual classes, while lower\n",
      "values for other metrics reflect the increased difficulty in maintaining overall\n",
      "clustering quality and cohesion with a higher number of classes. These re-\n",
      "sults indicate that clustering at higher levels can produce more cohesive and\n",
      "interpretable clusters that align with natural categorical divisions, though\n",
      "15at the expense of extracting less specific information for each document.\n",
      "Results of the summarisation experiment, depicted in Table 9, show that\n",
      "using summarisation as a dimensionality reduction technique does not con-\n",
      "sistently benefit all models. Clustering results for the original texts without\n",
      "generated summaries are generally higher than those with summarisation.\n",
      "This finding suggests that essential details necessary for accurate clustering\n",
      "might have been lost during the summarisation process. Alternatively, the\n",
      "inherent complexity and variations of textual representation might require\n",
      "a more sophisticated approach to text summarisation that can maintain\n",
      "essential information while reducing complexity. Additionally, it is impor-\n",
      "tant to highlight that we observed low-quality clustering results when using\n",
      "the summarisation output from the smaller-sized LLaMA-2-7b and Falcon-\n",
      "7b models, likely due to their limited ability to capture and reproduce the\n",
      "complexity and subtle gradations in the source texts.\n",
      "Results for the model size experiment, presented in Table 10, highlight\n",
      "the negative influence of the number of parameters on clustering outcomes\n",
      "for Falcon, given that the larger model, Falcon-40b, produced lower ARI,\n",
      "F1-score, and HS for the CSTR and SyskillWebert datasets. This may be\n",
      "because embeddings for the Falcon-40b model were created over a subset of\n",
      "the data used for the Falcon-7b embeddings (1.5 trillion tokens for Falcon-\n",
      "7b and 1 trillion tokens for Falcon-40b). In the case of LLaMA-2, models\n",
      "with sizes 7b and 13b were reportedly trained on identical datasets [41].\n",
      "Results indicate that embeddings from the larger model, LLaMA-2-13b,\n",
      "outperform those from LLaMA-2-7b. This outcome is anticipated, as larger\n",
      "models with more parameters generally have a greater capacity to capture\n",
      "complex patterns and relationships in the data, leading to richer and more\n",
      "expressive embeddings.\n",
      "Another perspective is given by Figure 1, which shows a visualisation\n",
      "of different LLM parameter sizes for the CSTR dataset using PCA and\n",
      "t-SNE. A noticeable artefact for LLaMA-2-7b (Figure 1a) and Falcon-40b\n",
      "(Figure 1d) is the lack of coherence in the four document classes, indicating\n",
      "an unclear delimitation in the feature space. This observation is consistent\n",
      "with the results in Table 10, where these two models have the lowest F1S,\n",
      "ARI, and HS values. Conversely, when employing LLaMA-2-13b (Figure 1b)\n",
      "and Falcon-7b (Figure 1c), the classes exhibit better separation, which aligns\n",
      "with the clustering results observed in Table 10, where these two models\n",
      "achieved the highest values in 3 out of the 5 metrics calculated. Increasing\n",
      "the number of tokens in Falcon yielded better results, aligning with existing\n",
      "16Table 8: Results of text clustering for the best-performing clustering algorithms for each\n",
      "combination of dataset and embedding. The best algorithm was determined by choosing\n",
      "the algorithm with the highest F1-score value. DS1 represents the CSTR dataset, DS2 is\n",
      "the SyskillWebert dataset, DS3 is the 20newsgroups dataset, DS4 is the MN-DS dataset\n",
      "for level 1 labels, DS5 is the MN-DS dataset for level 2 labels, and DS6 is the Reuters\n",
      "dataset. ‘Total’ represents the number of metrics per given embeddings/algorithm com-\n",
      "bination that outperform other combinations.\n",
      "Dataset Embed.\n",
      "Best Alg.\n",
      "F1S\n",
      "ARI\n",
      "HS\n",
      "SS\n",
      "CHI Total\n",
      "DS1\n",
      "TF-IDF\n",
      "k-means\n",
      "0.67\n",
      "0.38\n",
      "0.46\n",
      "0.016\n",
      "4\n",
      "0/5\n",
      "BERT\n",
      "Spectral\n",
      "0.85\n",
      "0.60\n",
      "0.63\n",
      "0.118\n",
      "25\n",
      "3/5\n",
      "OpenAI\n",
      "k-means\n",
      "0.84\n",
      "0.59\n",
      "0.64\n",
      "0.066\n",
      "13\n",
      "1/5\n",
      "LLaMA-2\n",
      "k-means\n",
      "0.41\n",
      "0.09\n",
      "0.17\n",
      "0.112\n",
      "49\n",
      "1/5\n",
      "Falcon\n",
      "k-means\n",
      "0.74\n",
      "0.39\n",
      "0.48\n",
      "0.111\n",
      "34\n",
      "0/5\n",
      "DS2\n",
      "TF-IDF\n",
      "Spectral\n",
      "0.82\n",
      "0.63\n",
      "0.58\n",
      "0.028\n",
      "8\n",
      "0/5\n",
      "BERT\n",
      "AHC\n",
      "0.74\n",
      "0.58\n",
      "0.53\n",
      "0.152\n",
      "37\n",
      "0/5\n",
      "OpenAI\n",
      "AHC\n",
      "0.90\n",
      "0.79\n",
      "0.75\n",
      "0.070\n",
      "19\n",
      "3/5\n",
      "LLaMA-2\n",
      "k-means\n",
      "0.51\n",
      "0.21\n",
      "0.25\n",
      "0.137\n",
      "69\n",
      "0/5\n",
      "Falcon\n",
      "k-means++\n",
      "0.45\n",
      "0.26\n",
      "0.30\n",
      "0.170\n",
      "85\n",
      "2/5\n",
      "DS3\n",
      "TF-IDF\n",
      "Spectral\n",
      "0.35\n",
      "0.13\n",
      "0.28\n",
      "-0.002\n",
      "37\n",
      "0/5\n",
      "BERT\n",
      "k-means\n",
      "0.43\n",
      "0.25\n",
      "0.44\n",
      "0.048\n",
      "412\n",
      "0/5\n",
      "OpenAI\n",
      "k-means\n",
      "0.69\n",
      "0.52\n",
      "0.66\n",
      "0.035\n",
      "213\n",
      "3/5\n",
      "LLaMA-2\n",
      "AHC\n",
      "0.17\n",
      "0.11\n",
      "0.26\n",
      "0.025\n",
      "264\n",
      "0/5\n",
      "Falcon\n",
      "k-means\n",
      "0.26\n",
      "0.15\n",
      "0.30\n",
      "0.071 1120\n",
      "2/5\n",
      "DS4\n",
      "TF-IDF\n",
      "k-means\n",
      "0.29\n",
      "0.13\n",
      "0.48\n",
      "0.034\n",
      "17\n",
      "0/5\n",
      "BERT\n",
      "k-means\n",
      "0.35\n",
      "0.24\n",
      "0.55\n",
      "0.072\n",
      "61\n",
      "1/5\n",
      "OpenAI\n",
      "k-means\n",
      "0.38\n",
      "0.26\n",
      "0.58\n",
      "0.053\n",
      "42\n",
      "3/5\n",
      "LLaMA-2\n",
      "k-means\n",
      "0.21\n",
      "0.11\n",
      "0.40\n",
      "0.053\n",
      "88\n",
      "0/5\n",
      "Falcon\n",
      "k-means++\n",
      "0.27\n",
      "0.16\n",
      "0.48\n",
      "0.071\n",
      "92\n",
      "1/5\n",
      "DS5\n",
      "TF-IDF\n",
      "AHC\n",
      "0.31\n",
      "0.09\n",
      "0.29\n",
      "0.010\n",
      "37\n",
      "0/5\n",
      "BERT\n",
      "k-means++\n",
      "0.43\n",
      "0.27\n",
      "0.42\n",
      "0.060\n",
      "178\n",
      "2/5\n",
      "OpenAI\n",
      "Spectral\n",
      "0.45\n",
      "0.25\n",
      "0.41\n",
      "0.036\n",
      "120\n",
      "1/5\n",
      "LLaMA-2\n",
      "AHC\n",
      "0.23\n",
      "0.10\n",
      "0.23\n",
      "0.031\n",
      "263\n",
      "0/5\n",
      "Falcon\n",
      "k-means++\n",
      "0.28\n",
      "0.12\n",
      "0.25\n",
      "0.070\n",
      "359\n",
      "2/5\n",
      "DS6\n",
      "TF-IDF\n",
      "FuzzyCM\n",
      "0.51\n",
      "0.19\n",
      "0.20\n",
      "0.01\n",
      "74\n",
      "0/5\n",
      "BERT\n",
      "Spectral\n",
      "0.51\n",
      "0.32\n",
      "0.35\n",
      "0.02\n",
      "37\n",
      "1/5\n",
      "OpenAI\n",
      "FuzzyCM\n",
      "0.52\n",
      "0.23\n",
      "0.21\n",
      "0.10 1095\n",
      "3/5\n",
      "LLaMA-2\n",
      "k-means++\n",
      "0.19\n",
      "0.08\n",
      "0.63\n",
      "0.07\n",
      "518\n",
      "1/5\n",
      "Falcon\n",
      "FuzzyCM\n",
      "0.22\n",
      "-0.03\n",
      "0.21\n",
      "0.00\n",
      "930\n",
      "0/5\n",
      "17literature that suggests models trained on larger and more diverse corpora\n",
      "are more capable of capturing complex patterns in the data [43].\n",
      "Balancing computational requirements with clustering quality involves\n",
      "weighing the benefits of larger embeddings against their resource demands.\n",
      "As results show, a larger number of parameters do not necessarily lead\n",
      "to improved clustering and require significantly more computational power\n",
      "and memory. Empirical evaluation is essential to determine if the improve-\n",
      "ments justify the additional costs, considering the specific task and resource\n",
      "constraints.\n",
      "Table 9: Results of summarisation effect on text clustering for the best-performing clus-\n",
      "tering algorithms in comparison to clustering without summarisation. The ‘DS’ column\n",
      "indicates the dataset being analysed; in particular, DS1 represents the CSTR dataset,\n",
      "while DS2 denotes the SyskillWebert dataset.\n",
      "DS\n",
      "Embed.\n",
      "Version\n",
      "Best Alg.\n",
      "F1S\n",
      "ARI\n",
      "HS\n",
      "SS CHI\n",
      "DS1 BERT\n",
      "Full\n",
      "Spectral\n",
      "0.85 0.60 0.63 0.118\n",
      "25\n",
      "Summary Spectral\n",
      "0.81\n",
      "0.50\n",
      "0.56\n",
      "0.114\n",
      "24\n",
      "OpenAI\n",
      "Full\n",
      "k-means\n",
      "0.84 0.59 0.64 0.066\n",
      "13\n",
      "Summary k-means\n",
      "0.81\n",
      "0.53\n",
      "0.58\n",
      "0.061\n",
      "13\n",
      "LLaMA-2\n",
      "Full\n",
      "k-means\n",
      "0.44\n",
      "0.12\n",
      "0.21 0.099\n",
      "53\n",
      "Summary AHC\n",
      "0.47 0.16 0.30\n",
      "0.072\n",
      "24\n",
      "Falcon\n",
      "Full\n",
      "k-means\n",
      "0.74 0.39 0.48\n",
      "0.111\n",
      "34\n",
      "Summary k-means\n",
      "0.40\n",
      "0.03\n",
      "0.02 0.224\n",
      "329\n",
      "DS2 BERT\n",
      "Full\n",
      "AHC\n",
      "0.74 0.58\n",
      "0.53 0.152\n",
      "37\n",
      "Summary AHC\n",
      "0.75\n",
      "0.57 0.54\n",
      "0.089\n",
      "22\n",
      "OpenAI\n",
      "Full\n",
      "AHC\n",
      "0.90 0.79 0.75 0.070\n",
      "19\n",
      "Summary Spectral\n",
      "0.79\n",
      "0.71\n",
      "0.64\n",
      "0.054\n",
      "18\n",
      "LLaMA-2\n",
      "Full\n",
      "k-means\n",
      "0.51 0.21 0.25\n",
      "0.137\n",
      "69\n",
      "Summary FuzzyCM\n",
      "0.25\n",
      "0.04\n",
      "0.06 0.548\n",
      "603\n",
      "Falcon\n",
      "Full\n",
      "k-means++ 0.45 0.26 0.30\n",
      "0.170\n",
      "85\n",
      "Summary FuzzyCM\n",
      "0.34\n",
      "0.04\n",
      "0.07 0.269\n",
      "577\n",
      "In summary, these results provide valuable insights into the relation-\n",
      "ship between text embeddings, clustering algorithms, and dimensionality\n",
      "reduction techniques. We evaluated five embeddings and four clustering\n",
      "18Table 10: Results for the model size experiment, displaying the best performing clustering\n",
      "algorithm comparing two sizes of the LLaMA and Falcon models on the DS1 (CSTR)\n",
      "and DS2 (SyskillWebert) datasets. Model sizes are given in billions of parameters.\n",
      "Dataset Embed.\n",
      "Best Alg.\n",
      "F1S\n",
      "ARI\n",
      "HS\n",
      "SS CHI\n",
      "DS1\n",
      "LLaMA-2-7b\n",
      "k-means\n",
      "0.41\n",
      "0.09\n",
      "0.17 0.112\n",
      "50\n",
      "LLaMA-2-13b\n",
      "AHC\n",
      "0.82 0.53 0.61\n",
      "0.084\n",
      "21\n",
      "Falcon-7b\n",
      "k-means\n",
      "0.74 0.39 0.48\n",
      "0.111\n",
      "34\n",
      "Falcon-40b\n",
      "AHC\n",
      "0.46\n",
      "0.17\n",
      "0.30\n",
      "0.111\n",
      "44\n",
      "DS2\n",
      "LLaMA-2-7b\n",
      "k-means\n",
      "0.51\n",
      "0.21\n",
      "0.25 0.137\n",
      "69\n",
      "LLaMA-2-13b\n",
      "k-means++ 0.60 0.49 0.39\n",
      "0.095\n",
      "37\n",
      "Falcon-7b\n",
      "k-means++ 0.45 0.26 0.30\n",
      "0.170\n",
      "85\n",
      "Falcon-40b\n",
      "k-means++ 0.40\n",
      "0.15\n",
      "0.13 0.188\n",
      "131\n",
      "algorithms across five datasets, including the MN-DS dataset with hierar-\n",
      "chical labels. Embeddings from the OpenAI model generally outperformed\n",
      "traditional techniques such as BERT and TF-IDF, aligning with previous\n",
      "studies.\n",
      "Conversely, LLaMA-2 and Falcon models produced inferior re-\n",
      "sults compared to BERT across most metrics. The k-means algorithm was\n",
      "the most effective on most datasets, while FuzzyCM performed better on\n",
      "the Reuters dataset. Additionally, summarisation models used for dimen-\n",
      "sionality reduction did not enhance clustering performance, and the use of\n",
      "higher-dimensional models displayed mixed results.\n",
      "5. Limitations\n",
      "Despite the insights revealed in this study, the available computational\n",
      "resources constrained the experiments.\n",
      "This limitation prohibited addi-\n",
      "tional trials with summary generation, higher-dimensional models on more\n",
      "voluminous datasets, and fine-tuning of model parameters. Consequently,\n",
      "the potential benefits of summaries in large-scale text clustering have yet\n",
      "to be thoroughly evaluated. Summarisation might behave differently when\n",
      "applied at scale or with distinct prompts, potentially providing more pro-\n",
      "nounced benefits or drawbacks depending on the complexity and diversity\n",
      "of the text data.\n",
      "Moreover, embeddings computed for extra-large models like Falcon-180b\n",
      "or LLaMA-2-70b could potentially deliver substantial gains. However, prac-\n",
      "tical application is restrained by the significant computational demands as-\n",
      "1920\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "(a) LLaMA-2-7b-chat-hf.\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "20\n",
      "15\n",
      "10\n",
      "5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "(b) LLaMA-2-13b-chat-hf.\n",
      "20\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "20\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "(c) Falcon-7b.\n",
      "20\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "20\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "(d) Falcon-40b.\n",
      "Figure 1: Representation of different embeddings for the CSTR dataset, where PCA was\n",
      "used as a preliminary dimensionality reduction algorithm and t-SNE for data projection\n",
      "into a lower-dimensional space.\n",
      "sociated with larger model sizes. This limitation potentially skews the un-\n",
      "derstanding of the absolute efficacy of the embeddings computed for these\n",
      "models, as performance improvements could only be inferred up to a specific\n",
      "size.\n",
      "Testing the selected clustering algorithms with a wider range of param-\n",
      "eters could also provide additional insights into the results. However, due\n",
      "20to the aforementioned computational limitations, a breadth-first approach\n",
      "was chosen—testing several different clustering algorithms—rather than a\n",
      "depth-first approach, which would involve experimenting with more param-\n",
      "eters for each algorithm. For example, in AHC, the analysis was limited\n",
      "to using Euclidean distance with Ward linkage. Ward linkage was chosen\n",
      "because of its ability to minimize the variance within clusters, resulting\n",
      "in more compact and cohesive clusters, which is particularly beneficial for\n",
      "text data clustering. While this approach provides a comprehensive com-\n",
      "parison across different methods, future research could explore parameter\n",
      "optimization in greater depth to further refine clustering outcomes.\n",
      "These constraints prompt essential considerations for future research.\n",
      "Specifically, experiments with larger datasets and higher-dimension models\n",
      "would enable a more comprehensive and accurate understanding of the po-\n",
      "tentials and limitations of text clustering algorithms and their scalability in\n",
      "real-world applications.\n",
      "6. Conclusions\n",
      "In this study, we examined the impact that various embeddings—namely\n",
      "TF-IDF, BERT, OpenAI, LLaMA-2, and Falcon—and clustering algorithms\n",
      "have on grouping textual data. Through detailed exploration, we evalu-\n",
      "ated the efficacy of dimensionality reduction via summarisation and the\n",
      "role of model size on the clustering efficiency of various datasets. We found\n",
      "that OpenAI’s embeddings generally outperform other embeddings, with\n",
      "BERT’s performance excelling among open-source alternatives, underscor-\n",
      "ing the potential of advanced models to positively influence text clustering\n",
      "results.\n",
      "A key finding from the experiments with summarisation is that summary-\n",
      "based dimensionality reduction does not consistently improve clustering\n",
      "performance.\n",
      "This indicates that careful consideration is required when\n",
      "preprocessing text to avoid losing essential information.\n",
      "This research highlights the trade-off between improved clustering per-\n",
      "formance and the computational costs of using embeddings from larger mod-\n",
      "els. Although results indicate that an increase in model size may yield su-\n",
      "perior clustering performance, potential benefits must be weighed against\n",
      "the practicality of available computing resources.\n",
      "These findings point towards continued research focused on developing\n",
      "strategies that leverage the strengths of advanced models while mitigat-\n",
      "ing their computational demands. It is also critical to expand the scope\n",
      "21of research to include more diverse text types, which will provide a more\n",
      "comprehensive understanding of clustering dynamics across different con-\n",
      "texts. Analyzing embeddings of very recent or yet-to-be-released models\n",
      "will also be important.\n",
      "Additionally, conducting parameter searches for\n",
      "clustering algorithms is essential to optimize their performance and ensure\n",
      "robust clustering outcomes across various datasets.\n",
      "In conclusion, our findings highlight the relationship between embedding\n",
      "types, dimensionality reduction, model size, and text clustering effectiveness\n",
      "in the context of structured, formal texts. While more advanced embeddings\n",
      "such as those from OpenAI offer clear advantages, researchers and practi-\n",
      "tioners must weigh the trade-offs regarding cost, computational resources,\n",
      "and the effects of text preprocessing techniques.\n",
      "Acknowledgements\n",
      "This work was financed by the Funda¸c˜ao para a Ciˆencia e a Tecnolo-\n",
      "gia, in the framework of projects UIDB/04111/2020, UIDB/00066/2020,\n",
      "CEECINST/00002/2021/CP2788/CT0001, CEECINST/00147/2018/CP1498\n",
      "/CT0015 as well as Instituto Lus´ofono de Investiga¸c˜ao e Desenvolvimento\n",
      "(ILIND) under project COFAC/ILIND/COPELABS/1/2022.\n",
      "References\n",
      "[1] G. Salton, M. J. McGill, Introduction to Modern Information Retrieval, McGraw-\n",
      "Hill, 1983.\n",
      "[2] J. Ramos, Using TF-IDF to determine word relevance in document queries, in: Pro-\n",
      "ceedings of the First Instructional Conference on Machine Learning (ICML 2003),\n",
      "2003, p. 29–48.\n",
      "[3] T. Mikolov, K. Chen, G. S. Corrado, J. Dean, Efficient Estimation of Word Rep-\n",
      "resentations in Vector Space, in: International Conference on Learning Representa-\n",
      "tions, 2013, pp. 1–12.\n",
      "URL https://api.semanticscholar.org/CorpusID:5959482\n",
      "[4] J. Pennington, R. Socher, C. Manning, Glove: Global Vectors for Word Represen-\n",
      "tation, in: Proceedings of the 2014 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP), Vol. 14, Association for Computational Linguistics,\n",
      "2014, pp. 1532–1543. doi:10.3115/v1/D14-1162.\n",
      "[5] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: pre-training of deep bidi-\n",
      "rectional transformers for language understanding., in:\n",
      "J. Burstein, C. Doran,\n",
      "T. Solorio (Eds.), NAACL-HLT (1), Association for Computational Linguistics,\n",
      "2019, pp. 4171–4186.\n",
      "[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Nee-\n",
      "lakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,\n",
      "22T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,\n",
      "E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Rad-\n",
      "ford, I. Sutskever, D. Amodei, Language Models are Few-Shot Learners, Advances\n",
      "in Neural Information Processing Systems 33 (2020) 1877–1901.\n",
      "[7] J. MacQueen, Some methods for classification and analysis of multivariate observa-\n",
      "tions, in: Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics\n",
      "and Probability, Vol. 1, University of California Press, Berkeley, Calif, 1967, pp.\n",
      "281–297.\n",
      "[8] J. H. Ward, Hierarchical grouping to optimize an objective function, Journal of the\n",
      "American Statistical Association 58 (301) (1963) 236–244.\n",
      "[9] A. Y. Ng, M. I. Jordan, Y. Weiss, On spectral clustering: Analysis and an algo-\n",
      "rithm, in: Advances in Neural Information Processing Systems, Vol. 2, MIT Press,\n",
      "Cambridge, MA, USA, 2002, pp. 849–856.\n",
      "[10] J. C. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms,\n",
      "Plenum Press, New York, 1981.\n",
      "[11] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clustering anal-\n",
      "ysis, in: International Conference on Machine Learning, JMLR.org, 2016, pp. 478–\n",
      "487.\n",
      "[12] K. Berahmand, F. Daneshfar, A. Golzari Oskouei, M. Dorosti, M. Aghajani, An\n",
      "improved deep text clustering via local manifold of an autoencoder embedding,\n",
      "SSRN Electronic Journal (01 2022). doi:10.2139/ssrn.4295242.\n",
      "[13] A. Strehl, J. Ghosh, Cluster ensembles – a knowledge reuse framework for combining\n",
      "multiple partitions, Journal of Machine Learning Research 3 (2002) 583–617.\n",
      "[14] L. Pugachev, M. Burtsev, Short text clustering with transformers, in: Interna-\n",
      "tional Conference in Computational Linguistics and intelligent technologies, CEUR-\n",
      "WS.org, 2021, pp. 571–577. doi:10.28995/2075-7182-2021-20-571-577.\n",
      "[15] J. Zhang, P. Lertvittayakumjorn, Y. Guo, Integrating semantic knowledge to tackle\n",
      "zero-shot text classification, in: J. Burstein, C. Doran, T. Solorio (Eds.), Proceed-\n",
      "ings of the 2019 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies, Volume 1 (Long and\n",
      "Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota,\n",
      "2019, pp. 1031–1040. doi:10.18653/v1/N19-1108.\n",
      "[16] I. Keraghel, S. Morbieu, M. Nadif, Beyond Words: A Comparative Analysis of LLM\n",
      "Embeddings for Effective Clustering, Springer Nature Switzerland, 2024, Ch. 1, pp.\n",
      "205–216. doi:10.1007/978-3-031-58547-0_17.\n",
      "[17] J. Yamagishi, C. Veaux, K. MacDonald, CSTR VCTK Corpus: English multi-\n",
      "speaker corpus for CSTR voice cloning toolkit (version 0.92), online; accessed 28\n",
      "May 2024 (2019). doi:10.7488/ds/2645.\n",
      "[18] M.\n",
      "Pazzani,\n",
      "SyskillWebert\n",
      "Web\n",
      "Page\n",
      "Ratings,\n",
      "https://kdd.ics.uci.edu/\n",
      "databases/SyskillWebert/, online; accessed 28 May 2024 (1999).\n",
      "[19] T. Mitchell, Twenty newsgroups, UCI Machine Learning Repository, online; ac-\n",
      "cessed 28 May 2024 (1999). doi:10.24432/C5C323.\n",
      "[20] A. Petukhova, N. Fachada, MN-DS: A multilabeled news dataset for news articles\n",
      "hierarchical classification, Data 8 (5) (2023). doi:10.3390/data8050074.\n",
      "[21] D. Lewis, Reuters-21578 text categorization collection, online; accessed 28 May 2024\n",
      "(1997). doi:10.24432/C52G6M.\n",
      "23[22] A. Petukhova, N. Fachada, TextCL: A Python package for NLP preprocessing tasks,\n",
      "SoftwareX 19 (2022) 101122, online; accessed 28 May 2024. doi:10.1016/j.softx.\n",
      "2022.101122.\n",
      "[23] W. Uther, D. Mladeni´c, M. Ciaramita, B. Berendt, A. Ko lcz, M. Grobelnik, M. Wit-\n",
      "brock, J. Risch, S. Bohn, S. Poteet, A. Kao, L. Quach, J. Wu, E. Keogh, R. Miikku-\n",
      "lainen, P. Flener, U. Schmid, F. Zheng, G. Webb, S. Nijssen, TF–IDF, Springer US,\n",
      "2010, Ch. TF–IDF, pp. 986–987. doi:10.1007/978-0-387-30164-8_832.\n",
      "[24] Y. Zhu, R. Kiros, R. S. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, S. Fidler,\n",
      "Aligning books and movies: Towards story-like visual explanations by watching\n",
      "movies and reading books, 2015 IEEE International Conference on Computer Vision\n",
      "(ICCV) (2015) 19–27doi:10.1109/ICCV.2015.11.\n",
      "[25] P. Przyby la, P. Borkowski, K. Kaczy´nski, Wikipedia complete citation corpus, on-\n",
      "line; accessed 28 May 2024 (Jul 2022). doi:10.5281/zenodo.6539054.\n",
      "[26] R. Greene, T. Sanders, L. Weng, A. Neelakantan, New and improved embedding\n",
      "model, https://openai.com/blog/new-and-improved-embedding-model, online;\n",
      "accessed 28 May 2024 (2023).\n",
      "[27] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah,\n",
      "´Etienne Goffinet, D. Hesslow, J. Launay, Q. Malartic, D. Mazzotta, B. Noune,\n",
      "B. Pannier, G. Penedo, The Falcon series of open language models (2023). arXiv:\n",
      "2311.16867.\n",
      "[28] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,\n",
      "S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen,\n",
      "G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami,\n",
      "N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez,\n",
      "M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee,\n",
      "D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog,\n",
      "Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\n",
      "Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan,\n",
      "P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez,\n",
      "R. Stojnic, S. Edunov, T. Scialom, Llama 2: Open foundation and fine-tuned chat\n",
      "models (2023). arXiv:2307.09288, doi:10.48550/arXiv.2307.09288.\n",
      "[29] Hugging face, https://huggingface.co/, online; accessed 28 May 2024 (2024).\n",
      "[30] D. Arthur, S. Vassilvitskii, et al., k-means++: The advantages of careful seeding, in:\n",
      "Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms,\n",
      "Vol. 7 of SODA ’07, SIAM, 2007, pp. 1027–1035.\n",
      "[31] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,\n",
      "M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\n",
      "D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine learn-\n",
      "ing in Python, Journal of Machine Learning Research 12 (2011) 2825–2830.\n",
      "[32] J. Warner, J. Sexauer, scikit fuzzy, twmeggs, alexsavio, A. Unnikrishnan, G. Cas-\n",
      "tel˜ao, F. A. Pontes, T. Uelwer, pd2f, laurazh, F. Batista, alexbuy, W. V. den Broeck,\n",
      "W. Song, T. G. Badger, R. A. M. P´erez, J. F. Power, H. Mishra, G. O. Trullols,\n",
      "A. H¨orteborn, 99991, Jdwarner/scikit-fuzzy: Scikit-fuzzy version 0.4.2, online; ac-\n",
      "cessed 28 May 2024 (Nov 2019). doi:10.5281/zenodo.3541386.\n",
      "[33] N. Chinchor, MUC-4 evaluation metrics, in: Proceedings of the 4th Conference\n",
      "on Message Understanding, MUC4 ’92, Association for Computational Linguistics,\n",
      "24USA, 1992, p. 22–29. doi:10.3115/1072064.1072067.\n",
      "URL https://doi.org/10.3115/1072064.1072067\n",
      "[34] D. Steinley, Properties of the hubert-arabie adjusted rand index, Psychological\n",
      "methods 9 (2004) 386–396. doi:10.1037/1082-989X.9.3.386.\n",
      "[35] A. Rosenberg, J. Hirschberg, V-measure: A conditional entropy-based external clus-\n",
      "ter evaluation measure, in: J. Eisner (Ed.), Proceedings of the 2007 Joint Conference\n",
      "on Empirical Methods in Natural Language Processing and Computational Natural\n",
      "Language Learning (EMNLP-CoNLL), Association for Computational Linguistics,\n",
      "Prague, Czech Republic, 2007, pp. 410–420.\n",
      "URL https://aclanthology.org/D07-1043\n",
      "[36] P. Rousseeuw, Silhouettes: A graphical aid to the interpretation and validation\n",
      "of cluster analysis, Journal of Computational and Applied Mathematics 20 (1987)\n",
      "53–65. doi:10.1016/0377-0427(87)90125-7.\n",
      "[37] T. Cali´nski, H. JA, A dendrite method for cluster analysis, Communications in\n",
      "Statistics - Theory and Methods 3 (1974) 1–27. doi:10.1080/03610927408827101.\n",
      "[38] D. Miller, Leveraging BERT for extractive text summarization on lectures (2019).\n",
      "arXiv:1906.04165.\n",
      "[39] gpt-3-5-turbo,\n",
      "https://platform.openai.com/docs/models/gpt-3-5-turbo/,\n",
      "online; accessed 28 May 2024 (2024).\n",
      "[40] falcon-7b, https://huggingface.co/tiiuae/falcon-7b/, online; accessed 28 May\n",
      "2024 (2024).\n",
      "[41] Llama-2-7b-chat-hf,\n",
      "https://huggingface.co/meta-llama/\n",
      "Llama-2-7b-chat-hf/, online; accessed 28 May 2024 (2024).\n",
      "[42] L. Van der Maaten, G. Hinton, Visualizing data using t-SNE., Journal of machine\n",
      "learning research 9 (11) (2008).\n",
      "[43] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Barnes, A. S.\n",
      "Mian, A comprehensive overview of large language models, ArXiv abs/2307.06435\n",
      "(2023).\n",
      "URL https://api.semanticscholar.org/CorpusID:259847443\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(f\"총 {len(texts)}개의 텍스트 청크(chunk)가 생성되었습니다.\")\n",
    "print(texts[0])  # 첫 번째 청크 일부 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9abc5daa-90c7-4492-8e6c-330870d790b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03792f60-e5a0-467c-8339-ea41a7fbafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"***REMOVED***proj-QeJnJHoxc-lPEocb4VXFjRWIqzmqekfp4y2FZBIctsXBqi-Lt8TCIjB6CJ9nCiZfjzcQofkjDhT3BlbkFJ1mLNkK9ZeQNrmAUlYBTQNzvavRYNRu5Ij-tnQdMruHO940WH_bK5C88fNV4a49Tii7gI-ne4QA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ab5d295-0014-4103-bf23-87fdd568c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.72)\n",
      "Requirement already satisfied: langchain-openai in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (1.97.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44ece569-c906-48dc-9f61-343ba14648f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'convert_to_openai_data_block' from 'langchain_core.messages' (/Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages/langchain_core/messages/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[32m      3\u001b[39m embeddings = OpenAIEmbeddings()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/langchain_openai/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/langchain_openai/chat_models/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mazure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mChatOpenAI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAzureChatOpenAI\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/langchain_openai/chat_models/azure.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field, SecretStr, model_validator\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal, Self\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[32m     23\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     26\u001b[39m _BM = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_BM\u001b[39m\u001b[33m\"\u001b[39m, bound=BaseModel)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:47\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModelInput\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     BaseChatModel,\n\u001b[32m     43\u001b[39m     LangSmithParams,\n\u001b[32m     44\u001b[39m     agenerate_from_stream,\n\u001b[32m     45\u001b[39m     generate_from_stream,\n\u001b[32m     46\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     AIMessage,\n\u001b[32m     49\u001b[39m     AIMessageChunk,\n\u001b[32m     50\u001b[39m     BaseMessage,\n\u001b[32m     51\u001b[39m     BaseMessageChunk,\n\u001b[32m     52\u001b[39m     ChatMessage,\n\u001b[32m     53\u001b[39m     ChatMessageChunk,\n\u001b[32m     54\u001b[39m     FunctionMessage,\n\u001b[32m     55\u001b[39m     FunctionMessageChunk,\n\u001b[32m     56\u001b[39m     HumanMessage,\n\u001b[32m     57\u001b[39m     HumanMessageChunk,\n\u001b[32m     58\u001b[39m     InvalidToolCall,\n\u001b[32m     59\u001b[39m     SystemMessage,\n\u001b[32m     60\u001b[39m     SystemMessageChunk,\n\u001b[32m     61\u001b[39m     ToolCall,\n\u001b[32m     62\u001b[39m     ToolMessage,\n\u001b[32m     63\u001b[39m     ToolMessageChunk,\n\u001b[32m     64\u001b[39m     convert_to_openai_data_block,\n\u001b[32m     65\u001b[39m     is_data_content_block,\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     InputTokenDetails,\n\u001b[32m     69\u001b[39m     OutputTokenDetails,\n\u001b[32m     70\u001b[39m     UsageMetadata,\n\u001b[32m     71\u001b[39m )\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tool_call_chunk\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'convert_to_openai_data_block' from 'langchain_core.messages' (/Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages/langchain_core/messages/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009131a-bc02-4007-a837-f572c630054d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
