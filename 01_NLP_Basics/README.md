# ðŸ“˜ 01_NLP_Basics

This folder introduces fundamental preprocessing techniques used in Natural Language Processing (NLP).  
Each notebook focuses on how raw text can be converted into numeric formats that machine learning models can understand.

---

## ðŸ“ Included Notebooks

| Notebook Filename                      | Description |
|----------------------------------------|-------------|
| `Tokenizer_NLP_Preprocessing.ipynb`    | Overview of text preprocessing in NLP: cleaning, stopwords, case normalization |
| `Tokenization_Basics.ipynb`            | Basic word and sentence tokenization using NLTK and Keras |
| `Integer_Encoding.ipynb`               | Converting words to integer IDs using tokenizers |
| `Text_to_Sequence_and_Padding.ipynb`   | Padding sequences to equal lengths for model training |
| `Bag_of_Words.ipynb`                   | Introduction to the Bag-of-Words model and sparse vector representations |

---

## ðŸ“Œ Key Concepts Covered

- What is tokenization and why it matters
- How to map text to integers
- Preparing uniform-length sequences
- Understanding sparse vector representations (BoW)
- Differences between word-level and character-level encoding

---

## ðŸ“¦ Tools Used

- Python 3
- NLTK
- TensorFlow / Keras (for tokenizers and padding)
- NumPy

---

## ðŸ§  Recommended Order

If you're following this as a course or structured study, the recommended order is:

1. `Tokenizer_NLP_Preprocessing.ipynb`
2. `Tokenization_Basics.ipynb`
3. `Integer_Encoding.ipynb`
4. `Text_to_Sequence_and_Padding.ipynb`
5. `Bag_of_Words.ipynb`

---

> All notebooks in this section are based on online course content and personal experimentation while studying NLP basics.
.
