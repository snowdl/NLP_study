{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60d3efaa-2355-4238-a38f-f8d6e2fd192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most relevant document using sentence embeddings → Ask a question using that context → Generate an answer with GPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75080bbc-7f23-4499-8986-fd0833994419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a2d61cf-06c5-48ac-a721-fc473112e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation: list of documents for retrieval\n",
    "documents = [\n",
    "    \"Transformers are a type of neural network architecture.\",\n",
    "    \"BERT stands for Bidirectional Encoder Representations from Transformers.\",\n",
    "    \"GPT models are good at generating human-like text.\",\n",
    "    \"Retrieval-Augmented Generation combines search and generation.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dee998b7-956f-4bc4-940b-49e0f7c5d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained sentence embedding model (MiniLM) for converting sentences into dense vector representations\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79981f40-70e3-4eae-9713-1d0b5896c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vector embeddings for each document to capture their semantic meaning\n",
    "doc_embeddings = embedder.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1715bc9-3518-4694-a852-717fa79d4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user’s query or question to find the most relevant document\n",
    "query = \"What does BERT mean?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1393956-c72b-4abf-b970-34354a7adad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the user’s query into a vector embedding for similarity comparison\n",
    "query_embedding = embedder.encode([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6877e240-c1bf-422f-bea1-fce7dd12e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity scores between the query embedding and each document embedding\n",
    "similarities = cosine_similarity(query_embedding, doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "deeeed44-70eb-4c06-a13e-47d7bedcf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the index of the document most similar to the query\n",
    "#argmax()  returns the index (position) of the largest value (highest similarity score) in the array.\n",
    "top_doc_idx = similarities.argmax()\n",
    "# In other words, compare the query embedding with all document embeddings using cosine similarity\n",
    "# and return the index of the closest (most similar) document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eaed4e9-b47e-4c32-a046-543dc49e23be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GPT-2 text generation pipeline to generate answers based on the given prompt\n",
    "# This will generate a text response using GPT-2 based on the context and question provided.\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94846f3e-9674-478a-bdfa-676934a9d52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt first retrieves relevant documents from a large collection based on a query, then uses a language generation model (like GPT) to produce answers grounded in the retrieved information.\\n\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RAG:framework that combines information retrieval and text generation.\n",
    "\"\"\"\n",
    "It first retrieves relevant documents from a large collection based on a query, then uses a language generation model (like GPT) to produce answers grounded in the retrieved information.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6df04188-3d4f-4fd4-bfb6-600348c2f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most relevant document as context based on similarity\n",
    "context = documents[top_doc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f1da527-e775-4c47-a9e9-699aefb1b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt combining the context and the user’s question to guide text generation\n",
    "prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0db55ed3-2bfe-4466-8ce6-99b08341a98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate a text response using GPT-2 based on the constructed prompt\n",
    "result = generator(prompt, max_length=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adbe24e6-f704-447a-9733-e197773e99e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: BERT stands for Bidirectional Encoder Representations from Transformers.\n",
      "Question: What does BERT mean?\n",
      "Answer: BERT is a data center for storing data in a persistent database. It allows an attacker to easily access any part of a database without decrypting it.\n",
      "Question: How do you use it on a machine where you know the key size is 1024 bytes?\n",
      "Answer: BERT is a data center for storing data in a persistent database. It allows an\n"
     ]
    }
   ],
   "source": [
    "# Print the generated answer text from GPT-2 output\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682cf0b-3cb0-49c1-8241-7ccd6cfa3f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
