# 📙 03_nlp_architectures

This folder contains study materials focused on various NLP model architectures.  
It covers fundamental neural network structures used in NLP, from traditional RNNs to modern Transformer models.

---

## 📝 Included Topics

- Recurrent Neural Networks (RNNs), LSTM, and GRU
- Transformer architecture basics
- Sequence modeling techniques
- Encoder-Decoder frameworks
- Attention mechanisms overview

---

## 📌 Key Concepts Covered

- How sequential data is processed with RNNs and variants
- Limitations of traditional models and motivation for Transformers
- Understanding the self-attention mechanism
- Architectural components of modern NLP models
- Practical implementations and sample codes

---

## 📦 Tools & Libraries

- Python 3
- TensorFlow / Keras or PyTorch
- Numpy
- Jupyter Notebook

---

## 🧠 Recommended Study Path

1. Start with RNN basics and understand sequence modeling
2. Explore advanced RNN variants like LSTM and GRU
3. Study the Transformer model in detail
4. Experiment with encoder-decoder architectures
5. Review attention mechanisms and their applications

---

> This folder builds a solid understanding of NLP architectures, essential for developing and fine-tuning NLP models.

