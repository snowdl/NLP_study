{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a642b429-550c-4408-8c0d-3250c800a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LoRA fine-tuning example using roberta-base on BANKING77\n",
    "# With seed setting, warmup, and weight decay for stability\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db15231e-4ab9-43f3-9c80-47f50873b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding, set_seed)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load as load_metric\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0382add9-a4b0-4af1-990e-37d1262558b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Configuration\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d492124a-e0e1-4d65-8ee2-7f1475cd5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)  # fix random seed for reproducibility\n",
    "\n",
    "MODEL = \"roberta-base\"   # backbone model\n",
    "NUM_LABELS = 77          # number of classes (BANKING77 dataset)\n",
    "EPOCHS = 3               # number of epochs\n",
    "LR = 2e-4                 # learning rate\n",
    "BTR, BTE = 16, 32        # batch sizes (train/eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d240e900-9bef-49b0-918e-5f5ba6ab7843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since PolyAI/banking77 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/jessicahong/.cache/huggingface/datasets/PolyAI___banking77/default/1.1.0/17ffc2ed47c2ed928bee64127ff1dbc97204cb974c2f980becae7c864007aed9 (last modified on Sat Aug 30 16:35:53 2025).\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Load dataset & tokenizer\n",
    "# ------------------------------\n",
    "ds = load_dataset(\"PolyAI/banking77\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3f612cf-b722-4972-a481-b689fbed11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize function\n",
    "def tok_fn(batch):\n",
    "    return tok(batch[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e23042d0-5866-4622-8c21-f8d13847f212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491b9b3d7e8d481c99621bde051404a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ea1029e0244dca9ed6a841aa6e1322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove original text column after tokenization\n",
    "ds_tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c186566-a087-4e73-8588-9e0a9646350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Metrics\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca2e4a89-7125-4ee6-adac-1a312f50efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = load_metric(\"accuracy\")\n",
    "f1 = load_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaeae0a9-830f-4816-b455-889687812d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=p.label_ids)[\"accuracy\"],\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=p.label_ids, average=\"macro\")[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5058588c-c8b6-4f97-9b70-adaf32fd9325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Model + LoRA\n",
    "# ------------------------------\n",
    "base = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c64e595-9156-45f5-b9e4-86c3a7888513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA target modules for RoBERTa (query, key, value, dense)\n",
    "targets = [\"query\", \"key\", \"value\", \"dense\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83d5506e-2784-4e8b-a403-bebceb6341db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r=16,              \n",
    "    lora_alpha=64,     \n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"query\",\"key\",\"value\",\"dense\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64c7adc5-d22d-4fba-96f7-38222fd3e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# TrainingArguments\n",
    "# ------------------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./out_lora_roberta\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BTR,\n",
    "    per_device_eval_batch_size=BTE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    report_to=\"none\",       # disable WandB/Hub logging\n",
    "    warmup_ratio=0.06,      # small warmup for stability\n",
    "    weight_decay=0.01       # add weight decay for generalization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4946711a-2ee0-4ee8-8aac-0cb7d45092f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/xtl4b0cx1cs9zrr9n5y814_h0000gn/T/ipykernel_63601/1085237875.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Trainer\n",
    "# ------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_tok[\"train\"],\n",
    "    eval_dataset=ds_tok[\"test\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3754e3a-650c-4358-95a9-0a1dea2551f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicahong/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1878' max='1878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1878/1878 05:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicahong/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/jessicahong/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/jessicahong/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='97' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [97/97 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25878748297691345, 'eval_accuracy': 0.9292207792207792, 'eval_macro_f1': 0.929054045685224, 'eval_runtime': 12.9636, 'eval_samples_per_second': 237.588, 'eval_steps_per_second': 7.482, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Run training + evaluation\n",
    "# ------------------------------\n",
    "trainer.train()\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a1ce49f-9750-4d08-aee1-09c8bf52da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# save LoRA adapter\n",
    "# ------------------------------\n",
    "trainer.model.save_pretrained(\"./out_lora_roberta_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aae52fdc-133c-41df-a233-ca5b2b8a4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Quick Inference Test ==========\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8735a769-ba1b-4960-b30b-c73716730701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since PolyAI/banking77 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/jessicahong/.cache/huggingface/datasets/PolyAI___banking77/default/1.1.0/17ffc2ed47c2ed928bee64127ff1dbc97204cb974c2f980becae7c864007aed9 (last modified on Sat Aug 30 16:46:43 2025).\n"
     ]
    }
   ],
   "source": [
    "# 1) Tokenizer + label names\n",
    "tok = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "ds = load_dataset(\"PolyAI/banking77\")\n",
    "label_names = ds[\"train\"].features[\"label\"].names  # id -> name mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25b1916b-0932-44a1-ad03-b4bc651e0d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2) Base + adapter load\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_names))\n",
    "model = PeftModel.from_pretrained(base, \"./out_lora_roberta_adapter\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94606a45-2239-49e1-ada5-047187bcfd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred id: 41 | label: lost_or_stolen_card\n"
     ]
    }
   ],
   "source": [
    "# 3) Prediction\n",
    "text = \"I lost my card and need help.\"\n",
    "inputs = tok(text, return_tensors=\"pt\", truncation=True)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "pred_id = int(logits.argmax(-1))\n",
    "print(\"pred id:\", pred_id, \"| label:\", label_names[pred_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5db027-9131-45e3-aa64-84865df8495e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
