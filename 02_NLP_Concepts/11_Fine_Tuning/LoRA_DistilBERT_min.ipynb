{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca4c8b1-19c8-4358-9f18-62ad1b64aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding, set_seed)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load as load_metric\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0ffe2d-bc21-487f-8d93-de37fe60a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Config\n",
    "set_seed(42)  # reproducibility\n",
    "\n",
    "MODEL = \"distilbert-base-uncased\"\n",
    "NUM_LABELS = 77              # BANKING77\n",
    "EPOCHS = 3                  \n",
    "LR = 2e-4\n",
    "BTR, BTE = 16, 32            # train/eval batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca73b2d-82f2-47ec-8487-5aeb7ac05e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since PolyAI/banking77 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/jessicahong/.cache/huggingface/datasets/PolyAI___banking77/default/1.1.0/17ffc2ed47c2ed928bee64127ff1dbc97204cb974c2f980becae7c864007aed9 (last modified on Sat Aug 30 18:07:46 2025).\n"
     ]
    }
   ],
   "source": [
    "# 3) Dataset & tokenizer\n",
    "ds = load_dataset(\"PolyAI/banking77\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5653026b-b105-4749-a5c6-b307d29cbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_fn(batch):\n",
    "    # truncation=True ensures consistent sequence length\n",
    "    return tok(batch[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76519251-6e42-4054-8507-61ea99abd031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370e76b9bc8049d9a334699ca82d3007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove_columns=[\"text\"] avoids \"too many dimensions 'str'\" errors later\n",
    "ds_tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2f6f69-7caf-4777-83d7-135f2db29a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Metrics\n",
    "acc = load_metric(\"accuracy\")\n",
    "f1  = load_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd9920ab-0a3e-4ec3-b46a-e5a031c2d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=p.label_ids)[\"accuracy\"],\n",
    "        \"macro_f1\": f1.compute(predictions=preds, references=p.label_ids, average=\"macro\")[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e0f3bb4-271f-427a-b32a-17a655443e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5) Model + LoRA (DistilBERT module names!)\n",
    "base = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=NUM_LABELS)\n",
    "\n",
    "targets = [\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]  # DistilBERT attention projections\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,                 \n",
    "    lora_alpha=64,      \n",
    "    lora_dropout=0.05,\n",
    "    target_modules=targets,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c804b8b8-b379-4064-8aeb-fe035a8d0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./out_lora_distilbert\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BTR,\n",
    "    per_device_eval_batch_size=BTE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    report_to=\"none\",      # disable external loggers\n",
    "    warmup_ratio=0.06,     # tiny warmup helps stability\n",
    "    weight_decay=0.01      # mild regularization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07301760-9cff-4fc2-99db-7809279826ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/xtl4b0cx1cs9zrr9n5y814_h0000gn/T/ipykernel_63875/4109794826.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/jessicahong/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1878' max='1878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1878/1878 02:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.303800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.414900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicahong/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/jessicahong/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/jessicahong/.pyenv/versions/3.10.12/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='97' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [97/97 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37591156363487244, 'eval_accuracy': 0.8925324675324675, 'eval_macro_f1': 0.8924499805083215, 'eval_runtime': 6.886, 'eval_samples_per_second': 447.288, 'eval_steps_per_second': 14.087, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# 7) Trainer + train/eval\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_tok[\"train\"],\n",
    "    eval_dataset=ds_tok[\"test\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "# lora_distilbert_min.py  (END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbd26d-ccf1-4929-99f1-3540d068894a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
