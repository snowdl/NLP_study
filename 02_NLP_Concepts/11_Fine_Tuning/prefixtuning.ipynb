{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c16b5e87-4156-4fa5-8e3e-341957da784d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "# Imports & Device setup\n",
    "# Basic Python libraries\n",
    "import random, numpy as np, torch\n",
    "from datasets import load_dataset                 # Hugging Face datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # Pretrained tokenizer & model\n",
    "from peft import PrefixTuningConfig, get_peft_model, TaskType   # PEFT for prefix-tuning\n",
    "\n",
    "# Fix the random seed for reproducibility (same results every run)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Choose device automatically:\n",
    "# 1) Apple Silicon GPU (M1/M2/M3) → \"mps\"\n",
    "# 2) NVIDIA GPU → \"cuda\"\n",
    "# 3) Otherwise fallback → \"cpu\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "856aabfb-085d-4a08-b6b3-08a3f3ca0fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since PolyAI/banking77 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/jessicahong/.cache/huggingface/datasets/PolyAI___banking77/default/1.1.0/17ffc2ed47c2ed928bee64127ff1dbc97204cb974c2f980becae7c864007aed9 (last modified on Sat Aug 30 22:38:57 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: 77 | train/test: 10003 / 3080\n"
     ]
    }
   ],
   "source": [
    "# Dataset / Model / Prefix setup\n",
    "# Block 2: Dataset / Model / Prefix setup\n",
    "MODEL = \"t5-small\"   # Small T5 model (fast to train, good for practice)\n",
    "\n",
    "# 1) Load dataset\n",
    "# \"PolyAI/banking77\" → intent classification dataset with 77 classes\n",
    "ds = load_dataset(\"PolyAI/banking77\")\n",
    "\n",
    "# Extract label names (list of 77 intent categories)\n",
    "label_names = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "# Print dataset info (number of labels, train set size, test set size)\n",
    "print(\"labels:\", len(label_names), \"| train/test:\", len(ds[\"train\"]), \"/\", len(ds[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb368cec-7740-4313-9cae-99ccfa184f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load tokenizer and model\n",
    "\n",
    "# Load the tokenizer for the chosen model (T5-small).\n",
    "# The tokenizer converts text → token IDs (numbers) and back.\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Load the pretrained T5-small sequence-to-sequence model.\n",
    "# Move the model to the selected device (MPS, CUDA, or CPU).\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(MODEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48781bb4-572e-4fce-b97b-33d054b517c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 98,304 || all params: 60,604,928 || trainable%: 0.1622\n"
     ]
    }
   ],
   "source": [
    "peft_cfg = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    num_virtual_tokens=16,  \n",
    ")\n",
    "model = get_peft_model(base, peft_cfg).to(device)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b7bf2d2-f38f-4f1c-b2e4-8061e1994da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cfa725c-0484-40b8-8ff8-0c28785f588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def preprocess(batch):\n",
    "    # Add a task-specific prefix to each input sentence.\n",
    "    # Example: \"classify intent: How do I reset my password?\"\n",
    "    inputs = [f\"classify intent: {t}\" for t in batch[\"text\"]]\n",
    "\n",
    "    # Convert the numeric labels (0–76) into their string names.\n",
    "    targets = [label_names[i] for i in batch[\"label\"]]\n",
    "\n",
    "    # Tokenize the input sentences (truncate if too long).\n",
    "    enc_in = tok(inputs, truncation=True)\n",
    "\n",
    "    # Tokenize the target labels as text (using text_target).\n",
    "    lab = tok(text_target=targets, truncation=True)\n",
    "\n",
    "    # Store tokenized labels in the encoding dictionary.\n",
    "    enc_in[\"labels\"] = lab[\"input_ids\"]\n",
    "\n",
    "    return enc_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86b5d6f6-bb42-4562-bb1b-b4b16cc66031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████| 10003/10003 [00:00<00:00, 40508.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove the original raw \"text\" and \"label\" columns (keep only tokenized data).\n",
    "ds_tok = ds.map(preprocess, batched=True, remove_columns=[\"text\", \"label\"])\n",
    "\n",
    "# Set format to PyTorch tensors so we can use DataLoader directly.\n",
    "ds_tok.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20358c2f-e494-42d3-8711-4593edf52ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function: pads inputs/labels in a batch to the same length\n",
    "# and replaces padding tokens in labels with -100 (so they are ignored in the loss).\n",
    "def collate_fn(features):\n",
    "    # Extract input IDs and attention masks\n",
    "    ins = [{\"input_ids\": f[\"input_ids\"], \"attention_mask\": f[\"attention_mask\"]} for f in features]\n",
    "\n",
    "    # Extract labels (already tokenized)\n",
    "    labs = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "    # Pad inputs dynamically so all sequences in the batch have the same length\n",
    "    batch = tok.pad(ins, return_tensors=\"pt\")\n",
    "\n",
    "    # Pad labels as well\n",
    "    lab = tok.pad(labs, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # Replace padding tokens in labels with -100 (PyTorch ignores -100 in loss computation)\n",
    "    lab[lab == tok.pad_token_id] = -100\n",
    "    batch[\"labels\"] = lab\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74368ede-ed48-4063-9882-f1ef4fcc18dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches: 834 257\n"
     ]
    }
   ],
   "source": [
    "# Build DataLoaders (train with shuffle, test without shuffle)\n",
    "train_dl = DataLoader(ds_tok[\"train\"], batch_size=12, shuffle=True, collate_fn=collate_fn)\n",
    "test_dl  = DataLoader(ds_tok[\"test\"],  batch_size=12, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Print number of mini-batches for train and test\n",
    "print(\"batches:\", len(train_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc08f71e-7d85-4faa-a13a-ab3033a0b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PrefixEncoder(\n",
       "      (embedding): Embedding(16, 6144)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(32128, 512)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick sanity check with a single batch to catch errors early\n",
    "\n",
    "model.train()  # set model to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d798946e-c8f8-4b68-ac02-5e9fed6f20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#블록 4 — 학습(짧게) + 빠른 평가(빨라요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2accda21-275c-474d-bcc4-ea169ba63352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep1] 100/834 loss 6.7220\n",
      "[ep1] 200/834 loss 6.6220\n",
      "[ep1] 300/834 loss 6.5329\n",
      "[ep1] 400/834 loss 6.4487\n",
      "[ep1] 500/834 loss 6.3615\n",
      "[ep1] 600/834 loss 6.2685\n",
      "[ep1] 700/834 loss 6.1786\n",
      "[ep1] 800/834 loss 6.0903\n",
      "[ep1] avg loss 6.0624\n",
      "[ep2] 100/834 loss 5.2467\n",
      "[ep2] 200/834 loss 5.1744\n",
      "[ep2] 300/834 loss 5.0974\n",
      "[ep2] 400/834 loss 5.0332\n",
      "[ep2] 500/834 loss 4.9651\n",
      "[ep2] 600/834 loss 4.8883\n",
      "[ep2] 700/834 loss 4.8088\n",
      "[ep2] 800/834 loss 4.7343\n",
      "[ep2] avg loss 4.7063\n",
      "✅ training done\n"
     ]
    }
   ],
   "source": [
    "# ===== 학습 =====\n",
    "trainable = [p for p in model.parameters() if p.requires_grad]  # Prefix 파라미터만 학습\n",
    "optim = torch.optim.AdamW(trainable, lr=5e-4)\n",
    "\n",
    "epochs, log_every, grad_clip = 2, 100, 1.0\n",
    "model.train()\n",
    "for ep in range(1, epochs+1):\n",
    "    running = 0.0\n",
    "    for step, batch in enumerate(train_dl, 1):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        loss = model(**batch).loss\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(trainable, grad_clip)\n",
    "        optim.step()\n",
    "        running += loss.item()\n",
    "        if step % log_every == 0:\n",
    "            print(f\"[ep{ep}] {step}/{len(train_dl)} loss {running/step:.4f}\")\n",
    "    print(f\"[ep{ep}] avg loss {running/len(train_dl):.4f}\")\n",
    "print(\"✅ training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8c8f923-13a3-408d-8bb2-a526d7bf0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 빠른 평가 (라벨 목록을 프롬프트에 같이 줌 + 퍼지 매칭, 정말 빠름) =====\n",
    "import difflib\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return s.strip().lower()\n",
    "\n",
    "labels_norm = [_norm(n) for n in label_names]\n",
    "name2id_norm = {ln:i for i, ln in enumerate(labels_norm)}\n",
    "options_str = \"; \".join(label_names)  # T5-small 입력 길이 내에서 OK\n",
    "\n",
    "model.eval()\n",
    "pred_ids, ref_ids = [], list(ds[\"test\"][\"label\"])\n",
    "texts = ds[\"test\"][\"text\"]\n",
    "bs = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c070df7-1185-41a2-8ceb-8d73c0488f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(0, len(texts), bs):\n",
    "        prompts = [f\"classify intent from options [{options_str}]. answer with label only: {t}\"\n",
    "                   for t in texts[i:i+bs]]\n",
    "        enc = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=6,\n",
    "            num_beams=4,           # 살짝 신중\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        outs = [_norm(tok.decode(g, skip_special_tokens=True)) for g in gen]\n",
    "        for o in outs:\n",
    "            if o in name2id_norm:\n",
    "                pred_ids.append(name2id_norm[o])\n",
    "            else:\n",
    "                # 컷오프 없이 가장 가까운 라벨로 강제 매핑(= 샘플 100% 사용)\n",
    "                m = difflib.get_close_matches(o, labels_norm, n=1, cutoff=0.0)\n",
    "                pred_ids.append(name2id_norm[m[0]] if m else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd7e61b6-a4ed-482f-8a98-51876c7e12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAST eval | accuracy=0.0130 (macro-F1는 원하면 나중에 추가)\n",
      "▶ How do I locate my card?\n",
      "   pred: cash_withdrawal_charge | true: card_arrival\n",
      "▶ I still have not received my new card, I ordered over a week ago.\n",
      "   pred: cash_withdrawal_charge | true: card_arrival\n",
      "▶ I ordered a card but it has not arrived. Help please!\n",
      "   pred: cash_withdrawal_charge | true: card_arrival\n",
      "▶ Is there a way to know when my card will arrive?\n",
      "   pred: cash_withdrawal_charge | true: card_arrival\n",
      "▶ My card has not arrived yet.\n",
      "   pred: cash_withdrawal_charge | true: card_arrival\n"
     ]
    }
   ],
   "source": [
    "# 간단 정확도(왕초보용: 파이썬 계산)\n",
    "refs = ref_ids\n",
    "acc = sum(int(p==r) for p,r in zip(pred_ids, refs)) / len(refs)\n",
    "print(f\"✅ FAST eval | accuracy={acc:.4f} (macro-F1는 원하면 나중에 추가)\")\n",
    "\n",
    "# 샘플 5개만 보기\n",
    "for k in range(5):\n",
    "    print(\"▶\", texts[k][:80])\n",
    "    print(\"   pred:\", label_names[pred_ids[k]], \"| true:\", label_names[refs[k]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8c82b-128f-46a4-92f4-941eb210d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Training Loop =====\n",
    "\n",
    "# Collect only trainable parameters (Prefix-tuning parameters, not the whole model)\n",
    "trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Optimizer: AdamW (commonly used for Transformers)\n",
    "optim = torch.optim.AdamW(trainable, lr=5e-4)\n",
    "\n",
    "# Training configuration\n",
    "epochs = 2           # number of passes over the training set\n",
    "log_every = 100      # print loss every 100 steps\n",
    "grad_clip = 1.0      # gradient clipping for stability\n",
    "\n",
    "model.train()  # set model to training mode\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    running = 0.0  # track cumulative loss\n",
    "    for step, batch in enumerate(train_dl, 1):\n",
    "        # Move batch to device (MPS/GPU/CPU)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass → compute loss\n",
    "        loss = model(**batch).loss\n",
    "\n",
    "        # Backward pass\n",
    "        optim.zero_grad()                   # reset gradients\n",
    "        loss.backward()                     # compute gradients\n",
    "        torch.nn.utils.clip_grad_norm_(trainable, grad_clip)  # prevent exploding gradients\n",
    "        optim.step()                        # update parameters\n",
    "\n",
    "        # Track average loss\n",
    "        running += loss.item()\n",
    "        if step % log_every == 0:\n",
    "            print(f\"[ep{ep}] {step}/{len(train_dl)}  loss {running/step:.4f}\")\n",
    "\n",
    "    # Print average loss per epoch\n",
    "    print(f\"[ep{ep}] avg loss {running/len(train_dl):.4f}\")\n",
    "\n",
    "print(\"✅ training done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (t5prefix)",
   "language": "python",
   "name": "t5prefix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
