{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a90a02-711a-4908-b7ff-fb5c2c3d1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 활성화 함수 비교 실험 (Jupyter Notebook)\n",
    "# ---------------------------------------------------------------\n",
    "# 목표: ReLU / Sigmoid / Tanh / GELU / SiLU(Swish) / LeakyReLU 활성화 함수를\n",
    "# 간단한 NLP 분류 문제에서 비교해본다.\n",
    "#\n",
    "# 아이디어:\n",
    "# - GPT 내부 활성화 함수는 바꿀 수 없으므로, 우리가 직접 만든 작은 분류 모델에서 실험.\n",
    "# - 같은 데이터셋, 같은 모델 구조, 같은 학습 설정에서 활성화만 바꿔 성능 차이를 본다.\n",
    "# - 텍스트 → TF-IDF → 작은 MLP → 분류 정확도/ROC AUC 등으로 비교.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69066eec-f979-4da2-913e-b9dfa26ca87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "980389e7-6ead-481d-9ddf-f501964bcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca6bdad9-f1f6-4082-9113-b761a1aaab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd7e0ffa-c282-448b-ba87-3b13aec9ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "DEVICE_STR = str(DEVICE)  # <-- fixed: now always defined as a string (\"cuda\", \"mps\" or \"cpu\")\n",
    "print(\"Using device:\", DEVICE_STR)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a46e1731-af46-49f3-a4ee-45604cbc3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac4b16ab-e727-4701-bf48-7a7e857c46d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categories:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(\"All categories:\")\n",
    "print(fetch_20newsgroups(subset=\"train\").target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a16b2bb6-854f-48f1-a5a4-e984bed91fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"sci.med\", \"sci.space\"]   # Medical + Space\n",
    "raw = fetch_20newsgroups(subset=\"all\", categories=CATEGORIES,\n",
    "                         remove=(\"headers\",\"footers\",\"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a234456d-aa48-4556-be1a-9526a5d50d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977 documents\n",
      "['sci.med', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "X_text = raw.data\n",
    "y = raw.target.astype(np.int64)\n",
    "\n",
    "print(len(X_text), \"documents\")\n",
    "print(raw.target_names)  # shows ['sci.med', 'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5da0aa78-aecb-4372-ab42-dd2b0c61ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into each category\n",
    "med_docs = [doc for doc, label in zip(X_text, y) if raw.target_names[label] == \"sci.med\"]\n",
    "space_docs = [doc for doc, label in zip(X_text, y) if raw.target_names[label] == \"sci.space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96b09128-982c-4d22-8f18-59800a5706c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990 sci.med docs\n"
     ]
    }
   ],
   "source": [
    "print(len(med_docs), \"sci.med docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb480a8f-41c8-436c-9e50-8ea981c3e96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987 sci.space docs\n"
     ]
    }
   ],
   "source": [
    "print(len(space_docs), \"sci.space docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c2e9f7c-5c21-4a24-8b70-f404d4047a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using Sentence-BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e50a937e-35a5-40db-86a8-cb799c033926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487b26aeef9440ec888b107a7bf3658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name, device=DEVICE_STR)\n",
    "X = sbert_model.encode(\n",
    "    X_text,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32  # safer batch size for most machines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56736abd-ce5b-4c29-a126-8ccbbfc78e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split + NewsDataset + DataLoader\n",
    "# - 분할은 scikit-learn(train_test_split)로: stratify/재현성 용이\n",
    "# - 배치는 PyTorch(Dataset/DataLoader)로: 미니배치/셔플/GPU 학습\n",
    "#\n",
    "# 이 코드는 상단에서 X, y가 이미 준비되어 있다고 가정합니다\n",
    "# (예: Sentence-BERT 임베딩 완료 후 X: (N, D), y: (N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "04671548-5367-4eea-aa3c-79473c71d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes → train: 1581, val: 198, test: 198\n"
     ]
    }
   ],
   "source": [
    "# 1) 학습/검증/테스트 분할 (sklearn)\n",
    "# - test 20%, val 10% (train 내부에서 12.5%를 떼면 전체의 10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(\n",
    "    f\"Split sizes → train: {len(y_train)}, val: {len(y_val)}, test: {len(y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd56c485-7767-4776-8091-1511c060b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset\n",
    "    #  We use PyTorch (instead of sklearn) because we need seamless integration\n",
    "    #   with neural networks (GPU acceleration, mini-batching, autograd).\n",
    "    #   sklearn handles splitting and evaluation well, but PyTorch handles training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30911acd-7c13-4acd-8abf-1c037e49e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \"\"\"Sentence-BERT 임베딩(X)과 라벨(y)을 PyTorch 학습 파이프라인에 맞게 래핑\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        # DataLoader가 인덱스로 접근해 (feature, label) 쌍을 꺼내갈 수 있도록 함\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "132e8505-f061-46bb-9543-4873b009811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NewsDataset(X_train, y_train)\n",
    "val_ds   = NewsDataset(X_val,   y_val)\n",
    "test_ds  = NewsDataset(X_test,  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e2be1e0-0a4d-4d49-b819-c055d01ffd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1581\n",
      "Val dataset size: 198\n",
      "Test dataset size: 198\n"
     ]
    }
   ],
   "source": [
    "#Dataset이 잘 정의되었는지 확인\n",
    "print(\"Train dataset size:\", len(train_ds))\n",
    "print(\"Val dataset size:\", len(val_ds))\n",
    "print(\"Test dataset size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "21ef2b29-981c-4c83-8fb6-8a69de586b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Example from train_ds:\", train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b188e518-623d-49bf-a97c-46590e56e6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ready: sklearn split + PyTorch Dataset/DataLoader pipeline\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 구성 (PyTorch)\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n",
    "\n",
    "# 4) (선택) 간단한 MLP 예시 — 활성화 함수만 바꿔가며 실험 가능\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn,\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "print(\"✅ Ready: sklearn split + PyTorch Dataset/DataLoader pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4ed6312-68cf-4701-9939-7683820fb05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 준비 완료 → train/val/test 배치 사이즈 확인:\n",
      "Train: X batch shape = torch.Size([64, 384]), y batch shape = torch.Size([64])\n",
      "Val: X batch shape = torch.Size([64, 384]), y batch shape = torch.Size([64])\n",
      "Test: X batch shape = torch.Size([64, 384]), y batch shape = torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(\"DataLoader 준비 완료 → train/val/test 배치 사이즈 확인:\")\n",
    "for name, loader in zip([\"Train\", \"Val\", \"Test\"], [train_loader, val_loader, test_loader]):\n",
    "    batch_X, batch_y = next(iter(loader))\n",
    "    print(f\"{name}: X batch shape = {batch_X.shape}, y batch shape = {batch_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e13d3387-d382-42fc-b4ed-3b285026987a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Build and return a simple MLP (Multi-Layer Perceptron) model.\\n\\n    Args:\\n        act_name (str): Name of the activation function (e.g., 'relu', 'tanh').\\n        input_dim (int): Number of input features.\\n        hidden_dim (int, optional): Number of hidden units. Default = 128.\\n\\n    Returns:\\n        MLP: A PyTorch model with the specified activation and dimensions.\\n \""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP= Multi-Layer Perceptron\n",
    "\"\"\"\n",
    "    Build and return a simple MLP (Multi-Layer Perceptron) model.\n",
    "\n",
    "    Args:\n",
    "        act_name (str): Name of the activation function (e.g., 'relu', 'tanh').\n",
    "        input_dim (int): Number of input features.\n",
    "        hidden_dim (int, optional): Number of hidden units. Default = 128.\n",
    "\n",
    "    Returns:\n",
    "        MLP: A PyTorch model with the specified activation and dimensions.\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c1371a77-9f47-4636-95f6-f0bd186270f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 모델/학습/평가 함수 정의\n",
    "# - build_model(...)\n",
    "# - train_model(...)   # 이미 만든 간결 버전 사용\n",
    "# - evaluate(...)      # 한 가지 버전만 남기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4d76d44c-aeaa-49a1-aab5-317f15aca541",
   "metadata": {},
   "outputs": [],
   "source": [
    " ㅍ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50359a-85d3-4e64-bc83-69ed2b1fccf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b9f8540b-508c-4d5e-bb70-dc025b3bde6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\"relu\", input_dim=384)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2c49e94e-809b-4266-9f6f-79034902bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] mps\n",
      "[Evaluate] loss=0.4828 acc=0.9343 f1=0.9366 auc=0.9753\n",
      "[Epoch 1] loss=0.6057 | val_acc=0.9343\n",
      "[Evaluate] loss=0.2554 acc=0.9242 f1=0.9268 auc=0.9771\n",
      "[Epoch 2] loss=0.3381 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1915 acc=0.9192 f1=0.9200 auc=0.9791\n",
      "[Epoch 3] loss=0.1839 | val_acc=0.9192\n",
      "[Evaluate] loss=0.1786 acc=0.9242 f1=0.9268 auc=0.9793\n",
      "[Epoch 4] loss=0.1387 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1743 acc=0.9242 f1=0.9261 auc=0.9796\n",
      "[Epoch 5] loss=0.1197 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1745 acc=0.9242 f1=0.9261 auc=0.9793\n",
      "[Epoch 6] loss=0.1087 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1751 acc=0.9242 f1=0.9239 auc=0.9795\n",
      "[Epoch 7] loss=0.1007 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1780 acc=0.9242 f1=0.9239 auc=0.9798\n",
      "[Epoch 8] loss=0.0946 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1572 acc=0.9040 f1=0.9005 auc=0.9863\n",
      "[Test Metrics] {'loss': 0.15724752632656483, 'acc': 0.9040404040404041, 'f1': 0.9005235602094241, 'auc': 0.9863279257218651}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Train Model Utility (Simplified)\n",
    "# -------------------------------\n",
    "def train_model(act_name: str = \"relu\", hidden_dim: int = 128, epochs: int = 8, lr: float = 1e-3):\n",
    "    input_dim = X_train.shape[1]\n",
    "    device = DEVICE if 'DEVICE' in globals() else (\n",
    "        torch.device(\"cuda\") if torch.cuda.is_available() else \n",
    "        torch.device(\"mps\") if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \n",
    "        torch.device(\"cpu\")\n",
    "    )\n",
    "    print(\"[Device]\", device)\n",
    "\n",
    "    model = build_model(act_name, input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss, total_n = 0.0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_n += xb.size(0)\n",
    "        train_loss = total_loss / max(total_n, 1)\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        print(f\"[Epoch {ep}] loss={train_loss:.4f} | val_acc={val_metrics['acc']:.4f}\")\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    return model, test_metrics\n",
    "\n",
    "# Explicit test print (Jupyter-friendly)\n",
    "model, test_metrics = train_model()\n",
    "print(\"[Test Metrics]\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b5de8945-cd51-4824-bd74-604fdc9a0c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Evaluation Utility (Jupyter-friendly)\n",
    "# -------------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader, device: torch.device, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given DataLoader and compute metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        loader (DataLoader): DataLoader for validation/test data.\n",
    "        device (torch.device): Device to run evaluation on.\n",
    "        verbose (bool): If True, print metrics directly (useful in Jupyter).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing loss, accuracy, f1, and auc.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to evaluation mode (disables dropout, etc.)\n",
    "    all_probs, all_preds, all_labels = [], [], []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # Softmax to get probabilities for the positive class\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = yb.cpu().numpy()\n",
    "\n",
    "        all_probs.extend(probs)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_n += xb.size(0)\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(total_n, 1),\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"[Evaluate]\",\n",
    "              f\"loss={metrics['loss']:.4f}\",\n",
    "              f\"acc={metrics['acc']:.4f}\",\n",
    "              f\"f1={metrics['f1']:.4f}\",\n",
    "              f\"auc={(metrics['auc'] if isinstance(metrics['auc'], float) else float('nan')):.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c6cb1-c83b-4f5b-95d4-22e752ade5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65131ee5-ddfa-469f-a688-6da69e4ac68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP + model training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bbc8e290-8f96-4a95-9995-e4809950273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 활성화 함수를 손쉽게 바꿔 끼우기 위한 레지스트리\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": nn.ReLU(),\n",
    "    \"leaky_relu\": nn.LeakyReLU(negative_slope=0.01),\n",
    "    \"gelu\": nn.GELU(),\n",
    "    \"silu\": nn.SiLU(),   # Swish\n",
    "    \"mish\": nn.Mish(),\n",
    "    \"elu\": nn.ELU(),\n",
    "    \"selu\": nn.SELU(),\n",
    "    \"tanh\": nn.Tanh(),\n",
    "    \"sigmoid\": nn.Sigmoid(),\n",
    "    \"softplus\": nn.Softplus(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9b3706e5-27e0-4c1f-85ad-3a9f92049b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBuild a simple MLP model with the specified activation function.\\n\\nArgs:\\n    act_name (str): Name of the activation function to use (must be in ACTIVATIONS).\\n    input_dim (int): Dimensionality of the input features.\\n    hidden_dim (int, optional): Number of hidden units. Defaults to 128.\\n\\nReturns:\\n    MLP: A multi-layer perceptron model with the chosen activation.\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Build a simple MLP model with the specified activation function.\n",
    "\n",
    "    Args:\n",
    "        act_name (str): Name of the activation function to use (must be in ACTIVATIONS).\n",
    "        input_dim (int): Dimensionality of the input features.\n",
    "        hidden_dim (int, optional): Number of hidden units. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        MLP: A multi-layer perceptron model with the chosen activation.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bac076c7-1e8f-4c35-a721-d0c17a3e3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(act_name: str, input_dim: int, hidden_dim: int = 128) -> MLP:\n",
    "\n",
    "    act_name = act_name.lower()\n",
    "    if act_name not in ACTIVATIONS:\n",
    "        raise ValueError(f\"Unknown activation: {act_name}. Available: {list(ACTIVATIONS.keys())}\")\n",
    "    model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, activation_fn=ACTIVATIONS[act_name])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "15c7cff6-5167-439c-8e82-1e1688285069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\"relu\", input_dim=384)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a73cedd6-777b-4e57-936e-5839771a87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader, device: torch.device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given DataLoader and compute metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        loader (DataLoader): DataLoader for validation/test data.\n",
    "        device (torch.device): Device to run evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing loss, accuracy, f1, and auc.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to evaluation mode (disables dropout, etc.)\n",
    "    all_probs, all_preds, all_labels = [], [], []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # Softmax to get probabilities for the positive class\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        # Predicted class (0 or 1)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = yb.cpu().numpy()\n",
    "\n",
    "        all_probs.extend(probs)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_n += xb.size(0)\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')  # handle case where ROC AUC cannot be computed\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(total_n, 1),\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cde2f998-f983-42c6-887f-def31aa5d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] loss=0.6927 acc=0.5000 f1=0.0000 auc=0.5940 n=198\n",
      "{'loss': 0.6927446313578673, 'acc': 0.5, 'f1': 0.0, 'auc': 0.5939700030609122, 'n_samples': 198}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(model, val_loader, device)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a3cdb-fcf2-4c43-a15f-5e39f2842baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
