# ðŸ“— 02_NLP_Concepts

This folder contains comprehensive lecture notes and study materials in PDF format, covering core NLP concepts essential for understanding and applying natural language processing techniques.

---

## ðŸ“ Included Materials

| Filename                                   | Description |
|--------------------------------------------|-------------|
| `BoW.pdf`                                  | Bag-of-Words model explained |
| `Confusion_Matrix_20250625.pdf`            | Explanation of confusion matrix for classification |
| `DW_WW_TF-IDF_20250626.pdf`                 | Detailed TF-IDF concept notes |
| `Deep_Learning_Training_Methods_Model_Selection_20250625.pdf` | Deep learning training strategies |
| `Encoding_Vs_Embedding_20250628.pdf`       | Difference between encoding and embeddings |
| `N-gram_20250626.pdf`                       | N-gram language models introduction |
| `NLP_Vector_Cooccurrence_Summary_20250626.pdf` | Summary of vector co-occurrence methods |
| `NLP_core_concept.md`                       | Text file summarizing core NLP ideas |
| `NLP_workflow_20250623.pdf`                 | NLP processing workflow overview |
| `One-hot_Encoding_20250625.pdf`             | One-hot encoding techniques |
| `Padding_20250625.pdf`                      | Sequence padding methods |
| `Part-of-Speech_Tagging_20250626.pdf`      | POS tagging fundamentals |
| `RNN_20250628.pdf`                          | Recurrent Neural Networks explained |
| `Statistical_Language_Model_SLM_20250626.pdf` | Statistical language models overview |
| `Stopwords.pdf`                             | List and handling of stopwords |
| `Tokenization_0624.pdf`                     | Tokenization basics |
| `Word_Representation_Methods_1_20250625.pdf` | Word embedding and representation methods |
| `avoid_overfittings_20250626.pdf`           | Strategies to avoid overfitting in models |
| `integer_encoding_20250625.pdf`             | Integer encoding explained |
| `sparse_vs_dense_representation_20250628.pdf` | Sparse vs dense vector representations |
| `stemming_and_lemmitatization_0624.pdf`    | Difference and implementation of stemming and lemmatization |
| `text_processing_0624.pdf`                   | Overview of text preprocessing techniques |

---

## ðŸ“Œ Study Focus

- Core NLP theoretical concepts in depth
- Classical NLP models and preprocessing methods
- Basic statistical and deep learning concepts relevant to NLP
- Strong emphasis on text vectorization and encoding
- Practical notes to complement coding notebooks elsewhere

---

## ðŸ’¡ How to Use

- Browse PDFs to deepen theoretical understanding alongside hands-on notebooks.
- Reference the markdown summary `NLP_core_concept.md` for quick refreshers.
- Use as a complementary resource for course review and self-study.

---

> This folder complements the hands-on coding notes by providing detailed lecture materials and summaries in document form.

