{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a90a02-711a-4908-b7ff-fb5c2c3d1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¬ í™œì„±í™” í•¨ìˆ˜ ë¹„êµ ì‹¤í—˜ (Jupyter Notebook)\n",
    "# ---------------------------------------------------------------\n",
    "# ëª©í‘œ: ReLU / Sigmoid / Tanh / GELU / SiLU(Swish) / LeakyReLU í™œì„±í™” í•¨ìˆ˜ë¥¼\n",
    "# ê°„ë‹¨í•œ NLP ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë¹„êµí•´ë³¸ë‹¤.\n",
    "#\n",
    "# ì•„ì´ë””ì–´:\n",
    "# - GPT ë‚´ë¶€ í™œì„±í™” í•¨ìˆ˜ëŠ” ë°”ê¿€ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ìš°ë¦¬ê°€ ì§ì ‘ ë§Œë“  ì‘ì€ ë¶„ë¥˜ ëª¨ë¸ì—ì„œ ì‹¤í—˜.\n",
    "# - ê°™ì€ ë°ì´í„°ì…‹, ê°™ì€ ëª¨ë¸ êµ¬ì¡°, ê°™ì€ í•™ìŠµ ì„¤ì •ì—ì„œ í™œì„±í™”ë§Œ ë°”ê¿” ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³¸ë‹¤.\n",
    "# - í…ìŠ¤íŠ¸ â†’ TF-IDF â†’ ì‘ì€ MLP â†’ ë¶„ë¥˜ ì •í™•ë„/ROC AUC ë“±ìœ¼ë¡œ ë¹„êµ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69066eec-f979-4da2-913e-b9dfa26ca87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "980389e7-6ead-481d-9ddf-f501964bcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca6bdad9-f1f6-4082-9113-b761a1aaab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd7e0ffa-c282-448b-ba87-3b13aec9ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "DEVICE_STR = str(DEVICE)  # <-- fixed: now always defined as a string (\"cuda\", \"mps\" or \"cpu\")\n",
    "print(\"Using device:\", DEVICE_STR)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a46e1731-af46-49f3-a4ee-45604cbc3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac4b16ab-e727-4701-bf48-7a7e857c46d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categories:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(\"All categories:\")\n",
    "print(fetch_20newsgroups(subset=\"train\").target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a16b2bb6-854f-48f1-a5a4-e984bed91fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"sci.med\", \"sci.space\"]   # Medical + Space\n",
    "raw = fetch_20newsgroups(subset=\"all\", categories=CATEGORIES,\n",
    "                         remove=(\"headers\",\"footers\",\"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a234456d-aa48-4556-be1a-9526a5d50d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977 documents\n",
      "['sci.med', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "X_text = raw.data\n",
    "y = raw.target.astype(np.int64)\n",
    "\n",
    "print(len(X_text), \"documents\")\n",
    "print(raw.target_names)  # shows ['sci.med', 'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5da0aa78-aecb-4372-ab42-dd2b0c61ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into each category\n",
    "med_docs = [doc for doc, label in zip(X_text, y) if raw.target_names[label] == \"sci.med\"]\n",
    "space_docs = [doc for doc, label in zip(X_text, y) if raw.target_names[label] == \"sci.space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96b09128-982c-4d22-8f18-59800a5706c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990 sci.med docs\n"
     ]
    }
   ],
   "source": [
    "print(len(med_docs), \"sci.med docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb480a8f-41c8-436c-9e50-8ea981c3e96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987 sci.space docs\n"
     ]
    }
   ],
   "source": [
    "print(len(space_docs), \"sci.space docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c2e9f7c-5c21-4a24-8b70-f404d4047a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using Sentence-BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e50a937e-35a5-40db-86a8-cb799c033926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487b26aeef9440ec888b107a7bf3658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name, device=DEVICE_STR)\n",
    "X = sbert_model.encode(\n",
    "    X_text,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32  # safer batch size for most machines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56736abd-ce5b-4c29-a126-8ccbbfc78e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split + NewsDataset + DataLoader\n",
    "# - ë¶„í• ì€ scikit-learn(train_test_split)ë¡œ: stratify/ì¬í˜„ì„± ìš©ì´\n",
    "# - ë°°ì¹˜ëŠ” PyTorch(Dataset/DataLoader)ë¡œ: ë¯¸ë‹ˆë°°ì¹˜/ì…”í”Œ/GPU í•™ìŠµ\n",
    "#\n",
    "# ì´ ì½”ë“œëŠ” ìƒë‹¨ì—ì„œ X, yê°€ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤\n",
    "# (ì˜ˆ: Sentence-BERT ì„ë² ë”© ì™„ë£Œ í›„ X: (N, D), y: (N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "04671548-5367-4eea-aa3c-79473c71d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes â†’ train: 1581, val: 198, test: 198\n"
     ]
    }
   ],
   "source": [
    "# 1) í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (sklearn)\n",
    "# - test 20%, val 10% (train ë‚´ë¶€ì—ì„œ 12.5%ë¥¼ ë–¼ë©´ ì „ì²´ì˜ 10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(\n",
    "    f\"Split sizes â†’ train: {len(y_train)}, val: {len(y_val)}, test: {len(y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd56c485-7767-4776-8091-1511c060b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset\n",
    "    #  We use PyTorch (instead of sklearn) because we need seamless integration\n",
    "    #   with neural networks (GPU acceleration, mini-batching, autograd).\n",
    "    #   sklearn handles splitting and evaluation well, but PyTorch handles training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30911acd-7c13-4acd-8abf-1c037e49e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \"\"\"Sentence-BERT ì„ë² ë”©(X)ê³¼ ë¼ë²¨(y)ì„ PyTorch í•™ìŠµ íŒŒì´í”„ë¼ì¸ì— ë§ê²Œ ë˜í•‘\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        # DataLoaderê°€ ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼í•´ (feature, label) ìŒì„ êº¼ë‚´ê°ˆ ìˆ˜ ìˆë„ë¡ í•¨\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "132e8505-f061-46bb-9543-4873b009811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NewsDataset(X_train, y_train)\n",
    "val_ds   = NewsDataset(X_val,   y_val)\n",
    "test_ds  = NewsDataset(X_test,  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e2be1e0-0a4d-4d49-b819-c055d01ffd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1581\n",
      "Val dataset size: 198\n",
      "Test dataset size: 198\n"
     ]
    }
   ],
   "source": [
    "#Datasetì´ ì˜ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "print(\"Train dataset size:\", len(train_ds))\n",
    "print(\"Val dataset size:\", len(val_ds))\n",
    "print(\"Test dataset size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "21ef2b29-981c-4c83-8fb6-8a69de586b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Example from train_ds:\", train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b188e518-623d-49bf-a97c-46590e56e6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ready: sklearn split + PyTorch Dataset/DataLoader pipeline\n"
     ]
    }
   ],
   "source": [
    "# DataLoader êµ¬ì„± (PyTorch)\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n",
    "\n",
    "# 4) (ì„ íƒ) ê°„ë‹¨í•œ MLP ì˜ˆì‹œ â€” í™œì„±í™” í•¨ìˆ˜ë§Œ ë°”ê¿”ê°€ë©° ì‹¤í—˜ ê°€ëŠ¥\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn,\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "print(\"âœ… Ready: sklearn split + PyTorch Dataset/DataLoader pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4ed6312-68cf-4701-9939-7683820fb05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader ì¤€ë¹„ ì™„ë£Œ â†’ train/val/test ë°°ì¹˜ ì‚¬ì´ì¦ˆ í™•ì¸:\n",
      "Train: X batch shape = torch.Size([64, 384]), y batch shape = torch.Size([64])\n",
      "Val: X batch shape = torch.Size([64, 384]), y batch shape = torch.Size([64])\n",
      "Test: X batch shape = torch.Size([64, 384]), y batch shape = torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(\"DataLoader ì¤€ë¹„ ì™„ë£Œ â†’ train/val/test ë°°ì¹˜ ì‚¬ì´ì¦ˆ í™•ì¸:\")\n",
    "for name, loader in zip([\"Train\", \"Val\", \"Test\"], [train_loader, val_loader, test_loader]):\n",
    "    batch_X, batch_y = next(iter(loader))\n",
    "    print(f\"{name}: X batch shape = {batch_X.shape}, y batch shape = {batch_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e13d3387-d382-42fc-b4ed-3b285026987a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Build and return a simple MLP (Multi-Layer Perceptron) model.\\n\\n    Args:\\n        act_name (str): Name of the activation function (e.g., 'relu', 'tanh').\\n        input_dim (int): Number of input features.\\n        hidden_dim (int, optional): Number of hidden units. Default = 128.\\n\\n    Returns:\\n        MLP: A PyTorch model with the specified activation and dimensions.\\n \""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP= Multi-Layer Perceptron\n",
    "\"\"\"\n",
    "    Build and return a simple MLP (Multi-Layer Perceptron) model.\n",
    "\n",
    "    Args:\n",
    "        act_name (str): Name of the activation function (e.g., 'relu', 'tanh').\n",
    "        input_dim (int): Number of input features.\n",
    "        hidden_dim (int, optional): Number of hidden units. Default = 128.\n",
    "\n",
    "    Returns:\n",
    "        MLP: A PyTorch model with the specified activation and dimensions.\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c1371a77-9f47-4636-95f6-f0bd186270f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ëª¨ë¸/í•™ìŠµ/í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "# - build_model(...)\n",
    "# - train_model(...)   # ì´ë¯¸ ë§Œë“  ê°„ê²° ë²„ì „ ì‚¬ìš©\n",
    "# - evaluate(...)      # í•œ ê°€ì§€ ë²„ì „ë§Œ ë‚¨ê¸°ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4d76d44c-aeaa-49a1-aab5-317f15aca541",
   "metadata": {},
   "outputs": [],
   "source": [
    " ã…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50359a-85d3-4e64-bc83-69ed2b1fccf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b9f8540b-508c-4d5e-bb70-dc025b3bde6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\"relu\", input_dim=384)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2c49e94e-809b-4266-9f6f-79034902bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] mps\n",
      "[Evaluate] loss=0.4828 acc=0.9343 f1=0.9366 auc=0.9753\n",
      "[Epoch 1] loss=0.6057 | val_acc=0.9343\n",
      "[Evaluate] loss=0.2554 acc=0.9242 f1=0.9268 auc=0.9771\n",
      "[Epoch 2] loss=0.3381 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1915 acc=0.9192 f1=0.9200 auc=0.9791\n",
      "[Epoch 3] loss=0.1839 | val_acc=0.9192\n",
      "[Evaluate] loss=0.1786 acc=0.9242 f1=0.9268 auc=0.9793\n",
      "[Epoch 4] loss=0.1387 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1743 acc=0.9242 f1=0.9261 auc=0.9796\n",
      "[Epoch 5] loss=0.1197 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1745 acc=0.9242 f1=0.9261 auc=0.9793\n",
      "[Epoch 6] loss=0.1087 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1751 acc=0.9242 f1=0.9239 auc=0.9795\n",
      "[Epoch 7] loss=0.1007 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1780 acc=0.9242 f1=0.9239 auc=0.9798\n",
      "[Epoch 8] loss=0.0946 | val_acc=0.9242\n",
      "[Evaluate] loss=0.1572 acc=0.9040 f1=0.9005 auc=0.9863\n",
      "[Test Metrics] {'loss': 0.15724752632656483, 'acc': 0.9040404040404041, 'f1': 0.9005235602094241, 'auc': 0.9863279257218651}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Train Model Utility (Simplified)\n",
    "# -------------------------------\n",
    "def train_model(act_name: str = \"relu\", hidden_dim: int = 128, epochs: int = 8, lr: float = 1e-3):\n",
    "    input_dim = X_train.shape[1]\n",
    "    device = DEVICE if 'DEVICE' in globals() else (\n",
    "        torch.device(\"cuda\") if torch.cuda.is_available() else \n",
    "        torch.device(\"mps\") if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \n",
    "        torch.device(\"cpu\")\n",
    "    )\n",
    "    print(\"[Device]\", device)\n",
    "\n",
    "    model = build_model(act_name, input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss, total_n = 0.0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_n += xb.size(0)\n",
    "        train_loss = total_loss / max(total_n, 1)\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        print(f\"[Epoch {ep}] loss={train_loss:.4f} | val_acc={val_metrics['acc']:.4f}\")\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    return model, test_metrics\n",
    "\n",
    "# Explicit test print (Jupyter-friendly)\n",
    "model, test_metrics = train_model()\n",
    "print(\"[Test Metrics]\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b5de8945-cd51-4824-bd74-604fdc9a0c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Evaluation Utility (Jupyter-friendly)\n",
    "# -------------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader, device: torch.device, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given DataLoader and compute metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        loader (DataLoader): DataLoader for validation/test data.\n",
    "        device (torch.device): Device to run evaluation on.\n",
    "        verbose (bool): If True, print metrics directly (useful in Jupyter).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing loss, accuracy, f1, and auc.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to evaluation mode (disables dropout, etc.)\n",
    "    all_probs, all_preds, all_labels = [], [], []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # Softmax to get probabilities for the positive class\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = yb.cpu().numpy()\n",
    "\n",
    "        all_probs.extend(probs)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_n += xb.size(0)\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(total_n, 1),\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"[Evaluate]\",\n",
    "              f\"loss={metrics['loss']:.4f}\",\n",
    "              f\"acc={metrics['acc']:.4f}\",\n",
    "              f\"f1={metrics['f1']:.4f}\",\n",
    "              f\"auc={(metrics['auc'] if isinstance(metrics['auc'], float) else float('nan')):.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c6cb1-c83b-4f5b-95d4-22e752ade5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65131ee5-ddfa-469f-a688-6da69e4ac68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP + model training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bbc8e290-8f96-4a95-9995-e4809950273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ëŸ¬ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì†ì‰½ê²Œ ë°”ê¿” ë¼ìš°ê¸° ìœ„í•œ ë ˆì§€ìŠ¤íŠ¸ë¦¬\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": nn.ReLU(),\n",
    "    \"leaky_relu\": nn.LeakyReLU(negative_slope=0.01),\n",
    "    \"gelu\": nn.GELU(),\n",
    "    \"silu\": nn.SiLU(),   # Swish\n",
    "    \"mish\": nn.Mish(),\n",
    "    \"elu\": nn.ELU(),\n",
    "    \"selu\": nn.SELU(),\n",
    "    \"tanh\": nn.Tanh(),\n",
    "    \"sigmoid\": nn.Sigmoid(),\n",
    "    \"softplus\": nn.Softplus(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9b3706e5-27e0-4c1f-85ad-3a9f92049b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBuild a simple MLP model with the specified activation function.\\n\\nArgs:\\n    act_name (str): Name of the activation function to use (must be in ACTIVATIONS).\\n    input_dim (int): Dimensionality of the input features.\\n    hidden_dim (int, optional): Number of hidden units. Defaults to 128.\\n\\nReturns:\\n    MLP: A multi-layer perceptron model with the chosen activation.\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Build a simple MLP model with the specified activation function.\n",
    "\n",
    "    Args:\n",
    "        act_name (str): Name of the activation function to use (must be in ACTIVATIONS).\n",
    "        input_dim (int): Dimensionality of the input features.\n",
    "        hidden_dim (int, optional): Number of hidden units. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        MLP: A multi-layer perceptron model with the chosen activation.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bac076c7-1e8f-4c35-a721-d0c17a3e3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(act_name: str, input_dim: int, hidden_dim: int = 128) -> MLP:\n",
    "\n",
    "    act_name = act_name.lower()\n",
    "    if act_name not in ACTIVATIONS:\n",
    "        raise ValueError(f\"Unknown activation: {act_name}. Available: {list(ACTIVATIONS.keys())}\")\n",
    "    model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, activation_fn=ACTIVATIONS[act_name])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "15c7cff6-5167-439c-8e82-1e1688285069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\"relu\", input_dim=384)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a73cedd6-777b-4e57-936e-5839771a87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader, device: torch.device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given DataLoader and compute metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        loader (DataLoader): DataLoader for validation/test data.\n",
    "        device (torch.device): Device to run evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing loss, accuracy, f1, and auc.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to evaluation mode (disables dropout, etc.)\n",
    "    all_probs, all_preds, all_labels = [], [], []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # Softmax to get probabilities for the positive class\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        # Predicted class (0 or 1)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = yb.cpu().numpy()\n",
    "\n",
    "        all_probs.extend(probs)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_n += xb.size(0)\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')  # handle case where ROC AUC cannot be computed\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(total_n, 1),\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cde2f998-f983-42c6-887f-def31aa5d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] loss=0.6927 acc=0.5000 f1=0.0000 auc=0.5940 n=198\n",
      "{'loss': 0.6927446313578673, 'acc': 0.5, 'f1': 0.0, 'auc': 0.5939700030609122, 'n_samples': 198}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(model, val_loader, device)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a3cdb-fcf2-4c43-a15f-5e39f2842baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
