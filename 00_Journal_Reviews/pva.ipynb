{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b70cb-c4d2-462b-b296-95a1a377ab04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4522de1-8d5f-40d1-9f76-b41e4a51db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#프롬프트 토큰화 (base, ctx)\n",
    "#drafter 제안 블록 (draft_ids)\n",
    "#verifier 그리디 시퀀스 (greedy_ids)\n",
    "#수락된 prefix 길이 (accepted_prefix_len)\n",
    "#누적 acceptance_rate_%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9054221-0aea-4ffc-93ff-245dd5e7e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicahong/.pyenv/versions/bpd_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027289cf-8d81-4ffe-95d4-e6ec0909f053",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (186763338.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"mps\" if torch.backends.mps.is_available()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#device = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "#print(\"using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e483fb-0f5c-4ef8-aeed-e1fa97026313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "058b3909-ec76-453f-8725-07779c3041ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "print(\"tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272c7108-8bf8-41e3-8663-5f91c01b8193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drafter model loaded\n"
     ]
    }
   ],
   "source": [
    "#Drafter model load\n",
    "drafter = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "print(\"drafter model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae79a861-d371-4dca-a9a0-9a416216ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verifier model loaded\n"
     ]
    }
   ],
   "source": [
    "#verifier\n",
    "verifier = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "print(\"verifier model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16300585-3ecc-4d67-adc0-d4de41a17f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In a distant future,\n"
     ]
    }
   ],
   "source": [
    "#prompt\n",
    "prompt = \"In a distant future,\"\n",
    "print(\"prompt:\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb9dd33-9bf9-4ff0-87a0-abde03e95e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|endoftext|>\n",
      "eos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#tokenizer PAD.EOS setting\n",
    "# distilgpt2 / gpt2 계열은 기본적으로 pad_token이 없음 → eos_token으로 맞춰줌\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "print(\"pad_token:\", tok.pad_token)\n",
    "print(\"eos_token:\", tok.eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "247cb1ad-9125-47f5-915b-19bd77a73772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict–Verify–Accept (P–V–A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0762a60-cf3d-437c-94d6-2a9c00dbfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-1. Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92f710e5-e5d9-4042-8440-7701093b919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_next_token(verifier, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    out = verifier(input_ids=input_ids)\n",
    "    next_id = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)  # shape [1,1]\n",
    "    return next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c7043c9-b055-4fc4-b2e8-fd4b32e7204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측된 토큰 ID: 262\n",
      "예측된 토큰 문자열:  the\n"
     ]
    }
   ],
   "source": [
    "# prompt를 토크나이즈해서 verifier에 넣어보기\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "next_id = greedy_next_token(verifier, inputs[\"input_ids\"])\n",
    "print(\"예측된 토큰 ID:\", next_id.item())\n",
    "print(\"예측된 토큰 문자열:\", tok.decode([next_id.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c469e3d-52fe-44d5-b45e-d685243af416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-2. draft_block func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6666603-864e-435e-b29b-ac28fde4e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def draft_block(\n",
    "    drafter,\n",
    "    input_ids: torch.Tensor,\n",
    "    block_size: int = 3,\n",
    "    do_sample: bool = True,\n",
    "    top_k: int = 50,\n",
    "    temperature: float = 0.8,\n",
    "    pad_token_id: int = None,\n",
    "    # ↓ 추가 옵션들\n",
    "    top_p: float = 0.95,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    no_repeat_ngram_size: int = 3,\n",
    "    length_penalty: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    drafter가 block_size만큼 한 번에 제안.\n",
    "    반환: 새로 제안된 토큰들만 (프롬프트 부분 제외)\n",
    "    \"\"\"\n",
    "    gen = drafter.generate(\n",
    "        input_ids,\n",
    "        attention_mask=(input_ids != tok.pad_token_id),  # pad/eos 구분 명시\n",
    "        max_new_tokens=block_size,\n",
    "        do_sample=do_sample,\n",
    "        top_k=top_k if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        pad_token_id=pad_token_id,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        length_penalty=length_penalty,\n",
    "    )\n",
    "    # 프롬프트 길이 이후 부분만 반환\n",
    "    return gen[:, input_ids.shape[1]:]  # shape: [1, block_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7441334a-d3da-410b-8fd9-927bd14fd42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draft token IDs: [262, 1692, 3234]\n",
      "draft tokens (decoded):  the human race\n"
     ]
    }
   ],
   "source": [
    "ctx = tok(prompt, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "block = draft_block(\n",
    "    drafter, ctx,\n",
    "    block_size=3,\n",
    "    do_sample=True, top_k=50, temperature=0.8,\n",
    "    pad_token_id=tok.eos_token_id,\n",
    "    top_p=0.95, repetition_penalty=1.1, no_repeat_ngram_size=3\n",
    ")\n",
    "\n",
    "print(\"draft token IDs:\", block.tolist()[0])\n",
    "print(\"draft tokens (decoded):\", tok.decode(block[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb1736f-df42-436f-8e49-da647400edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix-verify-accpet-once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e44eed84-1bec-4658-917f-65a24fb3871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pva_step(ctx, block_size=3):\n",
    "    \"\"\"\n",
    "    현재 컨텍스트(ctx)에서 drafter가 block 제안 → verifier로 앞에서부터 비교 →\n",
    "    일치하는 prefix 수락, 첫 불일치에서 greedy 대체 후 종료.\n",
    "    returns: new_ctx, accepted_len (이번 스텝에서 수락된 토큰 수)\n",
    "    \"\"\"\n",
    "    model_device = next(drafter.parameters()).device\n",
    "    ctx = ctx.to(model_device)\n",
    "\n",
    "    block = draft_block(\n",
    "        drafter, ctx,\n",
    "        block_size=block_size,\n",
    "        do_sample=True, top_k=50, temperature=0.8,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "        top_p=0.95, repetition_penalty=1.1, no_repeat_ngram_size=3\n",
    "    )\n",
    "\n",
    "    greedy_seq = []\n",
    "    gctx = ctx.clone().to(next(verifier.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(block.shape[1]):\n",
    "            attn = (gctx != tok.pad_token_id).to(gctx.device)\n",
    "            out = verifier(input_ids=gctx, attention_mask=attn)\n",
    "            g = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            greedy_seq.append(int(g.item()))\n",
    "            gctx = torch.cat([gctx, g], dim=1)\n",
    "\n",
    "    accepted = 0\n",
    "    cur = ctx.clone().to(gctx.device)\n",
    "    for i in range(block.shape[1]):\n",
    "        d_id = int(block[0, i].item())\n",
    "        g_id = greedy_seq[i]\n",
    "        if d_id == g_id:\n",
    "            cur = torch.cat([cur, block[:, i:i+1]], dim=1)\n",
    "            accepted += 1\n",
    "        else:\n",
    "            g = torch.tensor([[g_id]], device=cur.device)\n",
    "            cur = torch.cat([cur, g], dim=1)\n",
    "            break\n",
    "\n",
    "    return cur, accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6c955b5-056b-46c4-8315-f140847f9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "987c403e-4cc6-4a42-b822-5a8f0dd1ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted in this step: 0\n",
      "In a distant future, the\n"
     ]
    }
   ],
   "source": [
    "ctx = tok(prompt, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "new_ctx, accepted = pva_step(ctx, block_size=3)\n",
    "print(\"accepted in this step:\", accepted)\n",
    "print(tok.decode(new_ctx[0], skip_special_tokens=True)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c23bd961-3523-4618-83b1-f3d6a3b9dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-3-2. drafter가 블록 제안만 먼저 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97af91ec-4386-459a-a5cc-acb3852967a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draft ids: [465, 2802, 550]\n",
      "draft text:  his mother had\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "block = draft_block(\n",
    "    drafter, ctx,\n",
    "    block_size=block_size,\n",
    "    do_sample=True, top_k=50, temperature=0.8,\n",
    "    pad_token_id=tok.eos_token_id\n",
    ")\n",
    "print(\"draft ids:\", block.tolist()[0])\n",
    "print(\"draft text:\", tok.decode(block[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1aac2de5-e260-492c-923e-985d58bea274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-3-3. 같은 길이만큼 verifier의 greedy 시퀀스만 계산해서 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "501cd4f7-0e52-4790-8c7f-1ce1c0d01992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy ids: [262, 995, 561]\n",
      "greedy text (참고):  the world would\n"
     ]
    }
   ],
   "source": [
    "greedy_seq = []\n",
    "gctx = ctx.clone()\n",
    "with torch.no_grad():\n",
    "    for _ in range(block.shape[1]):\n",
    "        out = verifier(input_ids=gctx)\n",
    "        g = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        greedy_seq.append(int(g.item()))\n",
    "        gctx = torch.cat([gctx, g], dim=1)\n",
    "\n",
    "print(\"greedy ids:\", greedy_seq)\n",
    "print(\"greedy text (참고):\", tok.decode(greedy_seq, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eabe914b-6c53-46c5-8ef9-8a94b3dd7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-3-4. 앞에서부터 한 토큰씩 비교해서 accept할지 결정 (루프를 눈으로 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a92928b-91b1-4965-8ea4-9d776f799140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i=0] draft=465 vs greedy=262 ( his vs  the)\n",
      "  → mismatch, take greedy and stop\n",
      "accepted prefix: 0 / 3\n"
     ]
    }
   ],
   "source": [
    "accepted_ids = []\n",
    "cur = ctx.clone()\n",
    "\n",
    "for i in range(block.shape[1]):\n",
    "    d_id = int(block[0, i].item())\n",
    "    g_id = greedy_seq[i]\n",
    "    print(f\"[i={i}] draft={d_id} vs greedy={g_id} ({tok.decode([d_id])} vs {tok.decode([g_id])})\")\n",
    "\n",
    "    if d_id == g_id:\n",
    "        # 일치 → draft 토큰 수락\n",
    "        accepted_ids.append(d_id)\n",
    "        cur = torch.cat([cur, block[:, i:i+1]], dim=1)\n",
    "        print(\"  → accept draft\")\n",
    "    else:\n",
    "        # 불일치 → greedy 토큰으로 대체하고 중단\n",
    "        g = torch.tensor([[g_id]], device=cur.device)\n",
    "        cur = torch.cat([cur, g], dim=1)\n",
    "        print(\"  → mismatch, take greedy and stop\")\n",
    "        break\n",
    "\n",
    "print(\"accepted prefix:\", len(accepted_ids), \"/\", block.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a68b2ad9-c54c-49f1-9780-403ddb2347a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-3-5. 한 스텝 끝난 뒤의 텍스트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7e4058c-40c3-4d73-9f0d-1fe6fafb910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new text after one step:\n",
      " In a distant future, the\n"
     ]
    }
   ],
   "source": [
    "print(\"new text after one step:\\n\", tok.decode(cur[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "518fadb5-7cb1-46a0-90c6-a16045fe2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-4-1. 한 스텝만 수행하는 작은 함수(pva_step) 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6caf02a-0ef0-466c-bb67-c9bf3c1b4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pva_step(ctx, block_size=3):\n",
    "    \"\"\"\n",
    "    현재 컨텍스트(ctx)에서 drafter가 block 제안 → verifier로 앞에서부터 비교 →\n",
    "    일치하는 prefix 수락, 첫 불일치에서 greedy 대체 후 종료.\n",
    "    returns: new_ctx, accepted_len (이번 스텝에서 수락된 토큰 수)\n",
    "    \"\"\"\n",
    "    # 0) 디바이스 정렬\n",
    "    model_device = next(drafter.parameters()).device\n",
    "    ctx = ctx.to(model_device)\n",
    "\n",
    "    # 1) draft (반복 억제 옵션 포함)\n",
    "    block = draft_block(\n",
    "        drafter, ctx,\n",
    "        block_size=block_size,\n",
    "        do_sample=True, top_k=50, temperature=0.8,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "        top_p=0.95, repetition_penalty=1.1, no_repeat_ngram_size=3\n",
    "    )\n",
    "\n",
    "    # 2) 같은 길이만큼 verifier의 greedy 시퀀스 (attention_mask 명시)\n",
    "    greedy_seq = []\n",
    "    gctx = ctx.clone().to(next(verifier.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(block.shape[1]):\n",
    "            attn = (gctx != tok.pad_token_id).to(gctx.device)\n",
    "            out = verifier(input_ids=gctx, attention_mask=attn)\n",
    "            g = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            greedy_seq.append(int(g.item()))\n",
    "            gctx = torch.cat([gctx, g], dim=1)\n",
    "\n",
    "    # 3) 앞에서부터 비교(accept or stop)\n",
    "    accepted = 0\n",
    "    cur = ctx.clone().to(gctx.device)\n",
    "    for i in range(block.shape[1]):\n",
    "        d_id = int(block[0, i].item())\n",
    "        g_id = greedy_seq[i]\n",
    "        if d_id == g_id:\n",
    "            cur = torch.cat([cur, block[:, i:i+1]], dim=1)\n",
    "            accepted += 1\n",
    "        else:\n",
    "            g = torch.tensor([[g_id]], device=cur.device)\n",
    "            cur = torch.cat([cur, g], dim=1)\n",
    "            break\n",
    "\n",
    "    return cur, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be79c5be-4693-4d6a-af1b-d5f47cef19b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted in this step: 1\n",
      "In a distant future, the world\n"
     ]
    }
   ],
   "source": [
    "ctx = tok(prompt, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "new_ctx, accepted = pva_step(ctx, block_size=3)\n",
    "print(\"accepted in this step:\", accepted)\n",
    "print(tok.decode(new_ctx[0], skip_special_tokens=True)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef685a5-b6a9-46f5-a44a-6d6fd17fdb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05370ce9-94ac-4103-aa3f-013ef8a1ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "\n",
    "def run_pva_loop(prompt, target_new_tokens=60, block_size=3, max_steps=500):\n",
    "    # 1) padding=True로 만들어 attention_mask 일관성 확보\n",
    "    base = tok(prompt, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "    ctx  = base.clone()\n",
    "\n",
    "    proposed_total = 0\n",
    "    accepted_total = 0\n",
    "    steps = 0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    while (ctx.shape[1] - base.shape[1]) < target_new_tokens and steps < max_steps:\n",
    "        steps += 1\n",
    "        proposed_total += block_size\n",
    "\n",
    "        try:\n",
    "            ctx, accepted = pva_step(ctx, block_size=block_size)\n",
    "        except RuntimeError as e:\n",
    "            # 드문 OOM/디바이스 에러 대응: 캐시 비우고 block_size 줄여 재시도\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                ctx, accepted = pva_step(ctx, block_size=max(1, block_size // 2))\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        accepted_total += accepted\n",
    "\n",
    "        # EOS 조기 종료\n",
    "        if ctx[0, -1].item() == tok.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # 2) 간단 반복 가드: 마지막 두 줄이 동일하면 중단\n",
    "        text_now = tok.decode(ctx[0], skip_special_tokens=True)\n",
    "        lines = [s.strip() for s in text_now.splitlines() if s.strip()]\n",
    "        if len(lines) >= 2 and lines[-1] == lines[-2]:\n",
    "            break\n",
    "\n",
    "    dt = time.perf_counter() - t0\n",
    "    new_tokens = ctx.shape[1] - base.shape[1]\n",
    "    text = tok.decode(ctx[0], skip_special_tokens=True)\n",
    "    metrics = {\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"time_sec\": round(dt, 3),\n",
    "        \"tokens_per_sec\": round(new_tokens / max(dt, 1e-9), 2),\n",
    "        \"proposed\": proposed_total,\n",
    "        \"accepted\": accepted_total,\n",
    "        \"acceptance_rate_%\": round(100 * accepted_total / max(proposed_total, 1), 1),\n",
    "        \"steps\": steps,\n",
    "        \"block_size\": block_size,\n",
    "    }\n",
    "    return text, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfe55e68-5c45-4130-a10d-97feede97d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, the world would be a better place.\n",
      "\n",
      "The world would be a better place.\n",
      "\n",
      "The world would be a better place.\n",
      "{'new_tokens': 28, 'time_sec': 4.181, 'tokens_per_sec': 6.7, 'proposed': 81, 'accepted': 1, 'acceptance_rate_%': 1.2, 'steps': 27, 'block_size': 3}\n"
     ]
    }
   ],
   "source": [
    "gen_text, m = run_pva_loop(\"In a distant future,\", target_new_tokens=60, block_size=3)\n",
    "print(gen_text[:400])\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2693206-e3d0-4dd0-b068-f25a6dbed6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bpd_env)",
   "language": "python",
   "name": "bpd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
