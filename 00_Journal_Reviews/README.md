# Journal Reviews

This folder contains summary PDFs of key academic papers reviewed as part of our NLP study.

## File List

- **BERT.pdf**  
  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
  [Paper link](https://arxiv.org/abs/1810.04805)

- **DistilBERT.pdf**  
  DistilBERT: A distilled version of BERT â€” smaller, faster, cheaper and lighter  
  [Paper link](https://arxiv.org/pdf/1910.01108)

- **ULMFiT.pdf**  
  ULMFiT: Universal Language Model Fine-tuning for Text Classification  
  [Paper link](https://arxiv.org/abs/1801.06146)

## Highlights

### BERT
- Introduced a bidirectional transformer-based pre-training model for language understanding.
- Achieved state-of-the-art results on various NLP tasks.

### DistilBERT
- A lightweight version of BERT with faster inference and smaller size while maintaining comparable performance.
- Suitable for real-world applications with limited resources.

### ULMFiT
- Proposed an effective fine-tuning method for pre-trained language models in text classification.
- Particularly strong on low-resource and non-English datasets.


---

More papers and summaries will be added here as the study progresses.

