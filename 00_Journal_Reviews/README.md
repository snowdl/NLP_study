# Journal Reviews

This folder contains summary PDFs of key academic papers reviewed as part of our NLP study.

## File List
This folder contains summary PDFs of key academic papers reviewed as part of our NLP study.

## File List

- **BERT.pdf**  
  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
  [Paper link](https://arxiv.org/abs/1810.04805)

- **DistilBERT.pdf**  
  DistilBERT: A distilled version of BERT â€” smaller, faster, cheaper and lighter  
  [Paper link](https://arxiv.org/pdf/1910.01108)

- **ULMFiT.pdf**  
  ULMFiT: Universal Language Model Fine-tuning for Text Classification  
  [Paper link](https://arxiv.org/abs/1801.06146)

- **LLMs for Text Classification â€“ Case Study & Review**  
  Evaluates LLMs (LLaMA3, GPT-4, Mistral) and ML models (SVM, Naive Bayes) on binary/multiclass classification tasks.  
  Highlights prompt engineering techniques like CoT, RP, NA.  
  [Paper link](https://arxiv.org/pdf/2501.08457v1)  
  [ðŸ“„ PDF ë³´ê¸°](LARGE%20LANGUAGE%20MODELS%20FOR%20TEXT%20CLASSIFICATION-%20CASE%20STUDY%20AND%20COMPREHENSIVE%20REVIEW.pdf)

## Highlights

### BERT
- Introduced a bidirectional transformer-based pre-training model for language understanding.
- Achieved state-of-the-art results on various NLP tasks.

### DistilBERT- A lightweight version of BERT with faster inference and smaller size while maintaining comparable performance.
- Suitable for real-world applications with limited resources.

### ULMFiT
- Proposed an effective fine-tuning method for pre-trained language models in text classification.
- Particularly strong on low-resource and non-English datasets.

### LLMs for Text Classification
- Benchmarks LLMs and traditional ML models on real-world classification tasks (FakeNewsNet, Employee Reviews).
- Finds that RoBERTa and LLaMA3 70B perform best; traditional models still viable in simpler tasks.
- Emphasizes importance of prompt engineering for stable and effective LLM performance.

---

More papers and summaries will be added here as the study progresses.
es LLMs (LLaMA3, GPT-4, Mistral) and ML models (SVM, Naive Bayes) on binary/multiclass classification tasks.  
  Highlights prompt engineering techniques like CoT, RP, NA.  
  â†’ [ðŸ“„ PDF ë³´ê¸°](LARGE%20LANGUAGE%20MODELS%20FOR%20TEXT%20CLASSIFICATION-%20CASE%20STUDY%20AND%20COMPREHENSIVE%20REVIEW.pdf)



## Highlights

### BERT
- Introduced a bidirectional transformer-based pre-training model for language understanding.
- Achieved state-of-the-art results on various NLP tasks.

### DistilBERT
- A lightweight version of BERT with faster inference and smaller size while maintaining comparable performance.
- Suitable for real-world applications with limited resources.

### ULMFiT
- Proposed an effective fine-tuning method for pre-trained language models in text classification.
- Particularly strong on low-resource and non-English datasets.


---

More papers and summaries will be added here as the study progresses.

