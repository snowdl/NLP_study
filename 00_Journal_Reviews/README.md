# Journal Reviews

This folder contains summaries and related files for academic journal papers that we review as part of our NLP study.

## Folder Structure

00_Journal_Reviews/
├── distilbert/
│   └── DistilBERT.pdf
├── ULMFiT.pdf

## Reviewed Papers

- **DistilBERT: A distilled version of BERT — smaller, faster, cheaper and lighter**  
  [https://arxiv.org/pdf/1910.01108](https://arxiv.org/pdf/1910.01108)

- **ULMFiT: Universal Language Model Fine-tuning for Text Classification**  
  Summary PDF: [ULMFiT.pdf](./ULMFiT.pdf)  
  Highlights:
  - Achieves state-of-the-art results on multiple text classification benchmarks.
  - Especially effective for low-resource tasks, non-English languages, and new NLP problems.
  - Demonstrates the impact of language model pretraining and novel fine-tuning strategies.

---

More papers and summaries will be added here as the study progresses.
# Journal Reviews

This folder contains summaries and related files for academic journal papers that we review as part of our NLP study.

## Folder Structure

00_Journal_Reviews/
└── distilbert/
└── DistilBERT.pdf


Currently, there is only one paper reviewed here:

- **DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**  
  [https://arxiv.org/pdf/1910.01108](https://arxiv.org/pdf/1910.01108)

---

More papers and summaries will be added here as the study progresses.

