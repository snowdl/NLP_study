# ðŸ“š Journal Reviews

This folder contains summaries and key points from academic papers reviewed as part of the NLP study.  
Each entry links to the paper (if available) and the corresponding PDF or notes in this repository.

---

## ðŸ“„ Paper List

### 1. [BERT.pdf](./BERT.pdf)  
**Title:** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
- Introduced a bidirectional transformer-based pre-training model for language understanding.  
- Achieved state-of-the-art results on multiple NLP tasks.  
- **[Paper link](https://arxiv.org/abs/1810.04805)**

---

### 2. [DistilBERT.pdf](./DistilBERT.pdf)  
**Title:** DistilBERT: A distilled version of BERT â€” smaller, faster, cheaper, and lighter  
- Retains most of BERTâ€™s language understanding while being smaller and faster.  
- Suitable for resource-constrained real-world applications.  
- **[Paper link](https://arxiv.org/abs/1910.01108)**

---

### 3. [ULMFiT.pdf](./ULMFiT.pdf)  
**Title:** ULMFiT: Universal Language Model Fine-tuning for Text Classification  
- Proposed a transfer learning and fine-tuning method for text classification.  
- Particularly effective in low-resource and non-English datasets.  
- **[Paper link](https://arxiv.org/abs/1801.06146)**

---

### 4. [LARGE LANGUAGE MODELS FOR TEXT CLASSIFICATION- CASE STUDY AND COMPREHENSIVE REVIEW.pdf](./LARGE%20LANGUAGE%20MODELS%20FOR%20TEXT%20CLASSIFICATION-%20CASE%20STUDY%20AND%20COMPREHENSIVE%20REVIEW.pdf)  
**Title:** Large Language Models for Text Classification â€“ Case Study & Review  
- Benchmarks LLMs (e.g., LLaMA3, GPT-4, Mistral) and classic ML models (SVM, Naive Bayes).  
- Finds strong performance from large transformer models; classic models still viable on simpler tasks.  
- Highlights prompt engineering strategies (CoT, role prompting, etc.).  
- **[Paper link](https://arxiv.org/abs/2501.08457)**

---

### 5. [GPT_Clustering_2024.pdf](./GPT_Clustering_2024/GPT_Clustering_2024.pdf)  
**Title:** GPT Clustering for Large-Scale Text Data  
- Reviews clustering techniques leveraging GPT models for semantic grouping.  
- Explores combining embeddings with GPT for efficient large-scale clustering.  
- Discusses applications in document organization and topic modeling.  
- **[Paper link](https://arxiv.org/abs/2403.15112)**

---

### 6. [Evaluation (DOUBLE-BENCH)](./Evaluation/README.md)  
**Title:** Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?  
- Introduces DOUBLE-BENCH, a multilingual & multimodal benchmark for Document RAG.  
- Provides fine-grained evaluation across retrieval, selection, reasoning, and generation.  
- Addresses data contamination and supports diverse document types (text/tables/scans).  
- **[Paper link](https://arxiv.org/abs/2508.03644)**

---

### 7. [Reflexion_Language_Agents_with_RL.pdf](./Reflexion_Language_Agents_with_RL.pdf)  
**Title:** Reflexion: Language Agents with Verbal Reinforcement Learning  
- Enables agents to learn from verbal feedback via self-reflection.  
- Improves multi-step reasoning and decision-making through iterative refinement.  
- **[Paper link](https://arxiv.org/pdf/2303.11366)**

---

### 8. [CRAG.pdf](./CRAG/CRAG.pdf)  
**Title:** CRAG: A Comprehensive Retrieval-Augmented Generation Benchmark  
- Benchmarks RAG systems across tasks and domains, focusing on retrieval quality and grounded generation.  
- Designed to improve robustness and provide broad, extensible evaluation coverage.  
- **[Paper link](https://arxiv.org/pdf/2401.15884)**

---

### 9. [toolformer.pdf](./toolformer/toolformer.pdf)  
**Title:** Toolformer: Language Models Can Teach Themselves to Use Tools  
- Teaches LMs to call external tools/APIs (e.g., calculator, search) via self-annotated data.  
- Demonstrates gains on downstream tasks when tool usage is learned.  
- Introduces a self-supervised objective to acquire tool-calling behaviors.  
- **[Paper link](https://arxiv.org/abs/2303.16203)**


---

### 10. [ReAct_Reasoning_Acting_in_LLM.pdf](./ReAct/ReAct_Reasoning_Acting_in_LLM.pdf)  
**Title:** ReAct â€” Synergizing Reasoning and Acting in Language Models  
- Introduces a unified framework that combines reasoning (Thought) and acting (Action) in language models.  
- Enables explicit intermediate steps (Thought â†’ Action â†’ Observation â†’ Answer), improving transparency and error analysis.  
- Demonstrates effectiveness on tasks like question answering, fact-checking, and interaction with external tools.  
- Applied in this repo with a case study on the Iris dataset (ReAct vs Non-ReAct workflows).  
- **[Paper link](https://arxiv.org/abs/2210.03629)**

---

More papers and summaries will be added here as the study progresses.
