{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad1304a3-db39-42c5-a943-983f5d955a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greedy (argmax)\n",
    "#Always selects the token with the highest probability\n",
    "#→ Deterministic, always the same output\n",
    "#Sampling\n",
    "#Selects a token randomly according to the probability distribution\n",
    "#Example: softmax = [0.7, 0.2, 0.1] →70% chance for the first token,\n",
    "#20% chance for the second,\n",
    "#10% chance for the third\n",
    "#→ Nondeterministic, output may differ on each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "695c2afd-7594-442e-86e3-dd377555e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0572542-0fc1-4c8b-a10f-97c6490825d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70d47a76-546f-40cc-a3c7-863d17e2fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1098fac-f15a-4279-baf4-2be676096935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2513da6-1f30-4ec2-a7d4-b06128d8de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a76e3e64-32ea-4a3b-89ef-4a611e384f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In a distant future,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a distant future,\"\n",
    "print(\"prompt:\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "504c909a-9e0d-419d-9d63-41220898e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check PAD/EOS settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7bac3d0-31c9-4a2a-936a-6b726b444c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|endoftext|>\n",
      "eos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# distilgpt2 does not have a default pad_token, so we set it to eos.\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "print(\"pad_token:\", tok.pad_token)\n",
    "print(\"eos_token:\", tok.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa235b84-b534-45ac-8d3d-b2b8f800f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize input context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "adfcd17a-ae8d-46f5-9b97-f9eb0d8408de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctx shape: torch.Size([1, 5])\n",
      "ctx tokens: [818, 257, 12899, 2003, 11]\n",
      "ctx decoded: In a distant future,\n"
     ]
    }
   ],
   "source": [
    "ctx = tok(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "print(\"ctx shape:\", ctx.shape)\n",
    "print(\"ctx tokens:\", ctx[0].tolist())\n",
    "print(\"ctx decoded:\", tok.decode(ctx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a6093fd-ccea-4b02-a2d1-fd8ba08b7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft_block (for debugging: drafter also generates only 1 block using greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77e5ec0e-4124-459d-99ab-bc405dc40599",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def draft_block_greedy(drafter, input_ids, block_size=3, pad_token_id=None):\n",
    "    # Running on CPU → just align device\n",
    "    input_ids = input_ids.to(\"cpu\")\n",
    "    # When pad==eos, it's safer to explicitly provide attention_mask\n",
    "    attn = (input_ids != tok.pad_token_id)   # attention mask :creates an attention mask that sets padding token positions to 0 and keeps only the actual input tokens as 1\n",
    "\n",
    "    out = drafter.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attn,\n",
    "        max_new_tokens=block_size,\n",
    "        do_sample=False,          # disable sampling (sanity check)\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "    # Return only the newly generated block (after the prompt)\n",
    "    return out[:, input_ids.shape[1]:]   # shape: [1, block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a0eebf3-c09c-4b2a-b243-63fc0e481253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drafter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6ce3ef6-8272-4954-85ae-a2912eca6ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drafter ready\n"
     ]
    }
   ],
   "source": [
    "# Load drafter model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# drafter: small model (distilgpt2)\n",
    "drafter = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(\"cpu\")\n",
    "\n",
    "print(\"drafter ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b893d9f4-13a3-420b-b145-a06d85132672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First block proposal test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9afb6ad-a9f0-4a45-b7c8-efe90f5c9ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draft ids: [262, 995, 318]\n",
      "draft decoded:  the world is\n"
     ]
    }
   ],
   "source": [
    "# ctx는 이전 셀에서 만든 토큰 시퀀스\n",
    "block = draft_block_greedy(drafter, ctx, block_size=3, pad_token_id=tok.eos_token_id)\n",
    "\n",
    "print(\"draft ids:\", block[0].tolist())# Load verifier\n",
    "print(\"draft decoded:\", tok.decode(block[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f1fb6eb-a5ab-43f9-9c5e-ae41ba86424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifier= the same model as drafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4b12156-5dc1-4e79-bc9f-7f2a9c5507a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verifier ready \n"
     ]
    }
   ],
   "source": [
    "# Load verifier (debug mode: same model as drafter)\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "verifier = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(\"cpu\")\n",
    "print(\"verifier ready \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c136401-531c-4b9a-ac02-1a4925672b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate greedy sequence with verifier\n",
    "# Verifier: a model that checks whether the block proposed by the drafter is valid.\n",
    "# Greedy sequence generation: at each step, select the token with the highest probability (argmax) and append it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5c95302-868a-4198-b582-5b14d5efe40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy ids: [262, 995, 318]\n",
      "greedy decoded:  the world is\n"
     ]
    }
   ],
   "source": [
    "greedy_seq = []\n",
    "gctx = ctx.clone() # clone → keep ctx intact, work on a copy\n",
    "\n",
    "with torch.no_grad():  # no gradient calc (inference mode)\n",
    "    for _ in range(block.shape[1]):  # repeat = length of draft block\n",
    "        out = verifier(input_ids=gctx) # run verifier forward pass\n",
    "        g = out.logits[:, -1, :].argmax(dim=-1, keepdim=True) # greedy step (argmax)\n",
    "        greedy_seq.append(int(g.item()))  # save token id/ verifier baseline sequence\n",
    "        gctx = torch.cat([gctx, g], dim=1) # extend context with new token\n",
    "\n",
    "print(\"greedy ids:\", greedy_seq)\n",
    "print(\"greedy decoded:\", tok.decode(greedy_seq, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da83b377-1c5a-45fd-8bdf-f5001b5b3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare draft vs greedy token-by-token from the start and accept accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c437d1d4-47b3-447c-9171-003466dc4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] draft=' the' vs greedy=' the' -> ACCEPT\n",
      "[1] draft=' world' vs greedy=' world' -> ACCEPT\n",
      "[2] draft=' is' vs greedy=' is' -> ACCEPT\n",
      "accepted count: 3 / 3\n"
     ]
    }
   ],
   "source": [
    "accepted_ids = []\n",
    "cur = ctx.clone() # cur = current sequence\n",
    " \n",
    "for i in range(block.shape[1]):\n",
    "    d_id = int(block[0, i].item())\n",
    "    g_id = greedy_seq[i]\n",
    "    print(f\"[{i}] draft={tok.decode([d_id])!r} vs greedy={tok.decode([g_id])!r} ->\", end=\" \")\n",
    "\n",
    "    \n",
    "    if d_id == g_id:\n",
    "        #if they match → accept draft token,\n",
    "        accepted_ids.append(d_id)\n",
    "        cur = torch.cat([cur, block[:, i:i+1]], dim=1)\n",
    "        print(\"ACCEPT\")\n",
    "    else:\n",
    "        # if not → replace with greedy token and stop\n",
    "        g = torch.tensor([[g_id]], device=cur.device)\n",
    "        cur = torch.cat([cur, g], dim=1)\n",
    "        print(\"MISMATCH -> take greedy and STOP\")\n",
    "        break\n",
    "\n",
    "print(\"accepted count:\", len(accepted_ids), \"/\", block.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "189bfcaa-9887-4d9e-b50a-67d6f793afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check text after one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7669da2-9da6-42ca-8bda-042ecd0dd87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new text:\n",
      " In a distant future, the world is\n"
     ]
    }
   ],
   "source": [
    "print(\"new text:\\n\", tok.decode(cur[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77fe6109-874b-40d6-b737-0949b1cfd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-step function (pva_step_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc0e47be-d67f-4044-bc04-1732897e920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pva_step_once(ctx, block_size=3):\n",
    "    \"\"\"\n",
    "    Drafter proposes block_size tokens → \n",
    "    Verifier generates greedy predictions of the same length →\n",
    "    Accept only the matching prefix from the start, \n",
    "    at the first mismatch append the greedy token and stop.\n",
    "    Returns: new_ctx, accepted_count\n",
    "    \"\"\"\n",
    "    # 1) Draft (debug mode: greedy, no sampling)\n",
    "    block = draft_block_greedy(drafter, ctx, block_size=block_size, pad_token_id=tok.eos_token_id)\n",
    "\n",
    "    # 2) Verifier greedy sequence\n",
    "    greedy_seq = []\n",
    "    gctx = ctx.clone()\n",
    "    for _ in range(block.shape[1]):\n",
    "        out = verifier(input_ids=gctx)\n",
    "        g = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        greedy_seq.append(int(g.item()))\n",
    "        gctx = torch.cat([gctx, g], dim=1)\n",
    "\n",
    "    # 3) Compare and accept\n",
    "    accepted = 0\n",
    "    cur = ctx.clone()\n",
    "    for i in range(block.shape[1]):\n",
    "        d_id = int(block[0, i].item())\n",
    "        g_id = greedy_seq[i]\n",
    "        if d_id == g_id:\n",
    "            cur = torch.cat([cur, block[:, i:i+1]], dim=1)\n",
    "            accepted += 1\n",
    "        else:\n",
    "            g = torch.tensor([[g_id]], device=cur.device)\n",
    "            cur = torch.cat([cur, g], dim=1)\n",
    "            break\n",
    "    return cur, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "16de0593-bcf1-4339-b837-5e43dd4cbd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Draft vs Greedy comparison ---\n",
      "[0] draft=' the' vs greedy=' the' -> ACCEPT\n",
      "[1] draft=' world' vs greedy=' world' -> ACCEPT\n",
      "[2] draft=' is' vs greedy=' is' -> ACCEPT\n",
      "\n",
      "Accepted 3 / 3 tokens\n",
      "New ctx decoded: In a distant future, the world is\n"
     ]
    }
   ],
   "source": [
    "# Run one speculative step with block size 3\n",
    "new_ctx, accepted = pva_step_once(ctx, block_size=3)\n",
    "\n",
    "# For visualization: compare each token in the draft block with greedy\n",
    "block = draft_block_greedy(drafter, ctx, block_size=3, pad_token_id=tok.eos_token_id)\n",
    "\n",
    "greedy_seq = []\n",
    "gctx = ctx.clone()\n",
    "for _ in range(block.shape[1]):\n",
    "    out = verifier(input_ids=gctx)\n",
    "    g = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "    greedy_seq.append(int(g.item()))\n",
    "    gctx = torch.cat([gctx, g], dim=1)\n",
    "\n",
    "# Step-by-step comparison\n",
    "print(\"\\n--- Draft vs Greedy comparison ---\")\n",
    "for i in range(block.shape[1]):\n",
    "    d_id = int(block[0, i].item())\n",
    "    g_id = greedy_seq[i]\n",
    "    print(f\"[{i}] draft={tok.decode([d_id])!r} vs greedy={tok.decode([g_id])!r} -> \", end=\"\")\n",
    "    if d_id == g_id:\n",
    "        print(\"ACCEPT\")\n",
    "    else:\n",
    "        print(\"MISMATCH -> STOP\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nAccepted {accepted} / {block.shape[1]} tokens\")\n",
    "print(\"New ctx decoded:\", tok.decode(new_ctx[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cfaf0b56-906b-4097-a6b8-5b9521f36f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one more speculative step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97899f1d-5d20-49aa-bf24-c00ea159f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted this step: 3\n",
      "In a distant future, the world is in a state\n"
     ]
    }
   ],
   "source": [
    "# Run one more speculative step\n",
    "cur2, acc2 = pva_step_once(cur, block_size=3)\n",
    "\n",
    "# Number of draft tokens accepted in this step\n",
    "print(\"accepted this step:\", acc2)\n",
    "\n",
    "# Decode the updated context after this step\n",
    "print(tok.decode(cur2[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f6c395c-562d-4837-8f3e-45aca9ade234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short loop (run only 5 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04b7baa8-bd39-4281-b9d4-5e09e6d39f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1] accepted=3, text='In a distant future, the world is'\n",
      "[step 2] accepted=3, text='In a distant future, the world is in a state'\n",
      "[step 3] accepted=3, text='In a distant future, the world is in a state of flux.'\n",
      "[step 4] accepted=3, text='In a distant future, the world is in a state of flux. The world is'\n",
      "[step 5] accepted=3, text='In a distant future, the world is in a state of flux. The world is in a state'\n"
     ]
    }
   ],
   "source": [
    "ctx_loop = ctx.clone()\n",
    "for step in range(5):\n",
    "    # Run one speculative decoding step (drafter + verifier)\n",
    "    ctx_loop, acc = pva_step_once(ctx_loop, block_size=3)\n",
    "\n",
    "    # Print how many draft tokens were accepted and the current decoded text\n",
    "    print(f\"[step {step+1}] accepted={acc}, text='{tok.decode(ctx_loop[0], skip_special_tokens=True)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17e81a57-338c-4057-bbdb-957ba9e6a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling version - drafter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fbdf6c1c-f188-4396-bac0-91d115040b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def draft_block_sampled(\n",
    "    drafter, input_ids, block_size=3,\n",
    "    top_k=20, top_p=0.9, temperature=0.7,\n",
    "    pad_token_id=None, repetition_penalty=1.05, no_repeat_ngram_size=3\n",
    "):\n",
    "    # Move input to CPU\n",
    "    input_ids = input_ids.to(\"cpu\")\n",
    "    # Build attention mask (ignore pad tokens)\n",
    "    attn = (input_ids != tok.pad_token_id)\n",
    "\n",
    "    # Drafter generates a block of tokens using sampling\n",
    "    out = drafter.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attn,\n",
    "        max_new_tokens=block_size,\n",
    "        do_sample=True,                # enable sampling instead of greedy\n",
    "        top_k=top_k,                   # restrict to top-k candidates\n",
    "        top_p=top_p,                   # nucleus sampling (top cumulative probability p)\n",
    "        temperature=temperature,       # controls randomness (lower = more greedy)\n",
    "        pad_token_id=pad_token_id,     # pad token handling\n",
    "        repetition_penalty=repetition_penalty,  # penalize repeating tokens\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,  # block repeated n-grams\n",
    "    )\n",
    "    # Return only the newly generated tokens\n",
    "    return out[:, input_ids.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f8d8463-046f-48e3-9c92-81a40dccbf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy draft :  not yet clear.\n",
      "\n",
      "Sampled draft:  still a mystery, but\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"The future of AI is\"\n",
    "ctx = tok(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Greedy draft block (deterministic)\n",
    "block_greedy = draft_block_greedy(\n",
    "    drafter, ctx, block_size=5, pad_token_id=tok.eos_token_id\n",
    ")\n",
    "\n",
    "# Sampling draft block (nondeterministic)\n",
    "block_sampled = draft_block_sampled(\n",
    "    drafter, ctx, block_size=5,\n",
    "    top_k=20, top_p=0.9, temperature=0.7,\n",
    "    pad_token_id=tok.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode both results\n",
    "print(\"Greedy draft :\", tok.decode(block_greedy[0], skip_special_tokens=True))\n",
    "print(\"Sampled draft:\", tok.decode(block_sampled[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95116f72-9726-4c06-a35d-dca1404b406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic mode: one speculative decoding step (pva_step_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca38f3de-fc48-4707-a4f4-9c234f8ebf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "#pva_step_real: drafter uses guided drafting + sampling → closer to actual speculative decoding behavior\n",
    "def pva_step_real(ctx, block_size=3): \n",
    "    block = draft_block_guided(\n",
    "        drafter, verifier, ctx, block_size=block_size,\n",
    "        guide_topk=20,       \n",
    "        sample_topk=40, top_p=0.95, temperature=0.7,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "        repetition_penalty=1.05, no_repeat_ngram_size=3\n",
    "    )\n",
    "\n",
    " # Verifier greedy decoding (same as before)\n",
    "    greedy_seq = []\n",
    "    gctx = ctx.clone()\n",
    "    for _ in range(block.shape[1]):\n",
    "        attn = (gctx != tok.pad_token_id)\n",
    "        out = verifier(input_ids=gctx, attention_mask=attn)\n",
    "        g = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        greedy_seq.append(int(g.item()))\n",
    "        gctx = torch.cat([gctx, g], dim=1)\n",
    "\n",
    "    \n",
    "    # Compare draft vs greedy and accept prefix\n",
    "    accepted = 0\n",
    "    cur = ctx.clone()\n",
    "    for i in range(block.shape[1]):\n",
    "        d_id = int(block[0, i])\n",
    "        g_id = greedy_seq[i]\n",
    "        if d_id == g_id:\n",
    "            cur = torch.cat([cur, block[:, i:i+1]], dim=1)\n",
    "            accepted += 1\n",
    "        else:\n",
    "            g = torch.tensor([[g_id]], device=cur.device)\n",
    "            cur = torch.cat([cur, g], dim=1)\n",
    "            break\n",
    "    return cur, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a194eeca-000e-4f67-9b1c-8c5c7cddf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guided drafting (wrapper around sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88f87063-b98b-42ac-a37e-bc71d9321236",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def draft_block_guided(\n",
    "    drafter, verifier, input_ids, block_size=3,\n",
    "    guide_topk=20, sample_topk=40, top_p=0.95, temperature=0.7,\n",
    "    pad_token_id=None, repetition_penalty=1.05, no_repeat_ngram_size=3\n",
    "):\n",
    "    # Move to CPU & build attention mask\n",
    "    input_ids = input_ids.to(\"cpu\")\n",
    "    attn = (input_ids != tok.pad_token_id) if pad_token_id is not None else None\n",
    "\n",
    "    # Just sample with the drafter (no fancy guiding)\n",
    "    out = drafter.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attn,\n",
    "        max_new_tokens=block_size,\n",
    "        do_sample=True,\n",
    "        top_k=sample_topk,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=pad_token_id,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "    )\n",
    "    # Return only the newly generated tokens (the block)\n",
    "    return out[:, input_ids.shape[1]:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0960ced9-d999-4888-83ee-88b65fd8c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== pva_step_once (greedy drafter, debug mode) ===\n",
      "Accepted tokens: 5\n",
      "Decoded text   : The future of AI is not yet clear.\n",
      "\n",
      "\n",
      "=== pva_step_real (guided + sampling drafter, realistic mode) ===\n",
      "Accepted tokens: 0\n",
      "Decoded text   : The future of AI is not\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"The future of AI is\"\n",
    "ctx = tok(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Run one speculative step (debug mode: greedy drafter)\n",
    "ctx_once, acc_once = pva_step_once(ctx, block_size=5)\n",
    "\n",
    "# Run one speculative step (realistic mode: guided + sampling drafter)\n",
    "ctx_real, acc_real = pva_step_real(ctx, block_size=5)\n",
    "\n",
    "# Decode outputs\n",
    "print(\"=== pva_step_once (greedy drafter, debug mode) ===\")\n",
    "print(\"Accepted tokens:\", acc_once)\n",
    "print(\"Decoded text   :\", tok.decode(ctx_once[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\n=== pva_step_real (guided + sampling drafter, realistic mode) ===\")\n",
    "print(\"Accepted tokens:\", acc_real)\n",
    "print(\"Decoded text   :\", tok.decode(ctx_real[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "19407c99-4993-4104-a297-e66ee1f38e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted (real step): 1\n",
      "The future of AI is not yet\n"
     ]
    }
   ],
   "source": [
    " # Test a single step in realistic mode\n",
    "ctx_real = ctx.clone()\n",
    "ctx_real, acc_real = pva_step_real(ctx_real, block_size=3)\n",
    "\n",
    "# Print how many draft tokens were accepted in this step\n",
    "print(\"accepted (real step):\", acc_real)\n",
    "\n",
    "# Decode and print the updated context as text\n",
    "print(tok.decode(ctx_real[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af4e905f-32fa-4b6f-9302-e2e80446cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short loop (realistic mode, run 8 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a23a80e2-e5ed-480c-91a5-d71ac223a44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] accepted=0, acc_rate_so_far=0.0%\n",
      "[2] accepted=0, acc_rate_so_far=0.0%\n",
      "[3] accepted=2, acc_rate_so_far=33.3%\n",
      "[4] accepted=0, acc_rate_so_far=25.0%\n",
      "[5] accepted=2, acc_rate_so_far=40.0%\n",
      "[6] accepted=0, acc_rate_so_far=33.3%\n",
      "[7] accepted=0, acc_rate_so_far=28.6%\n",
      "[8] accepted=0, acc_rate_so_far=25.0%\n",
      "text:\n",
      "The future of AI is not yet clear.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Short loop: run 8 steps in realistic mode\n",
    "def run_short_real(ctx0, steps=8, block_size=3):\n",
    "    ctx = ctx0.clone()\n",
    "    proposed = accepted = 0\n",
    "    for i in range(steps):\n",
    "        ctx, acc = pva_step_real(ctx, block_size=block_size)\n",
    "        proposed += block_size\n",
    "        accepted += acc\n",
    "        print(f\"[{i+1}] accepted={acc}, acc_rate_so_far={round(100*accepted/proposed,1)}%\")\n",
    "    print(\"text:\")\n",
    "    print(tok.decode(ctx[0], skip_special_tokens=True))\n",
    "    return ctx\n",
    "\n",
    "_ = run_short_real(ctx, steps=8, block_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009a08b-66ac-42de-be6f-664aa22b9443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bpd_env)",
   "language": "python",
   "name": "bpd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
