{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02db0a57-bcda-4cc1-8813-4e9606e39848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSummarization\\n→ sshleifer/distilbart-cnn-12-6\\nA model optimized for condensing long Wikipedia texts into concise summaries.\\n\\nEmbedding\\n→ distilbert-base-uncased (via SentenceTransformer)\\nUsed to convert text into vector embeddings for semantic similarity calculation or extracting important sentences.\\n\\nQuestion Answering (Q&A) Generation\\n→ google/flan-t5-small\\nGenerates natural answers to questions based on the summarized content or embedding results.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summarization\n",
    "→ sshleifer/distilbart-cnn-12-6\n",
    "A model optimized for condensing long Wikipedia texts into concise summaries.\n",
    "\n",
    "Embedding\n",
    "→ distilbert-base-uncased (via SentenceTransformer)\n",
    "Used to convert text into vector embeddings for semantic similarity calculation or extracting important sentences.\n",
    "\n",
    "Question Answering (Q&A) Generation\n",
    "→ google/flan-t5-small\n",
    "Generates natural answers to questions based on the summarized content or embedding results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ac58934-3edf-4bee-8107-05dddf093a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58778af4-0595-42a8-9a77-51985829e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wikipedia page content retrieval function with error handling\n",
    "def wiki_search(query):\n",
    "    try:\n",
    "        # Try to get the exact Wikipedia page content without auto-suggestion\n",
    "        return wikipedia.page(query, auto_suggest=False).content\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        # Handle case when the page does not exist\n",
    "        return \"Page not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd0a6b45-0d62-48f5-851f-9c6f5f295b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Why we chose this model:\\n# - \"distilbart-cnn-12-6\" is a distilled, smaller version of BART optimized for summarization.\\n# - It balances performance and speed, making it suitable for environments with limited resources.\\n# - Its lightweight nature allows faster inference on CPU, like on a Mac, while still producing good-quality summaries.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Setting : \"sshleifer/distilbart-cnn-12-6\"\n",
    "\"\"\"\n",
    "# Why we chose this model:\n",
    "# - \"distilbart-cnn-12-6\" is a distilled, smaller version of BART optimized for summarization.\n",
    "# - It balances performance and speed, making it suitable for environments with limited resources.\n",
    "# - Its lightweight nature allows faster inference on CPU, like on a Mac, while still producing good-quality summaries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f66169fc-48f9-4200-868e-d042f76506f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model setting for text summarization\n",
    "summary_model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "\n",
    "# Load tokenizer for the summarization model\n",
    "summary_tokenizer = AutoTokenizer.from_pretrained(summary_model_name)\n",
    "\n",
    "# Load pre-trained Seq2Seq model for summarization tasks\n",
    "summary_model = AutoModelForSeq2SeqLM.from_pretrained(summary_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c81ab264-51ec-4e8c-910b-b06394806703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name distilbert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# 3. Setting up the sentence embedding model\n",
    "embedding_model = SentenceTransformer('distilbert-base-uncased')\n",
    "\n",
    "# Explanation:\n",
    "# - 'distilbert-base-uncased' is a smaller, faster version of BERT.\n",
    "# - It efficiently converts sentences into fixed-size vector embeddings.\n",
    "# - These embeddings help measure semantic similarity, useful for tasks like filtering relevant sentences or clustering.\n",
    "# - Its lightweight architecture makes it suitable for CPU-based environments without sacrificing too much accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dd95ea8-4ff8-46f6-b995-788fa086ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Setting up the Question Answering (Q&A) generation model\n",
    "qa_model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Load tokenizer for the Q&A model\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "\n",
    "# Load pre-trained Seq2Seq model for generating answers to questions\n",
    "qa_model = AutoModelForSeq2SeqLM.from_pretrained(qa_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dea23567-8899-439c-b753-207370503ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Document summarization function\n",
    "def summarize(text):\n",
    "    # Tokenize the input text with truncation and max length\n",
    "    inputs = summary_tokenizer([text], max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate the summary using the model with specific generation parameters:\n",
    "    # - max_length: maximum tokens in the summary\n",
    "    # - min_length: minimum tokens in the summary\n",
    "    # - length_penalty: encourages shorter or longer summaries (2.0 favors shorter)\n",
    "    # - num_beams: beam search size for better quality\n",
    "    # - early_stopping: stop when an end condition is met\n",
    "    summary_ids = summary_model.generate(\n",
    "        **inputs,\n",
    "        max_length=150,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated token ids to readable text, skipping special tokens\n",
    "    summary = summary_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e30eaba2-de91-4c63-b294-46c384b46a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Question Answering (Q&A) function\n",
    "def generate_answer(question, context):\n",
    "    # Prepare the prompt by combining the question and context\n",
    "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the prompt into model input format\n",
    "    inputs = qa_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate the answer tokens with a limit on max new tokens\n",
    "    outputs = qa_model.generate(**inputs, max_new_tokens=50)\n",
    "    \n",
    "    # Decode the generated tokens back to readable text, ignoring special tokens\n",
    "    answer = qa_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28b041b7-4c64-4feb-b96b-65c3cfcaaef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making . High-profile applications of AI include advanced web search engines (e.g., Google Search) and recommendation systems (used by YouTube, Amazon, and Netflix) Some companies, like OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence .\n"
     ]
    }
   ],
   "source": [
    "query = \"Artificial intelligence\"\n",
    "wiki_text = wiki_search(query)\n",
    "summary = summarize(wiki_text)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0c2ad36-f2e5-4db9-b52f-f566185c709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: computational systems to perform tasks\n"
     ]
    }
   ],
   "source": [
    "question = \"What is artificial intelligence?\"\n",
    "answer = generate_answer(question, summary)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c457e41-98bd-469f-9778-8bee8e24b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan is an island country in East Asia. Located in the Pacific Ocean off the northeast coast of the Asian mainland, it is bordered to the west by the Sea of Japan and extends from the Sea of Okhotsk in the north to the East China Sea in the south. The Japanese archipelago consists of four major islands alongside 14,121 smaller islands, covering 377,975 square kilometers (145,937 sq mi). Divided into 47 administrative prefectures and eight traditional regions, about 75% of the country's terrain \n"
     ]
    }
   ],
   "source": [
    "query = \"Japan\"\n",
    "wiki_text = wiki_search(query)\n",
    "print(wiki_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183d357-97d0-4297-8a87-db2c51f303a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pyenv)",
   "language": "python",
   "name": "pyenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
