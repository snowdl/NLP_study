{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cec392e-bb3e-4232-8a5a-d70f30844407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langgraph\n",
      "Version: 0.6.2\n",
      "Summary: Building stateful, multi-actor applications with LLMs\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License-Expression: MIT\n",
      "Location: /Users/jessicahong/.pyenv/versions/3.11.11/lib/python3.11/site-packages\n",
      "Requires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk, pydantic, xxhash\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28251a4-3429-40dc-830e-8bd6f083d852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import langgraph\n",
    "print(dir(langgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57f1296-52c1-4231-a62b-e3a4794c9ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[사용자 입력]\\n     ↓\\n[LangGraph로 제어 흐름 구성]\\n     ↓\\n[1. Toolformer 스타일로 도구 선택]\\n     ↓\\n[2. Tavily로 Web + Wikipedia 검색 (or custom retriever)]\\n     ↓\\n[3. CRAG로 요약 + QA 생성 (예: DistilBART + FLAN-T5)]\\n     ↓\\n[4. LangChain으로 전체 파이프라인 연결]\\n     ↓\\n[5. sklearn/evaluation을 통해 결과 평가 및 튜닝]\\n     ↓\\n[최종 응답 반환]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"[사용자 입력]\n",
    "     ↓\n",
    "[LangGraph로 제어 흐름 구성]\n",
    "     ↓\n",
    "[1. Toolformer 스타일로 도구 선택]\n",
    "     ↓\n",
    "[2. Tavily로 Web + Wikipedia 검색 (or custom retriever)]\n",
    "     ↓\n",
    "[3. CRAG로 요약 + QA 생성 (예: DistilBART + FLAN-T5)]\n",
    "     ↓\n",
    "[4. LangChain으로 전체 파이프라인 연결]\n",
    "     ↓\n",
    "[5. sklearn/evaluation을 통해 결과 평가 및 튜닝]\n",
    "     ↓\n",
    "[최종 응답 반환]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e69dd8-1295-4923-a3a4-90fe62ddd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from transformers import pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c71acd-2163-4899-bb9f-687b68c887b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2510ff30-7391-415d-841f-54b6268cefc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# 2. 모델 준비\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", tokenizer=\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090350b2-d488-47ca-adff-999fa3880873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results: ['Donald Trump', 'Donald Trump Jr.', 'Donald Trump sexual misconduct allegations', 'Trump family', 'Second cabinet of Donald Trump', 'Donald Trump and golf', 'Age and health concerns about Donald Trump', 'Donald Trump and religion', 'False or misleading statements by Donald Trump', 'Trump–Musk feud']\n",
      "Donald John Trump (born June 14, 1946) is an American politician, media personality, and businessman who is the 47th president of the United States. A member of the Republican Party, he served as the 45th president from 2017 to 2021.\n",
      "Born into a wealthy family in New York City, Trump graduated from the University of Pennsylvania in 1968 with a bachelor's degree in economics. He became the president of his family's real estate business in 1971, renamed it the Trump Organization, and began acquiri\n",
      "... [중략] ...\n"
     ]
    }
   ],
   "source": [
    "def fetch_wiki_content(query):\n",
    "    try:\n",
    "        search_results = wikipedia.search(query)\n",
    "        if not search_results:\n",
    "            return \"No pages found for this query.\"\n",
    "        \n",
    "        # search_results 출력해서 확인\n",
    "        print(\"Search results:\", search_results)\n",
    "        \n",
    "        # search_results 중 첫 번째부터 차례로 시도해보기\n",
    "        for title in search_results:\n",
    "            try:\n",
    "                page = wikipedia.page(title, auto_suggest=False)\n",
    "                return page.content\n",
    "            except wikipedia.PageError:\n",
    "                continue\n",
    "            except wikipedia.DisambiguationError:\n",
    "                continue\n",
    "        \n",
    "        return \"Page not found. Try another query.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n",
    "\n",
    "result = fetch_wiki_content(\"Donald Trump\")\n",
    "\n",
    "if isinstance(result, str) and len(result) > 500:\n",
    "    print(result[:500] + \"\\n... [중략] ...\")\n",
    "else:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83a8314b-95ce-454b-a710-1283be9eeaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_long_text(text, chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        input_len = len(chunk.split())\n",
    "        max_len = max(min(input_len // 3, 150), 10)\n",
    "        min_len = min(40, max_len - 1)\n",
    "        if min_len < 10:\n",
    "            min_len = 10\n",
    "        \n",
    "        summary = summarizer(chunk, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    combined_summary = \" \".join(summaries)\n",
    "    \n",
    "    combined_len = len(combined_summary.split())\n",
    "    final_max_len = max(min(combined_len // 3, 200), 50)\n",
    "    final_min_len = min(40, final_max_len - 1)\n",
    "    if final_min_len < 10:\n",
    "        final_min_len = 10\n",
    "    \n",
    "    final_summary = summarizer(combined_summary, max_length=final_max_len, min_length=final_min_len, do_sample=False)[0]['summary_text']\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77313af-da41-434a-8e66-2aa79671fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crag_summarize_and_qa(wiki_content, question):\n",
    "    summary = summarize_long_text(wiki_content)\n",
    "    \n",
    "    # question과 context를 하나의 문자열로 합침 (FLAN-T5 스타일)\n",
    "    input_text = f\"question: {question} context: {summary}\"\n",
    "    \n",
    "    answer = qa_pipeline(input_text)[0]['generated_text']\n",
    "    return summary, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b416a881-cdef-4486-9a87-e6a6b8757e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results: ['Natural language processing', 'Natural language', 'Natural-language user interface', 'Quantum natural language processing', 'History of natural language processing', 'Outline of natural language processing', 'Natural Language Toolkit', 'Semantic decomposition (natural language processing)', 'Natural language understanding', 'Empirical Methods in Natural Language Processing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1443 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "  The study of NLP, a subfield of computer science, is generally associated with artificial intelligence . NLP is related to information retrieval, knowledge representation, computational linguistics, and more broadly with linguistics . The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English . Since 2015, the statistical approach has been replaced by the neural networks approach .\n",
      "\n",
      "Answer:\n",
      " natural language processing\n"
     ]
    }
   ],
   "source": [
    "def run_all_in_one(query):\n",
    "    wiki_text = fetch_wiki_content(query)\n",
    "    \n",
    "    if wiki_text.startswith(\"Page not found\") or wiki_text.startswith(\"No pages found\") or wiki_text.startswith(\"Query ambiguous\"):\n",
    "        print(\"Error fetching wiki content:\", wiki_text)\n",
    "        return\n",
    "    \n",
    "    question = f\"What is {query}?\"\n",
    "    summary, answer = crag_summarize_and_qa(wiki_text, question)\n",
    "    \n",
    "    print(\"Summary:\\n\", summary)\n",
    "    print(\"\\nAnswer:\\n\", answer)\n",
    "\n",
    "# 실행 예시\n",
    "run_all_in_one(\"Natural language processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509554b6-cbb8-48ea-9fa0-2625ca26dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results: ['Noam Chomsky bibliography and filmography', 'Political positions of Noam Chomsky', 'Noam Chomsky', 'Is the Man Who Is Tall Happy?', 'Carol Chomsky', 'Decoding Chomsky', 'William Chomsky', 'List of honorary degrees awarded to Noam Chomsky', 'Manufacturing Consent', 'The Withdrawal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "  This is a list of writings published by the American writer Noam Chomsky . Chomsky's full b  Chomsky wrote in preparation for his PhD thesis, including hand-written notes made in preparation of the 1975 book, is available as a 149 MiB, 919 page  Roman Jakobson (Ph.D.) — (Jun 1955). Transformational Analysis . University of Pennsylvania . Chomsky has written numerous books about language .\n",
      "\n",
      "Answer:\n",
      " Chomsky's full b Chomsky wrote in preparation for his PhD thesis, including hand-written notes made in preparation of the 1975 book, is available as a 149 MiB, 919 page Roman Jakobson (Ph.D.) — (Jun 1955). Transformational Analysis . University of Pennsylvania\n"
     ]
    }
   ],
   "source": [
    "run_all_in_one(\"Who is Noam Chomsky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e59f88b-5a38-4fed-ab37-f42438699422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Langchain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8bcf874-ae64-4f91-a505-664a2b884d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "# 기존에 있던 fetch_wiki_content 함수\n",
    "def tool_fetch_wiki_content(query: str) -> str:\n",
    "    return fetch_wiki_content(query)\n",
    "\n",
    "# 기존에 있던 crag_summarize_and_qa 함수 (요약 + QA)\n",
    "def tool_crag_summarize_and_qa(wiki_content: str, question: str) -> dict:\n",
    "    summary, answer = crag_summarize_and_qa(wiki_content, question)\n",
    "    return {\"summary\": summary, \"answer\": answer}\n",
    "\n",
    "# Tool로 래핑\n",
    "wiki_tool = Tool(\n",
    "    name=\"Wikipedia Search\",\n",
    "    func=tool_fetch_wiki_content,\n",
    "    description=\"Given a topic string, searches Wikipedia and returns content.\"\n",
    ")\n",
    "\n",
    "crag_tool = Tool(\n",
    "    name=\"CRAG Summarizer and QA\",\n",
    "    func=tool_crag_summarize_and_qa,\n",
    "    description=\"Given wiki content and a question, returns summarized content and answer.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "927ef2bb-fa74-42b6-9070-08c0d4c52efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utilities import WikipediaAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41e4efc1-e6ae-49e3-bed7-03e6d0ca1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] =\"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\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 이제 llm 쓸 준비 완료\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "318ff366-a405-4d6a-9f04-5c99d858f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/xtl4b0cx1cs9zrr9n5y814_h0000gn/T/ipykernel_41540/3253022580.py:23: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = agent.run(\"Tell me about Natural Language Processing.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use Wikipedia to gather information about Natural Language Processing.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Natural Language Processing\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Natural language processing\n",
      "Summary: Natural language processing (NLP) is the processing of natural language information by a computer. The study of NLP, a subfield of computer science, is generally associated with artificial intelligence. NLP is related to information retrieval, knowledge representation, computational linguistics, and more broadly with linguistics.\n",
      "Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation.\n",
      "\n",
      "\n",
      "\n",
      "Page: Natural language\n",
      "Summary: A natural language or ordinary language is a language that occurs organically in a human community by a process of use, repetition, and change and in forms such as written, spoken and signed. Categorization as natural includes languages associated with linguistic prescriptivism or language regulation, but excludes constructed and formal languages such as those used for computer programming and logic. Nonstandard dialects can be viewed as a wild type in comparison with standard languages. An official language with a regulating academy such as Standard French, overseen by the Académie Française, is classified as a natural language (e.g. in the field of natural language processing), as its prescriptive aspects do not make it constructed enough to be a constructed language or controlled enough to be a controlled natural language.\n",
      "Categorization as natural excludes:\n",
      "\n",
      "Artificial and constructed languages\n",
      "Constructed international auxiliary languages\n",
      "Non-human communication systems in nature such as whale and other marine mammal vocalizations or honey bees' waggle dance.\n",
      "\n",
      "Page: Natural-language user interface\n",
      "Summary: Natural-language user interface (LUI or NLUI) is a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.\n",
      "In interface design, natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input.\n",
      "Natural-language interfaces are an active area of study in the field of natural-language processing and computational linguistics. An intuitive general natural-language interface is one of the active goals of the Semantic Web.\n",
      "Text interfaces are \"natural\" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a \"shallow\" natural-language user interface.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have gathered information about Natural Language Processing from Wikipedia.\n",
      "\n",
      "Thought: I should now provide a summary of the information gathered.\n",
      "\n",
      "Final Answer: Natural Language Processing (NLP) is the processing of natural language information by a computer. It is a subfield of computer science associated with artificial intelligence and is related to information retrieval, knowledge representation, computational linguistics, and linguistics. Major processing tasks in an NLP system include speech recognition, text classification, natural language understanding, and natural language generation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Natural Language Processing (NLP) is the processing of natural language information by a computer. It is a subfield of computer science associated with artificial intelligence and is related to information retrieval, knowledge representation, computational linguistics, and linguistics. Major processing tasks in an NLP system include speech recognition, text classification, natural language understanding, and natural language generation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "wiki = WikipediaAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wiki.run,\n",
    "        description=\"Useful for answering questions about general topics.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=\"chat-zero-shot-react-description\",  # 여기 바뀜\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "response = agent.run(\"Tell me about Natural Language Processing.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc37623-6c18-44ec-9c5c-409c68265379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pyenv)",
   "language": "python",
   "name": "pyenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
