# ðŸ§® Core Math Concepts for NLP & Machine Learning

This folder contains a curated set of study notes that build the mathematical foundation necessary for understanding machine learning and natural language processing (NLP).

These notes support my graduate-level preparation by deepening my understanding of key topics such as derivatives, gradients, probability, and optimization theory.

---

## ðŸ“˜ Included Topics

### âœ… Calculus & Optimization
- `06-15 Derivatives, Chain Rule, and Binomial Coefficients - Study Notes.md`:  
  Introduction to derivatives, chain rule applications, and their roles in model training.  
  Also includes combinatorics fundamentals.

- `0616_Partial_Derivatives_and_Gradient.md`:  
  Explains multivariate calculus concepts like partial derivatives and gradient vectors, essential for understanding backpropagation and optimization in deep learning.

### âœ… Probability & Inference
- `06-17 Bayes_Theorem_Notes.md`:  
  Covers conditional probability, Bayes' theorem, and their role in probabilistic models such as Naive Bayes classifiers.

### âœ… Additional Topics
- Logarithmic identities and their application to log-likelihood functions  
- Limit theory & Îµ-Î´ definitions  
- Logistic regression cost function derivation

---

## ðŸŽ¯ Purpose

These notes are designed to:
- Provide a clear, intuitive grasp of the math behind ML/NLP models  
- Serve as quick references for future study and project work  
- Act as foundational materials for my upcoming master's applications

---

## ðŸ”— Related Areas

- See [`linear_models`](../linear_models) for applied models built upon these math concepts.  
- See [`nlp_basics`](../nlp_basics) for how these principles support tokenization, vectorization, and classification in NLP.


