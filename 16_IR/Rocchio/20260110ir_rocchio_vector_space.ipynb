{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31665d47-9ccf-40b6-8906-43fa28c263b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d08e55fe-5d5c-4983-bd8a-c67e9273631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Data preparation\n",
    "# =========================\n",
    "vocab = [\"China\", \"Japan\", \"Tokyo\", \"Beijing\"]\n",
    "term2idx = {term: i for i, term in enumerate(vocab)}\n",
    "\n",
    "class1_texts = [\"China China Tokyo\", \"China Beijing\"]\n",
    "class2_texts = [\"Japan Japan Tokyo\", \"Japan Beijing Beijing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21291c89-8ffa-48dc-b1c5-434875a377fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "602e3c42-26fe-4033-9780-c4dd150a7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tf(text: str, term2idx: dict, vocab_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert raw text into a raw term-frequency (tf) vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input document text (space-separated tokens)\n",
    "    term2idx : dict\n",
    "        Mapping from term to index (e.g., \"China\" -> 0)\n",
    "    vocab_size : int\n",
    "        Size of the vocabulary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Raw tf vector of shape (vocab_size,)\n",
    "    \"\"\"\n",
    "\n",
    "    # At initial state, generate an empty vector\n",
    "    # Example: [0.0, 0.0, 0.0, 0.0]\n",
    "    vec = np.zeros(vocab_size, dtype=float)\n",
    "\n",
    "    # Tokenize text and count term frequency\n",
    "    for token in text.split():\n",
    "        # If the token exists in the vocabulary, increment its count\n",
    "        if token in term2idx:\n",
    "            vec[term2idx[token]] += 1.0\n",
    "\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0dda0b6-1170-4e12-8b2a-f73909ecacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 0. 1. 0.]\n",
      "[0. 1. 0. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(text_to_tf(\"China China Tokyo\", term2idx, len(vocab)))\n",
    "print(text_to_tf(\"Japan Beijing Beijing\", term2idx, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "359e3e5b-e3c2-42ad-91de-14eb55eb9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(vec: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform L2 normalization on a vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : np.ndarray\n",
    "        Input vector (e.g., raw term-frequency vector)\n",
    "    eps : float\n",
    "        Small constant (epsilon) to avoid division by zero\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        L2-normalized vector\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1. Compute the L2 norm (length) of the vector\n",
    "    # Mathematical definition:\n",
    "    # ||d|| = sqrt(d1^2 + d2^2 + ... + dn^2)\n",
    "    norm = np.linalg.norm(vec)\n",
    "\n",
    "    print(\"vec:\", vec)\n",
    "    print(\"norm:\", norm)\n",
    "    print(\"vec / norm:\", vec / norm)\n",
    "    \n",
    "\n",
    "    # Step 2. Safety check\n",
    "    # If the norm is zero (or extremely close to zero),\n",
    "    # normalization would cause division by zero.\n",
    "    # In that case, return a copy of the original vector.\n",
    "    if norm < eps:\n",
    "        return vec.copy()\n",
    "\n",
    "    # Step 3. Normalize the vector\n",
    "    # Mathematical definition:\n",
    "    # d_hat = d / ||d||\n",
    "    # This preserves the direction of the vector\n",
    "    # while scaling its length to 1.\n",
    "    return vec / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54013962-b666-4bad-8f28-e8a9789d9801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec: [2. 0. 1. 0.]\n",
      "norm: 2.23606797749979\n",
      "vec / norm: [0.89442719 0.         0.4472136  0.        ]\n"
     ]
    }
   ],
   "source": [
    "v = text_to_tf(\"China China Tokyo\", term2idx, len(vocab))\n",
    "v_hat = l2_normalize(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "136a337b-3fd6-43ac-be96-8fc2d57df343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get centriod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dcbe8849-87a9-4660-85c7-7a6c4127e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_from_texts(texts, term2idx, vocab_size):\n",
    "    #preprocessing : tern the document to rawtf \n",
    "    #orchestrates the process by applying the document-level transformations to all documents and then averaging them\n",
    "    docs_raw = [text_to_tf(t, term2idx, vocab_size) for t in texts]\n",
    "    #using l2_normalize to perform L2 normalization \n",
    "    docs_hat = [l2_normalize(d) for d in docs_raw]\n",
    "    #compute average = Rocchio\n",
    "    mu = np.mean(docs_hat, axis=0)\n",
    "    \n",
    "    return np.array(docs_raw), np.array(docs_hat), mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5270136-bf5d-441c-b3d2-19dcdc6e46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rocchio Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de0ed36b-6ff4-4e12-834a-08312ca609dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_euclid(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    diff = a - b\n",
    "    return float(np.dot(diff, diff))\n",
    "\n",
    "\n",
    "def rocchio_predict(x_hat: np.ndarray, centroids: dict):\n",
    "    # centroids: {\"ClassName\": mu_vector, ...}\n",
    "    dists = {name: sq_euclid(x_hat, mu) for name, mu in centroids.items()}\n",
    "    pred = min(dists, key=dists.get)\n",
    "    return pred, dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ee66760-d910-4370-95cd-f7e65efe47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test document (user input / test document)\n",
    "test_text = \"China Tokyo Beijing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd8f97d0-364c-45bf-b42a-0b8a181d4308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec: [2. 0. 1. 0.]\n",
      "norm: 2.23606797749979\n",
      "vec / norm: [0.89442719 0.         0.4472136  0.        ]\n",
      "vec: [1. 0. 0. 1.]\n",
      "norm: 1.4142135623730951\n",
      "vec / norm: [0.70710678 0.         0.         0.70710678]\n",
      "vec: [0. 2. 1. 0.]\n",
      "norm: 2.23606797749979\n",
      "vec / norm: [0.         0.89442719 0.4472136  0.        ]\n",
      "vec: [0. 1. 0. 2.]\n",
      "norm: 2.23606797749979\n",
      "vec / norm: [0.         0.4472136  0.         0.89442719]\n",
      "vec: [1. 0. 1. 1.]\n",
      "norm: 1.7320508075688772\n",
      "vec / norm: [0.57735027 0.         0.57735027 0.57735027]\n",
      "Vocabulary: ['China', 'Japan', 'Tokyo', 'Beijing']\n",
      "\n",
      "=== Class 1 docs (raw tf) ===\n",
      "China China Tokyo         -> [2. 0. 1. 0.]\n",
      "China Beijing             -> [1. 0. 0. 1.]\n",
      "\n",
      "=== Class 1 docs (L2-normalized) ===\n",
      "China China Tokyo         -> [0.894, 0.000, 0.447, 0.000]\n",
      "China Beijing             -> [0.707, 0.000, 0.000, 0.707]\n",
      "\n",
      "mu1 (Class1 centroid): [0.801, 0.000, 0.224, 0.354]\n",
      "\n",
      "=== Class 2 docs (raw tf) ===\n",
      "Japan Japan Tokyo         -> [0. 2. 1. 0.]\n",
      "Japan Beijing Beijing     -> [0. 1. 0. 2.]\n",
      "\n",
      "=== Class 2 docs (L2-normalized) ===\n",
      "Japan Japan Tokyo         -> [0.000, 0.894, 0.447, 0.000]\n",
      "Japan Beijing Beijing     -> [0.000, 0.447, 0.000, 0.894]\n",
      "\n",
      "mu2 (Class2 centroid): [0.000, 0.671, 0.224, 0.447]\n",
      "\n",
      "=== Test doc ===\n",
      "text: China Tokyo Beijing\n",
      "x_raw: [1. 0. 1. 1.]\n",
      "x_hat: [0.577, 0.000, 0.577, 0.577]\n",
      "\n",
      "=== Squared distances to centroids ===\n",
      "Class1_ChinaSide: 0.225135\n",
      "Class2_JapanSide: 0.925403\n",
      "\n",
      "PREDICT = Class1_ChinaSide\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5) 실행\n",
    "# ------------------------------------------------------------\n",
    "V = len(vocab)\n",
    "\n",
    "raw1, hat1, mu1 = centroid_from_texts(class1_texts, term2idx, V)\n",
    "raw2, hat2, mu2 = centroid_from_texts(class2_texts, term2idx, V)\n",
    "\n",
    "x_raw = text_to_tf(test_text, term2idx, V)\n",
    "x_hat = l2_normalize(x_raw)\n",
    "\n",
    "centroids = {\"Class1_ChinaSide\": mu1, \"Class2_JapanSide\": mu2}\n",
    "pred, dists = rocchio_predict(x_hat, centroids)\n",
    "\n",
    "# 보기 좋게 출력\n",
    "def fmt(vec):\n",
    "    return \"[\" + \", \".join(f\"{v:.3f}\" for v in vec) + \"]\"\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"\\n=== Class 1 docs (raw tf) ===\")\n",
    "for t, v in zip(class1_texts, raw1):\n",
    "    print(f\"{t:25s} -> {v}\")\n",
    "\n",
    "print(\"\\n=== Class 1 docs (L2-normalized) ===\")\n",
    "for t, v in zip(class1_texts, hat1):\n",
    "    print(f\"{t:25s} -> {fmt(v)}\")\n",
    "\n",
    "print(\"\\nmu1 (Class1 centroid):\", fmt(mu1))\n",
    "\n",
    "print(\"\\n=== Class 2 docs (raw tf) ===\")\n",
    "for t, v in zip(class2_texts, raw2):\n",
    "    print(f\"{t:25s} -> {v}\")\n",
    "\n",
    "print(\"\\n=== Class 2 docs (L2-normalized) ===\")\n",
    "for t, v in zip(class2_texts, hat2):\n",
    "    print(f\"{t:25s} -> {fmt(v)}\")\n",
    "\n",
    "print(\"\\nmu2 (Class2 centroid):\", fmt(mu2))\n",
    "\n",
    "print(\"\\n=== Test doc ===\")\n",
    "print(\"text:\", test_text)\n",
    "print(\"x_raw:\", x_raw)\n",
    "print(\"x_hat:\", fmt(x_hat))\n",
    "\n",
    "print(\"\\n=== Squared distances to centroids ===\")\n",
    "for k, v in dists.items():\n",
    "    print(f\"{k}: {v:.6f}\")\n",
    "\n",
    "print(\"\\nPREDICT =\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5aca9-07f3-486d-95f0-d49965972c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175faabe-bd33-4661-82a8-fa051b03bfed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
