{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1caf6d46-3fef-4908-bf8f-797470c07b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Set, Iterable\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7101d59e-238e-4f0e-b921-29af51ef4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 0) Sample data (label, text)\n",
    "# ==================================================\n",
    "docs = [\n",
    "    (0, \"The government announced a new economic policy today.\"),\n",
    "    (0, \"The election results sparked debates across the country.\"),\n",
    "    (0, \"The president met with foreign leaders to discuss trade.\"),\n",
    "    (0, \"New laws were introduced to reform the healthcare system.\"),\n",
    "    (0, \"The parliament voted on the proposed budget plan.\"),\n",
    "\n",
    "    (1, \"The football team won the championship after a tough season.\"),\n",
    "    (1, \"Fans celebrated the victory late into the night.\"),\n",
    "    (1, \"The coach emphasized teamwork and discipline.\"),\n",
    "    (1, \"A star player scored the winning goal in the final match.\"),\n",
    "    (1, \"The team trained hard to prepare for the tournament.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84e84275-8ce7-4132-8f57-da61407cbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Build the skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c0f1088-1d02-4e81-a99f-97ab3eb6bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================================================\n",
    "# 1) make_subset\n",
    "# ==================================================\n",
    "def make_subset(docs: List[Tuple[int, str]]) -> Tuple[\n",
    "    List[Tuple[int, str]],\n",
    "    List[Tuple[int, str]]\n",
    "]:\n",
    "    c0_docs, c1_docs = [], []\n",
    "    for label, text in docs:\n",
    "        if label == 0:\n",
    "            c0_docs.append((label, text))\n",
    "        elif label == 1:\n",
    "            c1_docs.append((label, text))\n",
    "    return c0_docs, c1_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba1d7d60-7797-44a7-9f9c-a07c634c7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2) Preprocessing config\n",
    "# ==================================================\n",
    "@dataclass(frozen=True)\n",
    "class PrepConfig:\n",
    "    pattern: re.Pattern\n",
    "    stopwords: Set[str]\n",
    "    min_len: int = 2\n",
    "    drop_pure_digits: bool = True\n",
    "    lowercase: bool = True\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"for\",\"with\",\"was\",\"were\",\n",
    "    \"is\",\"are\",\"in\",\"on\",\"after\",\"new\",\"we\",\"i\",\"them\",\"they\",\n",
    "    \"this\",\"that\",\"it\",\"as\",\"helps\"\n",
    "}\n",
    "\n",
    "CFG = PrepConfig(\n",
    "    pattern=re.compile(r\"[a-z0-9%]+\"),\n",
    "    stopwords=STOPWORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3f56a42-4ab8-47f3-8ec7-bc7ad0438edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function tokenizes and normalizes text by applying case normalization, regex-based token extraction, and multiple noise-filtering steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59856337-eec6-44b6-b525-f31582444df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 3) tokenize / text_prep\n",
    "# ==================================================\n",
    "def tokenize(text: str, cfg: PrepConfig = CFG) -> List[str]:\n",
    "    # 1) Convert text to lowercase (case normalization)\n",
    "    if cfg.lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    # 2) Extract tokens using the regex pattern\n",
    "    #    → keeps only allowed character sequences (e.g., a-z, 0-9, %)\n",
    "    tokens = cfg.pattern.findall(text)\n",
    "\n",
    "    # 3) Filter out tokens shorter than the minimum length\n",
    "    #    → removes short, low-information noise tokens\n",
    "    if cfg.min_len > 1:\n",
    "        tokens = [t for t in tokens if len(t) >= cfg.min_len]\n",
    "\n",
    "    # 4) Remove stopwords\n",
    "    #    → removes common function words with little semantic value\n",
    "    if cfg.stopwords:\n",
    "        tokens = [t for t in tokens if t not in cfg.stopwords]\n",
    "\n",
    "    # 5) Remove tokens that consist of digits only\n",
    "    #    → prevents numeric-only tokens from inflating the vocabulary\n",
    "    if cfg.drop_pure_digits:\n",
    "        tokens = [t for t in tokens if not t.isdigit()]\n",
    "\n",
    "    # Return the final list of normalized tokens\n",
    "    return tokens\n",
    "\n",
    "def text_prep(\n",
    "    docs: Iterable[Tuple[int, str]],\n",
    "    cfg: PrepConfig = CFG\n",
    ") -> List[Tuple[int, List[str]]]:\n",
    "    return [(label, tokenize(text, cfg)) for label, text in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "328c1215-5609-4799-b1ec-0c596a892c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 4) tokens → sparse vectors (TF)\n",
    "# ==================================================\n",
    "def to_sparse_vectors(\n",
    "    prepped_docs: Iterable[Tuple[int, List[str]]]\n",
    ") -> List[Tuple[int, Counter]]:\n",
    "    return [(label, Counter(tokens)) for label, tokens in prepped_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3340c3d0-f48b-4030-a84d-54f11b4e33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rocchio training computes a centroid for each class by averaging the document vectors belonging to that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03c12a26-2de1-4ab7-afea-53cb277c0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 5) Rocchio: centroid (class prototype)\n",
    "# ==================================================\n",
    "def make_centroid(vectors: List[Counter]) -> Counter:\n",
    "    # Create the centroid (average vector) for a class\n",
    "    centroid = Counter()\n",
    "\n",
    "    # Safety check: return empty vector if there are no documents\n",
    "    if not vectors:\n",
    "        return centroid\n",
    "\n",
    "    # 1) Sum all document vectors belonging to the same class\n",
    "    #    → accumulate term frequencies\n",
    "    for vec in vectors:\n",
    "        centroid.update(vec)\n",
    "\n",
    "    # 2) Divide by the number of documents to compute the average\n",
    "    #    → results in the class centroid\n",
    "    n = len(vectors)\n",
    "    for term in centroid:\n",
    "        centroid[term] /= n\n",
    "\n",
    "    return centroid\n",
    "\n",
    "\n",
    "def train_rocchio(\n",
    "    sparse_docs: List[Tuple[int, Counter]]\n",
    ") -> dict:\n",
    "    # Group document vectors by class label\n",
    "    by_label = defaultdict(list)\n",
    "\n",
    "    # Convert [(label, vector), ...] into\n",
    "    # {label: [vector1, vector2, ...]}\n",
    "    for label, vec in sparse_docs:\n",
    "        by_label[label].append(vec)\n",
    "\n",
    "    # Compute a centroid for each class\n",
    "    # Result: {label: centroid_vector}\n",
    "    return {\n",
    "        label: make_centroid(vecs)\n",
    "        for label, vecs in by_label.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44e6abce-b5c5-4615-90da-c92af4704cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 6) cosine similarity\n",
    "# ==================================================\n",
    "def cosine_sim(a: Counter, b: Counter) -> float:\n",
    "    dot = sum(v * b.get(t, 0.0) for t, v in a.items())\n",
    "    norm_a = math.sqrt(sum(v*v for v in a.values()))\n",
    "    norm_b = math.sqrt(sum(v*v for v in b.values()))\n",
    "\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    return dot / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "601e1c3c-f586-4bdd-b4ed-c526bb61dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 7) predict\n",
    "# ==================================================\n",
    "def predict_rocchio(vec: Counter, centroids: dict) -> int:\n",
    "    scores = {\n",
    "        label: cosine_sim(vec, centroid)\n",
    "        for label, centroid in centroids.items()\n",
    "    }\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c66f0e22-a82f-4841-856a-abd128f38aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Centroids ===\n",
      "label=0, unique_terms=27\n",
      "label=1, unique_terms=27\n",
      "\n",
      "=== Predictions (training data) ===\n",
      "doc 0: true=0, pred=0\n",
      "doc 1: true=0, pred=0\n",
      "doc 2: true=0, pred=0\n",
      "doc 3: true=0, pred=0\n",
      "doc 4: true=0, pred=0\n",
      "doc 5: true=1, pred=1\n",
      "doc 6: true=1, pred=1\n",
      "doc 7: true=1, pred=1\n",
      "doc 8: true=1, pred=1\n",
      "doc 9: true=1, pred=1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==================================================\n",
    "# 8) RUN\n",
    "# ==================================================\n",
    "# subset (optional, test)\n",
    "c0_docs, c1_docs = make_subset(docs)\n",
    "\n",
    "# preprocessing\n",
    "prepped_docs = text_prep(docs)\n",
    "\n",
    "# vectorize\n",
    "sparse_docs = to_sparse_vectors(prepped_docs)\n",
    "\n",
    "# train\n",
    "centroids = train_rocchio(sparse_docs)\n",
    "\n",
    "print(\"=== Centroids ===\")\n",
    "for label, vec in centroids.items():\n",
    "    print(f\"label={label}, unique_terms={len(vec)}\")\n",
    "\n",
    "print(\"\\n=== Predictions (training data) ===\")\n",
    "for i, (label, vec) in enumerate(sparse_docs):\n",
    "    pred = predict_rocchio(vec, centroids)\n",
    "    print(f\"doc {i}: true={label}, pred={pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1388c9a-d0a7-43f7-b887-7cb1295f9ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
