{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afaece-f542-4013-aeff-513e7ccf1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ㄹ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9bf2b5d-88f9-4d20-8918-3de6da0b1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Set, Iterable\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2a76fa-c536-4b47-96f1-c8e38a1a240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 0) Sample data (label, text)\n",
    "# ==================================================\n",
    "docs = [\n",
    "    (0, \"The government announced a new economic policy today.\"),\n",
    "    (0, \"The election results sparked debates across the country.\"),\n",
    "    (0, \"The president met with foreign leaders to discuss trade.\"),\n",
    "    (0, \"New laws were introduced to reform the healthcare system.\"),\n",
    "    (0, \"The parliament voted on the proposed budget plan.\"),\n",
    "\n",
    "    (1, \"The football team won the championship after a tough season.\"),\n",
    "    (1, \"Fans celebrated the victory late into the night.\"),\n",
    "    (1, \"The coach emphasized teamwork and discipline.\"),\n",
    "    (1, \"A star player scored the winning goal in the final match.\"),\n",
    "    (1, \"The team trained hard to prepare for the tournament.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81c5ce8-5de4-4213-a9e8-ef34aabae36d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Build the skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb911a42-7350-43dc-b0fb-284425ec2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 1) make_subset\n",
    "# ==================================================\n",
    "def make_subset(docs: List[Tuple[int, str]]) -> Tuple[\n",
    "    List[Tuple[int, str]],\n",
    "    List[Tuple[int, str]]\n",
    "]:\n",
    "    # Initialize two empty lists to store documents\n",
    "    # belonging to class 0 and class 1 respectively\n",
    "    c0_docs, c1_docs = [], []\n",
    "\n",
    "    # Iterate over all documents\n",
    "    # Each document is a (label, text) tuple\n",
    "    for label, text in docs:\n",
    "\n",
    "        # If the document belongs to class 0,\n",
    "        # append it to the class-0 list\n",
    "        if label == 0:\n",
    "            c0_docs.append((label, text))\n",
    "\n",
    "        # If the document belongs to class 1,\n",
    "        # append it to the class-1 list\n",
    "        elif label == 1:\n",
    "            c1_docs.append((label, text))\n",
    "\n",
    "        # (Optional) other labels are ignored\n",
    "        # This assumes binary classification (0 vs 1)\n",
    "\n",
    "    # Return the two subsets:\n",
    "    # - documents with label 0\n",
    "    # - documents with label 1\n",
    "    return c0_docs, c1_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb611870-6e31-4ac9-8095-11f29a46ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2) Preprocessing config\n",
    "# ==================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Set\n",
    "import re\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepConfig:\n",
    "    # Regular expression pattern used for token extraction\n",
    "    # e.g., r\"[a-z0-9%]+\" will match lowercase words, digits, percentages\n",
    "    pattern: re.Pattern\n",
    "\n",
    "    # Set of stopwords to remove during preprocessing\n",
    "    # These are common words that usually do not carry semantic meaning\n",
    "    stopwords: Set[str]\n",
    "\n",
    "    # Minimum token length to keep (e.g., drop tokens of length < 2)\n",
    "    min_len: int = 2\n",
    "\n",
    "    # Whether to drop tokens that are purely digits (e.g., \"123\")\n",
    "    drop_pure_digits: bool = True\n",
    "\n",
    "    # Whether to lowercase text before tokenization\n",
    "    lowercase: bool = True\n",
    "\n",
    "\n",
    "# A predefined stopword list for this project\n",
    "# (kept simple and task-specific, not a huge generic list)\n",
    "STOPWORDS = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"for\",\"with\",\"was\",\"were\",\n",
    "    \"is\",\"are\",\"in\",\"on\",\"after\",\"new\",\"we\",\"i\",\"them\",\"they\",\n",
    "    \"this\",\"that\",\"it\",\"as\",\"helps\"\n",
    "}\n",
    "\n",
    "# Concrete preprocessing configuration used throughout the pipeline\n",
    "# This object will be passed to tokenization / preprocessing functions\n",
    "CFG = PrepConfig(\n",
    "    pattern=re.compile(r\"[a-z0-9%]+\"),\n",
    "    stopwords=STOPWORDS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a58f75-3acb-4d52-b64a-a47116e9ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 3) tokenize / text_prep\n",
    "# ==================================================\n",
    "def tokenize(text: str, cfg: PrepConfig = CFG) -> List[str]:\n",
    "    # 1) Convert text to lowercase (case normalization)\n",
    "    if cfg.lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    # 2) Extract tokens using the regex pattern\n",
    "    #    → keeps only allowed character sequences (e.g., a-z, 0-9, %)\n",
    "    tokens = cfg.pattern.findall(text)\n",
    "\n",
    "    # 3) Filter out tokens shorter than the minimum length\n",
    "    #    → removes short, low-information noise tokens\n",
    "    if cfg.min_len > 1:\n",
    "        tokens = [t for t in tokens if len(t) >= cfg.min_len]\n",
    "\n",
    "    # 4) Remove stopwords\n",
    "    #    → removes common function words with little semantic value\n",
    "    if cfg.stopwords:\n",
    "        tokens = [t for t in tokens if t not in cfg.stopwords]\n",
    "\n",
    "    # 5) Remove tokens that consist of digits only\n",
    "    #    → prevents numeric-only tokens from inflating the vocabulary\n",
    "    if cfg.drop_pure_digits:\n",
    "        tokens = [t for t in tokens if not t.isdigit()]\n",
    "\n",
    "    # Return the final list of normalized tokens\n",
    "    return tokens\n",
    "\n",
    "def text_prep(\n",
    "    docs: Iterable[Tuple[int, str]],\n",
    "    cfg: PrepConfig = CFG\n",
    ") -> List[Tuple[int, List[str]]]:\n",
    "    return [(label, tokenize(text, cfg)) for label, text in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0819363-2b47-47c7-a99e-041dbafdd9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 4) tokens → sparse vectors (TF)\n",
    "# ==================================================\n",
    "def to_sparse_vectors(\n",
    "    prepped_docs: Iterable[Tuple[int, List[str]]]\n",
    ") -> List[Tuple[int, Counter]]:\n",
    "    return [(label, Counter(tokens)) for label, tokens in prepped_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b60eec8-b172-4f51-8935-f2b9c1d10b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_before_after(prepped_docs):\n",
    "    print(\"=== BEFORE (tokens) ===\")\n",
    "    for i, (label, tokens) in enumerate(prepped_docs):\n",
    "        print(f\"[doc {i}] label={label}, tokens={tokens}\")\n",
    "\n",
    "    sparse = to_sparse_vectors(prepped_docs)\n",
    "\n",
    "    print(\"\\n=== AFTER (Counter) ===\")\n",
    "    for i, (label, vec) in enumerate(sparse):\n",
    "        print(f\"[doc {i}] label={label}, vec={dict(vec)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9867be60-fc0d-493b-a9e9-20e678830095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE (tokens) ===\n",
      "[doc 0] label=1, tokens=['nlp', 'search', 'nlp', 'model']\n",
      "[doc 1] label=0, tokens=['cat', 'dog', 'dog']\n",
      "\n",
      "=== AFTER (Counter) ===\n",
      "[doc 0] label=1, vec={'nlp': 2, 'search': 1, 'model': 1}\n",
      "[doc 1] label=0, vec={'cat': 1, 'dog': 2}\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\n",
    "    (1, \"NLP search NLP model\"),\n",
    "    (0, \"cat dog dog\")\n",
    "]\n",
    "\n",
    "prepped = text_prep(test_docs)\n",
    "debug_before_after(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f24d73b4-a1f1-4d26-84a4-dd049aa73eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 5) Rocchio: centroid (class prototype)\n",
    "# ==================================================\n",
    "def make_centroid(vectors: List[Counter]) -> Counter:\n",
    "    # Create the centroid (average vector) for a class\n",
    "    centroid = Counter()\n",
    "\n",
    "    # Safety check: return empty vector if there are no documents\n",
    "    if not vectors:\n",
    "        return centroid\n",
    "\n",
    "    # 1) Sum all document vectors belonging to the same class\n",
    "    #    → accumulate term frequencies\n",
    "    for vec in vectors:\n",
    "        centroid.update(vec)\n",
    "\n",
    "    # 2) Divide by the number of documents to compute the average\n",
    "    #    → results in the class centroid\n",
    "    n = len(vectors)\n",
    "    for term in centroid:\n",
    "        centroid[term] /= n\n",
    "\n",
    "    return centroid\n",
    "\n",
    "\n",
    "def train_rocchio(\n",
    "    sparse_docs: List[Tuple[int, Counter]]\n",
    ") -> dict:\n",
    "    # Group document vectors by class label\n",
    "    by_label = defaultdict(list)\n",
    "\n",
    "    # Convert [(label, vector), ...] into\n",
    "    # {label: [vector1, vector2, ...]}\n",
    "    for label, vec in sparse_docs:\n",
    "        by_label[label].append(vec)\n",
    "\n",
    "    # Compute a centroid for each class\n",
    "    # Result: {label: centroid_vector}\n",
    "    return {\n",
    "        label: make_centroid(vecs)\n",
    "        for label, vecs in by_label.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349fa082-74b3-4f60-8090-dab2a105a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a: Counter, b: Counter, verbose: bool = False) -> float:\n",
    "    # 1) Dot product: sum of products over shared tokens\n",
    "    dot = sum(v * b.get(t, 0.0) for t, v in a.items())\n",
    "\n",
    "    # 2) L2 norm (vector length)\n",
    "    norm_a = math.sqrt(sum(v * v for v in a.values()))\n",
    "    norm_b = math.sqrt(sum(v * v for v in b.values()))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== cosine_sim DEBUG ===\")\n",
    "        print(f\"dot product   : {dot}\")\n",
    "        print(f\"norm_a (||a||): {norm_a}\")\n",
    "        print(f\"norm_b (||b||): {norm_b}\")\n",
    "        print(f\"a size        : {len(a)}\")\n",
    "        print(f\"b size        : {len(b)}\")\n",
    "\n",
    "    # 3) Guard for zero vectors\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        if verbose:\n",
    "            print(\"→ Zero vector detected, returning 0.0\\n\")\n",
    "        return 0.0\n",
    "\n",
    "    sim = dot / (norm_a * norm_b)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"cosine sim    : {sim}\\n\")\n",
    "\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e9b444d-e68a-40d5-88df-9080d40cd8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== cosine_sim DEBUG ===\n",
      "dot product   : 3.5\n",
      "norm_a (||a||): 2.449489742783178\n",
      "norm_b (||b||): 2.5495097567963922\n",
      "a size        : 3\n",
      "b size        : 3\n",
      "cosine sim    : 0.5604485383178051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Debudding \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "vec = Counter({\"nlp\": 2, \"ir\": 1, \"model\": 1})\n",
    "centroid = Counter({\"nlp\": 1.5, \"ir\": 0.5, \"data\": 2})\n",
    "\n",
    "score = cosine_sim(vec, centroid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7266bee-9c22-40d4-bf71-634903c57193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 7) predict\n",
    "# ==================================================\n",
    "def predict_rocchio(vec: Counter, centroids: dict) -> int:\n",
    "    # 1) Compute similarity scores for each class\n",
    "    #    scores[label] = cosine similarity between\n",
    "    #    the input vector and that class's centroid\n",
    "    scores = {lbl: cosine_sim(vec, c) for lbl, c in centroids.items()}\n",
    "\n",
    "    # 2) Sanity check: no centroids means no classification is possible\n",
    "    #    (fail fast to avoid obscure errors later)\n",
    "    if not scores:\n",
    "        raise ValueError(\"empty centroids\")\n",
    "\n",
    "    # 3) Find the highest similarity score\n",
    "    #    This represents the most similar centroid\n",
    "    best = max(scores.values())\n",
    "\n",
    "    # 4) Collect all labels that achieve this maximum score\n",
    "    #    (ties are possible if two centroids are equally similar)\n",
    "    winners = [lbl for lbl, s in scores.items() if s == best]\n",
    "\n",
    "    # 5) Deterministic tie-break:\n",
    "    #    Always return the same label order for reproducibility\n",
    "    return sorted(winners)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "282b8d6a-cae5-4817-a36e-4e4de47bc05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Centroids ===\n",
      "label=0, unique_terms=27\n",
      "label=1, unique_terms=27\n",
      "\n",
      "=== Predictions (training data) ===\n",
      "doc 0: true=0, pred=0\n",
      "doc 1: true=0, pred=0\n",
      "doc 2: true=0, pred=0\n",
      "doc 3: true=0, pred=0\n",
      "doc 4: true=0, pred=0\n",
      "doc 5: true=1, pred=1\n",
      "doc 6: true=1, pred=1\n",
      "doc 7: true=1, pred=1\n",
      "doc 8: true=1, pred=1\n",
      "doc 9: true=1, pred=1\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 8) RUN\n",
    "# ==================================================\n",
    "# subset (optional, test)\n",
    "c0_docs, c1_docs = make_subset(docs)\n",
    "\n",
    "# preprocessing\n",
    "prepped_docs = text_prep(docs)\n",
    "\n",
    "# vectorize\n",
    "sparse_docs = to_sparse_vectors(prepped_docs)\n",
    "\n",
    "# train\n",
    "centroids = train_rocchio(sparse_docs)\n",
    "\n",
    "print(\"=== Centroids ===\")\n",
    "for label, vec in centroids.items():\n",
    "    print(f\"label={label}, unique_terms={len(vec)}\")\n",
    "\n",
    "print(\"\\n=== Predictions (training data) ===\")\n",
    "for i, (label, vec) in enumerate(sparse_docs):\n",
    "    pred = predict_rocchio(vec, centroids)\n",
    "    print(f\"doc {i}: true={label}, pred={pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e3328-4cdb-4874-bdcb-33822f4ffeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
