{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d461685d-ed3c-4294-91ca-002fce03b097",
   "metadata": {},
   "source": [
    "# KNN Text Classification with TF-IDF (Dense Representation)\n",
    "\n",
    "## 1. Dataset\n",
    "#- Toy dataset with positive / negative labels\n",
    "\n",
    "## 2. Text Preprocessing\n",
    "#- Tokenization\n",
    "\n",
    "## 3. Vocabulary Construction\n",
    "#- Build vocab and term2idx\n",
    "\n",
    "## 4. Posting List & TF\n",
    "#- Term-centric posting list\n",
    "#- Term frequency (TF)\n",
    "\n",
    "## 5. DF & IDF\n",
    "#- Document frequency\n",
    "#- Inverse document frequency\n",
    "\n",
    "## 6. TF-IDF Construction\n",
    "#- Sparse TF-IDF (doc_id → {term: value})\n",
    "#- Dense TF-IDF matrix\n",
    "\n",
    "## 7. Cosine Similarity\n",
    "#- Vector-based similarity measure\n",
    "\n",
    "## 8. KNN Classification\n",
    "#- Majority vote based on top-k neighbors\n",
    "\n",
    "## 9. Leave-One-Out (LOO) Evaluation\n",
    "#- Evaluation on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "74b83e96-2ba1-4e36-b54e-b0d30a7b4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "TOKEN_DEBUG = False\n",
    "VEC_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4066e03b-ecb9-47be-a7a4-a8add4579a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    (\"pos\", \"good service\"),\n",
    "    (\"pos\", \"great product\"),\n",
    "    (\"pos\", \"excellent quality\"),\n",
    "    (\"neg\", \"bad service\"),\n",
    "    (\"neg\", \"poor product\"),\n",
    "    (\"neg\", \"terrible quality\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "575327c8-a0db-40ad-933c-284d6942d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Step 1. Prepare Documents (Class-wise separation)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "pos_docs = []\n",
    "neg_docs = []\n",
    "\n",
    "for label, sentence in docs:\n",
    "    if label == \"pos\":\n",
    "        pos_docs.append(sentence)\n",
    "    elif label == \"neg\":\n",
    "        neg_docs.append(sentence)\n",
    "\n",
    "docs_all = pos_docs + neg_docs\n",
    "doc_labels = np.array([1]*len(pos_docs) + [0]*len(neg_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ef71dca-e0cb-4726-aec2-9037ffa44bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e6c07b51-610b-4023-8355-6c122bed4a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 6\n",
      "['good service', 'great product', 'excellent quality', 'bad service', 'poor product', 'terrible quality']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(docs_all))\n",
    "print(docs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9fe607bc-c34e-4d79-808d-213701bd5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Tokenization (Doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3bb8825d-3fa2-4dd7-b179-2a40aadc6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenization(s):\n",
    "    normalized = re.sub(r\"[^A-Za-z0-9]\", \" \", s).strip()\n",
    "    return normalized.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5b3e7fd2-58f5-4cd2-a713-2ffcd048136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Vocabulary construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eb052816-0f78-438a-aa1d-f4d68ebfc495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(docs_all):\n",
    "    # Collect all unique tokens from the entire corpus\n",
    "    vocab_set = set()\n",
    "    for sentence in docs_all:\n",
    "        # Tokenize each sentence and add tokens to the vocabulary set\n",
    "        vocab_set.update(text_tokenization(sentence))\n",
    "\n",
    "    # Sort the vocabulary to ensure a deterministic order\n",
    "    vocab = sorted(vocab_set)\n",
    "\n",
    "    # Create a mapping from each term to a unique index\n",
    "    # This index defines the feature dimension in vector representations\n",
    "    term2idx = {t: i for i, t in enumerate(vocab)}\n",
    "\n",
    "    return vocab, term2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0e135f4a-a6f0-4f49-8160-7601964dcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, term2idx = build_vocab(docs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7965da1b-4d8d-42c8-a1af-56fb471363aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab order: ['bad', 'excellent', 'good', 'great', 'poor', 'product', 'quality', 'service', 'terrible']\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab order:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784aed0-27fd-422c-8a11-ccd96cf55d04",
   "metadata": {},
   "source": [
    " #4. Create a posting list :We iterate over term–frequency pairs in each document to populate the posting list, which reorganizes the corpus from a document-centric view to a term-centric view.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "578e99a4-3093-480b-a891-ef3a043013a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_posting_list_tf_debug(docs_all, debug=True):\n",
    "    posting = defaultdict(dict)  # term -> {doc_id: tf}\n",
    "\n",
    "    #The same tokenization function is reused,but at a different stage and for a different purpose.\n",
    "    for doc_id, sentence in enumerate(docs_all):\n",
    "        # Tokenize the sentence again, this time for per-document analysis.\n",
    "        # This tokenization is used to compute term frequencies (TF) within a single document,\n",
    "        # not for vocabulary construction.\n",
    "        tokens = text_tokenization(sentence)\n",
    "    \n",
    "        # Count how many times each token appears in this document.\n",
    "        # The result summarizes the sentence as term -> frequency (TF).\n",
    "        tf_counter = Counter(tokens)\n",
    "\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\n[DOC {doc_id}] {sentence}\")\n",
    "            print(\"  Tokens:\", tokens)\n",
    "            print(\"  TF Counter:\", tf_counter)\n",
    "\n",
    "        for term, tf in tf_counter.items():\n",
    "            posting[term][doc_id] = tf\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n=== POSTING LIST SUMMARY ===\")\n",
    "        for term in sorted(posting.keys()):\n",
    "            print(f\"{term} -> {posting[term]}\")\n",
    "\n",
    "    return dict(posting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "44e90e19-b3ee-4bad-bbb0-888af1f1880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DOC 0] good service\n",
      "  Tokens: ['good', 'service']\n",
      "  TF Counter: Counter({'good': 1, 'service': 1})\n",
      "\n",
      "[DOC 1] great product\n",
      "  Tokens: ['great', 'product']\n",
      "  TF Counter: Counter({'great': 1, 'product': 1})\n",
      "\n",
      "[DOC 2] excellent quality\n",
      "  Tokens: ['excellent', 'quality']\n",
      "  TF Counter: Counter({'excellent': 1, 'quality': 1})\n",
      "\n",
      "[DOC 3] bad service\n",
      "  Tokens: ['bad', 'service']\n",
      "  TF Counter: Counter({'bad': 1, 'service': 1})\n",
      "\n",
      "[DOC 4] poor product\n",
      "  Tokens: ['poor', 'product']\n",
      "  TF Counter: Counter({'poor': 1, 'product': 1})\n",
      "\n",
      "[DOC 5] terrible quality\n",
      "  Tokens: ['terrible', 'quality']\n",
      "  TF Counter: Counter({'terrible': 1, 'quality': 1})\n",
      "\n",
      "=== POSTING LIST SUMMARY ===\n",
      "bad -> {3: 1}\n",
      "excellent -> {2: 1}\n",
      "good -> {0: 1}\n",
      "great -> {1: 1}\n",
      "poor -> {4: 1}\n",
      "product -> {1: 1, 4: 1}\n",
      "quality -> {2: 1, 5: 1}\n",
      "service -> {0: 1, 3: 1}\n",
      "terrible -> {5: 1}\n"
     ]
    }
   ],
   "source": [
    "posting = build_posting_list_tf_debug(docs_all, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "556da6e1-ce31-44f7-abce-5c0d14fb2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF(Document Frequency)\n",
    "#이 단어가 몇 개의 문서에 등장했나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9a9f10dc-01c9-4ddb-a2b9-090a545008b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df(posting):\n",
    "    # DF(term) = number of documents that contain the term\n",
    "    return {term: len(doc_dict) for term, doc_dict in posting.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7ad5cd14-1c18-4094-9e31-5b532b1450cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': 1, 'service': 2, 'great': 1, 'product': 2, 'excellent': 1, 'quality': 2, 'bad': 1, 'poor': 1, 'terrible': 1}\n"
     ]
    }
   ],
   "source": [
    "df = compute_df(posting)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "52d6d419-9737-497c-9454-5975f72bc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF(Inverse Document Frequency)\n",
    "#이 단어가 얼마나 흔한가 / 희귀한가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "13d0d818-4e8f-4c6d-9b6d-d1e2255937cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: {'good': 1.791759469228055, 'service': 1.0986122886681098, 'great': 1.791759469228055, 'product': 1.0986122886681098, 'excellent': 1.791759469228055, 'quality': 1.0986122886681098, 'bad': 1.791759469228055, 'poor': 1.791759469228055, 'terrible': 1.791759469228055}\n"
     ]
    }
   ],
   "source": [
    "def compute_idf(df, N):\n",
    "    idf = {}\n",
    "    for term, df_t in df.items():\n",
    "        idf[term] = math.log(N / df_t)\n",
    "    return idf\n",
    "\n",
    "N = len(docs_all)\n",
    "idf = compute_idf(df, N)\n",
    "print(\"IDF:\", idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a55be8ae-09c3-4bef-804e-c7adda5ab1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b09f9cd9-c2cf-474f-a7a6-27fbccbe505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def compute_tfidf(posting, idf):\n",
    "    # Create a sparse TF-IDF representation.\n",
    "    # The result is organized by document:\n",
    "    #   doc_id -> {term : tf-idf value}\n",
    "    tfidf = defaultdict(dict)\n",
    "\n",
    "    # Iterate over the posting list (term-centric structure)\n",
    "    # posting: term -> {doc_id : tf}\n",
    "    for term, doc_dict in posting.items():\n",
    "\n",
    "        # For the current term, iterate over all documents\n",
    "        # in which this term appears\n",
    "        for doc_id, tf in doc_dict.items():\n",
    "\n",
    "            # Compute TF-IDF for this (term, document) pair\n",
    "            # TF  : term frequency in the document\n",
    "            # IDF : inverse document frequency of the term\n",
    "            tfidf[doc_id][term] = tf * idf[term]\n",
    "\n",
    "    # Convert defaultdict to a regular dict for cleaner output\n",
    "    return dict(tfidf)\n",
    "\n",
    "# Build sparse TF-IDF representation\n",
    "tfidf = compute_tfidf(posting, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "201b531f-bae3-4b2b-9945-916ffd1bedcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TERM: good\n",
      "  doc_dict: {0: 1}\n",
      "    doc_id=0, tf=1\n",
      "\n",
      "TERM: service\n",
      "  doc_dict: {0: 1, 3: 1}\n",
      "    doc_id=0, tf=1\n",
      "    doc_id=3, tf=1\n",
      "\n",
      "TERM: great\n",
      "  doc_dict: {1: 1}\n",
      "    doc_id=1, tf=1\n",
      "\n",
      "TERM: product\n",
      "  doc_dict: {1: 1, 4: 1}\n",
      "    doc_id=1, tf=1\n",
      "    doc_id=4, tf=1\n",
      "\n",
      "TERM: excellent\n",
      "  doc_dict: {2: 1}\n",
      "    doc_id=2, tf=1\n",
      "\n",
      "TERM: quality\n",
      "  doc_dict: {2: 1, 5: 1}\n",
      "    doc_id=2, tf=1\n",
      "    doc_id=5, tf=1\n",
      "\n",
      "TERM: bad\n",
      "  doc_dict: {3: 1}\n",
      "    doc_id=3, tf=1\n",
      "\n",
      "TERM: poor\n",
      "  doc_dict: {4: 1}\n",
      "    doc_id=4, tf=1\n",
      "\n",
      "TERM: terrible\n",
      "  doc_dict: {5: 1}\n",
      "    doc_id=5, tf=1\n"
     ]
    }
   ],
   "source": [
    "#---Debugging code--\n",
    "for term, doc_dict in posting.items():\n",
    "    print(\"\\nTERM:\", term)\n",
    "    print(\"  doc_dict:\", doc_dict)\n",
    "    for doc_id, tf in doc_dict.items():\n",
    "        print(f\"    doc_id={doc_id}, tf={tf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f24a4-b075-4607-a308-cae7dda9041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문서별 TF-IDF(딕셔너리 형태)를모든 문서를 같은 길이의 벡터(행렬)로 바꾸는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f607b81c-9c27-45eb-a914-aafdf3ec8861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tfidf shape: (6, 9)\n",
      "[[0.         0.         1.79175947 0.         0.         0.\n",
      "  0.         1.09861229 0.        ]\n",
      " [0.         0.         0.         1.79175947 0.         1.09861229\n",
      "  0.         0.         0.        ]\n",
      " [0.         1.79175947 0.         0.         0.         0.\n",
      "  1.09861229 0.         0.        ]\n",
      " [1.79175947 0.         0.         0.         0.         0.\n",
      "  0.         1.09861229 0.        ]\n",
      " [0.         0.         0.         0.         1.79175947 1.09861229\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.09861229 0.         1.79175947]]\n"
     ]
    }
   ],
   "source": [
    "def tfidf_to_dense(tfidf, term2idx, N):\n",
    "    # Number of unique terms in the vocabulary\n",
    "    # This determines the dimensionality of each document vector\n",
    "    V = len(term2idx)\n",
    "\n",
    "    # Initialize a dense TF-IDF matrix with zeros\n",
    "    # Shape:\n",
    "    #   N rows    -> documents (doc_id)\n",
    "    #   V columns -> vocabulary terms\n",
    "    X = np.zeros((N, V))\n",
    "\n",
    "    # Iterate over the sparse TF-IDF representation\n",
    "    # tfidf structure:\n",
    "    #   doc_id -> {term : tf-idf value}\n",
    "    for doc_id, term_dict in tfidf.items():\n",
    "\n",
    "        # Iterate over terms that actually appear in this document\n",
    "        for term, val in term_dict.items():\n",
    "\n",
    "            # Map the term to its column index using term2idx\n",
    "            # and assign the TF-IDF value to the corresponding position\n",
    "            X[doc_id, term2idx[term]] = val\n",
    "\n",
    "    # Return the dense TF-IDF document-term matrix\n",
    "    return X\n",
    "\n",
    "\n",
    "# Convert sparse TF-IDF representation to dense matrix\n",
    "X_tfidf = tfidf_to_dense(tfidf, term2idx, N)\n",
    "\n",
    "# Inspect the shape and contents of the dense TF-IDF matrix\n",
    "print(\"X_tfidf shape:\", X_tfidf.shape)\n",
    "print(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7f6abb60-ee12-4cb1-b81d-34d168524181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity (dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e13aee17-6d14-4dc9-8756-a9544f42bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b, eps=1e-12):\n",
    "    return float(np.dot(a, b) / ((np.linalg.norm(a) * np.linalg.norm(b)) + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1d32d25a-0bce-4e56-bdd3-412ecf1d7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_debug(a, b, eps=1e-12):\n",
    "    dot = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    denom = (norm_a * norm_b) + eps\n",
    "    cos = dot / denom\n",
    "\n",
    "    print(\"Vector a:\", a)\n",
    "    print(\"Vector b:\", b)\n",
    "    print(\"dot(a, b):\", dot)\n",
    "    print(\"||a||:\", norm_a)\n",
    "    print(\"||b||:\", norm_b)\n",
    "    print(\"denominator:\", denom)\n",
    "    print(\"cosine similarity:\", cos)\n",
    "\n",
    "    return float(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "308d3648-c596-4b46-893e-4c59ccfce5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a: [1. 1. 0.]\n",
      "Vector b: [2. 2. 0.]\n",
      "dot(a, b): 4.0\n",
      "||a||: 1.4142135623730951\n",
      "||b||: 2.8284271247461903\n",
      "denominator: 4.000000000001001\n",
      "cosine similarity: 0.9999999999997498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999997498"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1.0, 1.0, 0.0])\n",
    "b = np.array([2.0, 2.0, 0.0])\n",
    "\n",
    "cosine_sim_debug(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b1336375-9835-47fd-9520-26ede33234de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a: [0.         0.         1.79175947 0.         0.         0.\n",
      " 0.         1.09861229 0.        ]\n",
      "Vector b: [1.79175947 0.         0.         0.         0.         0.\n",
      " 0.         1.09861229 0.        ]\n",
      "dot(a, b): 1.206948960812582\n",
      "||a||: 2.1017494989605643\n",
      "||b||: 2.1017494989605643\n",
      "denominator: 4.417350956381983\n",
      "cosine similarity: 0.27322913047441666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27322913047441666"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_debug(X_tfidf[0], X_tfidf[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db0d3e-d755-40ac-afe1-3d22bbdf8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN: predict one doc (dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c294e-7722-4437-8385-5c555e84f63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "da083bff-8675-4a1e-9e3b-ffe26572530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict_one(X_train, y_train, x_test, k=3):\n",
    "    # cosine similarity vs all train docs\n",
    "    dots = X_train @ x_test\n",
    "    denom = (np.linalg.norm(X_train, axis=1) * np.linalg.norm(x_test) + 1e-12)\n",
    "    sims = dots / denom\n",
    "\n",
    "    # top-k neighbors (highest similarity)\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    topk_labels = y_train[topk_idx]\n",
    "\n",
    "    # majority vote (tie -> 0)\n",
    "    counts = np.bincount(topk_labels.astype(int), minlength=2)\n",
    "    pred = int(np.argmax(counts))\n",
    "\n",
    "    return pred, topk_idx, sims[topk_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6ca5af66-985e-414d-a584-3212d6ef9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict_one_debug(X_train, y_train, x_test, k=3):\n",
    "    dots = X_train @ x_test\n",
    "    denom = (np.linalg.norm(X_train, axis=1) * np.linalg.norm(x_test) + 1e-12)\n",
    "    sims = dots / denom\n",
    "\n",
    "    print(\"Cosine similarities:\", sims)\n",
    "\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    print(\"Top-k indices:\", topk_idx)\n",
    "\n",
    "    topk_labels = y_train[topk_idx]\n",
    "    print(\"Top-k labels:\", topk_labels)\n",
    "\n",
    "    counts = np.bincount(topk_labels.astype(int), minlength=2)\n",
    "    print(\"Label counts [neg, pos]:\", counts)\n",
    "\n",
    "    pred = int(np.argmax(counts))\n",
    "    print(\"Prediction:\", pred)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "47d04ba9-248c-47c4-8dbf-c48137135b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities: [0.         0.         0.27322913 0.         0.        ]\n",
      "Top-k indices: [2 0 1]\n",
      "Top-k labels: [0 1 1]\n",
      "Label counts [neg, pos]: [1 2]\n",
      "Prediction: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_predict_one_debug(\n",
    "    X_train=X_tfidf[1:], \n",
    "    y_train=doc_labels[1:], \n",
    "    x_test=X_tfidf[0], \n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e5d26-ebb0-4435-b68a-3add099f80ad",
   "metadata": {},
   "source": [
    "As a result of selecting the three documents most similar to the test document, two of them were labeled as positive, so the final prediction is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a5265-5da7-4e17-ab88-595be24759fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-One-Out evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "258f287d-5108-46cf-a0da-5463b9d15eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loo_knn_eval(X, y, k=3):\n",
    "    # Number of documents\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # Array to store predictions for each document\n",
    "    y_pred = np.zeros(n, dtype=int)\n",
    "\n",
    "    # Leave-One-Out loop:\n",
    "    # Each document is used once as the test document\n",
    "    for i in range(n):\n",
    "        # Use all documents except i as training data\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        y_train = np.delete(y, i, axis=0)\n",
    "\n",
    "        # Use the i-th document as the test sample\n",
    "        x_test = X[i]\n",
    "\n",
    "        # Predict the label using KNN\n",
    "        # (returns prediction, neighbor indices, similarities)\n",
    "        pred, _, _ = knn_predict_one(X_train, y_train, x_test, k=k)\n",
    "\n",
    "        # Store the prediction\n",
    "        y_pred[i] = pred\n",
    "\n",
    "    # Compute accuracy by comparing predictions with true labels\n",
    "    acc = float(np.mean(y_pred == y))\n",
    "\n",
    "    return y_pred, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4d1c8-bb85-4a79-9622-d8f8c05cd1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c4a64e24-8575-4f39-b23b-f8f250ee1889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1 | y_true=[1, 1, 1, 0, 0, 0] | y_pred=[0, 0, 0, 1, 1, 1] | acc=0.000\n",
      "k=3 | y_true=[1, 1, 1, 0, 0, 0] | y_pred=[1, 1, 1, 1, 1, 1] | acc=0.500\n",
      "k=5 | y_true=[1, 1, 1, 0, 0, 0] | y_pred=[0, 0, 0, 1, 1, 1] | acc=0.000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Run\n",
    "# -----------------------------\n",
    "for k in [1, 3, 5]:\n",
    "    y_pred, acc = loo_knn_eval(X_tfidf, doc_labels, k=k)\n",
    "    print(f\"k={k} | y_true={doc_labels.tolist()} | y_pred={y_pred.tolist()} | acc={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6ebf1-30bf-4969-9346-bdd5e23763c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8fb49-7031-4718-83b7-6689592994e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
