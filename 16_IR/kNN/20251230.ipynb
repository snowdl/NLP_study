{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6f4fcad-6a72-487f-bdb2-1ed9f6a8e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "TOKEN_DEBUG = False\n",
    "VEC_DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6c148-819e-41d2-b5f4-880837db3457",
   "metadata": {},
   "source": [
    "TF-IDF + kNN (Cosine) Pipeline\n",
    "\n",
    "raw documents (label, text)\n",
    "   ↓\n",
    "tokenization\n",
    "   - lowercase\n",
    "   - regex cleanup\n",
    "   - split into tokens (optionally drop very short tokens)\n",
    "   ↓\n",
    "build_vocab\n",
    "   - collect unique terms from all tokenized documents\n",
    "   - create term2idx mapping (term → index)\n",
    "   ↓\n",
    "compute_idf\n",
    "   - compute document frequency (df) internally using unique terms per document\n",
    "   - compute smoothed IDF: idf = log((N+1)/(df+1)) + 1\n",
    "   ↓\n",
    "vectorize_dense_tfidf\n",
    "   - compute TF per document (raw / log / norm)\n",
    "   - multiply TF * IDF to form a dense TF-IDF vector\n",
    "   - optional L2 normalization\n",
    "   ↓\n",
    "make_Xy_dense_tfidf\n",
    "   - stack document vectors into X (num_docs × vocab_size)\n",
    "   - align labels into y\n",
    "   ↓\n",
    "(train/test split OR leave-one-out evaluation)\n",
    "   - X_train, y_train / X_test, y_test\n",
    "   ↓\n",
    "kNN (cosine similarity)\n",
    "   - compute cosine similarity between test vector and all train vectors\n",
    "   - select top-k neighbors\n",
    "   - majority vote to predict label\n",
    "   ↓\n",
    "evaluation\n",
    "   - accuracy (standard)\n",
    "   - top-k hit rate (optional, label appears among k neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d16bedc-ddec-493b-b896-c0c25b877235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf8c95e2-e44f-417c-9184-b9f2e21c66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    (\"positive\", \"The service received excellent reviews from users.\"),\n",
    "    (\"negative\", \"The product launch was delayed due to technical issues.\"),\n",
    "    (\"positive\", \"Employee satisfaction improved after the policy change.\"),\n",
    "    (\"negative\", \"Several employees resigned amid internal conflicts.\"),\n",
    "    (\"positive\", \"Customers praised the new update for its improved performance.\"),\n",
    "    (\"negative\", \"The system outage caused frustration among users.\"),\n",
    "    (\"positive\", \"The successful launch boosted investor confidence.\"),\n",
    "    (\"negative\", \"Customer complaints about the service have increased recently.\"),\n",
    "    (\"positive\", \"The company announced record-breaking profits this quarter.\"),\n",
    "    (\"negative\", \"The company reported a significant drop in quarterly revenue.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "510f714f-095d-4507-ac52-3c72005fa6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Documents\n",
    "# - Collect raw text documents and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3f0eac0-6b32-4294-8283-edd2413f80ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split + build docs_all and y\n",
    "pos_docs, neg_docs = [], []\n",
    "for label, sentence in docs:\n",
    "    if label == \"positive\":\n",
    "        pos_docs.append(sentence)\n",
    "    else:\n",
    "        neg_docs.append(sentence)\n",
    "\n",
    "docs_all = pos_docs + neg_docs\n",
    "\n",
    "# Create label array based on the number of documents:\n",
    "# assign 1 for each positive document and 0 for each negative document\n",
    "y = np.array([1]*len(pos_docs) + [0]*len(neg_docs))  # pos=1, neg=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec1a0740-932e-4469-8d50-c985716abbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Tokenization\n",
    "# - Convert each document into a list of tokens.\n",
    "# - Typical steps:\n",
    "#   - Lowercasing\n",
    "#   - Regex-based token extraction\n",
    "#   - Removing very short or noisy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7c22b6b-763d-430c-8a33-2ee57b839d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenization(s):\n",
    "    if TOKEN_DEBUG:\n",
    "        print(\"[TOKEN_DEBUG] raw:\", repr(s))\n",
    "\n",
    "    # 1. Lowercasing\n",
    "    s = s.lower()\n",
    "\n",
    "    # 2. Regex-based token extraction (keep alphanumeric characters)\n",
    "    normalized = re.sub(r\"[^a-z0-9]\", \" \", s).strip()\n",
    "    tokens = normalized.split()\n",
    "\n",
    "    # 3. Remove very short or noisy tokens (length < 2)\n",
    "    tokens = [t for t in tokens if len(t) > 1]\n",
    "\n",
    "    if TOKEN_DEBUG:\n",
    "        print(\"[TOKEN_DEBUG] tokens:\", tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "262a4249-830b-4a2c-bcb5-eac02359e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Vocabulary Construction\n",
    "# - Build a vocabulary from all tokenized documents (baseline version).\n",
    "# - Note: Document-frequency (df) filtering is NOT applied here.\n",
    "#   will compute df/idf next and can filter very rare terms later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53d84f91-5ece-4685-b21c-66adf43d442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(docs_all):\n",
    "    vocab_set = set()\n",
    "    for sentence in docs_all:\n",
    "        # Add tokens one by one into the vocabulary set.\n",
    "        # update() unpacks the token list and inserts each term separately,\n",
    "        # which is required for proper vocabulary construction.\n",
    "        vocab_set.update(text_tokenization(sentence))\n",
    "\n",
    "    vocab = sorted(vocab_set)\n",
    "    term2idx = {t: i for i, t in enumerate(vocab)}\n",
    "\n",
    "    return vocab, term2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "34aa4ce8-2cb9-4f1a-9bc3-357476e9dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, term2idx = build_vocab(docs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bbe02e3d-127e-4a3b-a471-50a1e6ead333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "[('about', 0), ('after', 1), ('amid', 2), ('among', 3), ('announced', 4), ('boosted', 5), ('breaking', 6), ('caused', 7), ('change', 8), ('company', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(len(term2idx))\n",
    "print(list(term2idx.items())[:10])  # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc9ebcdf-b2b1-4e59-b039-6f0e87d7ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DF/IDF/ TFIDF Computation\n",
    "# - Compute document frequency (df) and inverse document frequency (idf).\n",
    "# - Optionally apply min_df filtering at this stage (e.g., remove terms with df < 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9f4321be-abc4-432c-866c-f1f3a73a6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def compute_df(docs_all, term2idx):\n",
    "    \"\"\"\n",
    "    Compute document frequency (df) for each term.\n",
    "    df[i] = number of documents containing term i.\n",
    "    \"\"\"\n",
    "    # Initialize document frequency counts to zero for all terms in the vocabulary\n",
    "    df = np.zeros(len(term2idx), dtype=int)\n",
    "\n",
    "    # Iterate over each document \n",
    "    for sentence in docs_all:\n",
    "        # Tokenize the document and convert to a set so that\n",
    "        tokens = set(text_tokenization(sentence))  # one count per document\n",
    "        # Loop over unique terms appearing in the current document\n",
    "        for t in tokens:\n",
    "            # Map the term to its vocabulary index (returns None if not in vocab)\n",
    "            idx = term2idx.get(t)\n",
    "            if idx is not None:\n",
    "                df[idx] += 1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "abdf521b-0331-4def-b913-c060b13b0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_df(docs_all, term2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b98f7260-2bb9-4732-b7a1-35d9d6168918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(df, N):\n",
    "    \"\"\"\n",
    "    Compute smoothed inverse document frequency (idf).\n",
    "    \"\"\"\n",
    "    return np.log((N + 1) / (df + 1)) + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8376dfae-5e0b-4470-a987-f00dddabf49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = compute_idf(df, len(docs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4eff710-cf98-452a-a369-4c652c0f1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(tokens, tf_mode=\"raw\"):\n",
    "    \"\"\"\n",
    "    Compute term frequency values for a single document.\n",
    "    \"\"\"\n",
    "    tf_counts = Counter(tokens)\n",
    "    tf = {}\n",
    "\n",
    "    for term, freq in tf_counts.items():\n",
    "        if tf_mode == \"raw\":\n",
    "            tf[term] = float(freq)\n",
    "        elif tf_mode == \"log\":\n",
    "            tf[term] = 1.0 + np.log(float(freq))\n",
    "        elif tf_mode == \"norm\":\n",
    "            tf[term] = float(freq) / max(len(tokens), 1)\n",
    "        else:\n",
    "            raise ValueError(\"tf_mode must be one of: 'raw', 'log', 'norm'\")\n",
    "\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8eb9e555-ae44-4807-9011-b55f21c2a274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2.0, 'b': 1.0, 'c': 1.0}\n",
      "{'a': 1.6931471805599454, 'b': 1.0, 'c': 1.0}\n",
      "{'a': 0.5, 'b': 0.25, 'c': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(compute_tf([\"a\", \"b\", \"a\", \"c\"], tf_mode=\"raw\"))\n",
    "print(compute_tf([\"a\", \"b\", \"a\", \"c\"], tf_mode=\"log\"))\n",
    "print(compute_tf([\"a\", \"b\", \"a\", \"c\"], tf_mode=\"norm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "62bccc87-bb1b-4990-96cc-4609899d5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2-normalize the TF-IDF vector to remove document length effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67740dfa-a5bf-4831-a6fe-050f112b16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dense_tfidf(sentence, term2idx, idf, *, tf_mode=\"raw\", l2_normalize=True):\n",
    "    \"\"\"\n",
    "    Convert a document into a dense TF-IDF vector.\n",
    "    \"\"\"\n",
    "    assert len(idf) == len(term2idx), \"idf must align with vocab\"\n",
    "\n",
    "    tokens = text_tokenization(sentence)\n",
    "    tf = compute_tf(tokens, tf_mode=tf_mode)\n",
    "\n",
    "    vec = np.zeros(len(term2idx), dtype=float)\n",
    "\n",
    "    # Iterate over each term and its TF value in the document\n",
    "    for term, tf_value in tf.items():\n",
    "        # Look up the vocabulary index for the current term\n",
    "\n",
    "        # If the term exists in the vocabulary\n",
    "        idx = term2idx.get(term)\n",
    "        if idx is not None:\n",
    "             # Compute the TF-IDF weight for this term and store it\n",
    "            vec[idx] = tf_value * idf[idx]\n",
    "\n",
    "    # L2-normalize the vector to remove document length effects\n",
    "    if l2_normalize:\n",
    "        # Compute the L2 (Euclidean) norm of the TF-IDF vector\n",
    "        norm = np.linalg.norm(vec)\n",
    "        if norm > 0:\n",
    "            vec = vec / norm\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d4fc5b98-2c6b-48c3-ad5a-db7d41ea7e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Example: vectorize a single document\n",
    "vec = vectorize_dense_tfidf(\n",
    "    docs_all[0],      # one sentence\n",
    "    term2idx,\n",
    "    idf,\n",
    "    tf_mode=\"raw\",\n",
    "    l2_normalize=True\n",
    ")\n",
    "\n",
    "print(vec.shape)\n",
    "print(vec[:10])  # first 10 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "de2d969d-53e2-48c6-88b9-e2bc91ba0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xy_dense_tfidf(docs_all, y, term2idx, idf, *, tf_mode=\"raw\", l2_normalize=True):\n",
    "    \"\"\"\n",
    "    Build TF-IDF feature matrix X and label vector y.\n",
    "    \"\"\"\n",
    "    # Convert each document into a TF-IDF vector\n",
    "    X = [\n",
    "        vectorize_dense_tfidf(\n",
    "            s,\n",
    "            term2idx,\n",
    "            idf,\n",
    "            tf_mode=tf_mode,\n",
    "            l2_normalize=l2_normalize\n",
    "        )\n",
    "        for s in docs_all\n",
    "        # Iterate over all documents and vectorize them one by one\n",
    "    ]\n",
    "\n",
    "    # Stack all document vectors into a 2D feature matrix\n",
    "    # Shape: (num_documents, vocab_size)\n",
    "    # Convert labels into a NumPy array for alignment\n",
    "    return np.vstack(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a36a33b3-1d91-44cb-86a4-dc83bf7685a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10, 62)\n",
      "y shape: (10,)\n",
      "First row of X (first 10 values): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# --- Execute TF-IDF matrix construction ---\n",
    "\n",
    "X, y = make_Xy_dense_tfidf(\n",
    "    docs_all,      # list of documents\n",
    "    y,             # label array\n",
    "    term2idx,      # vocabulary mapping\n",
    "    idf,           # IDF vector\n",
    "    tf_mode=\"raw\", # TF scheme: \"raw\", \"log\", or \"norm\"\n",
    "    l2_normalize=True\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"First row of X (first 10 values):\", X[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b0272d82-2ddd-4e41-bcc7-ffa7e9b1e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    Returns 0.0 if either vector has zero norm.\n",
    "    \"\"\"\n",
    "      # Compute the product of the L2 norms of the two vectors\n",
    "    denom = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(vec1, vec2) / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5c734166-a32f-4450-8426-335f0a41c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_neighbors_cosine(X_train, y_train, x_test, k=3):\n",
    "    \"\"\"\n",
    "    Return top-k neighbors as a list of (similarity, label, index).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a list to store similarity results\n",
    "    sims = []\n",
    "    # Iterate over all training vectors\n",
    "    for i in range(len(X_train)):\n",
    "        # Compute cosine similarity between the test vector and the i-th training vector\n",
    "        sim = cosine_similarity(x_test, X_train[i])\n",
    "        sims.append((sim, y_train[i], i))\n",
    "\n",
    "    sims.sort(key=lambda x: x[0], reverse=True)\n",
    "    return sims[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7ba29253-ef67-4553-8bc3-abb6bc25bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def majority_vote(neighbors):\n",
    "    \"\"\"\n",
    "    Predict label by majority vote from neighbors.\n",
    "    neighbors: list of (sim, label, index)\n",
    "    \"\"\"\n",
    "    # Extract labels from the neighbors and count how many times\n",
    "    votes = Counter(lbl for _, lbl, _ in neighbors)\n",
    "\n",
    "    # Select the label with the highest vote count\n",
    "    return votes.most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cec691ba-8c1c-4995-a3aa-5b0cb2df95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict_cosine(X_train, y_train, x_test, k=3):\n",
    "    \"\"\"\n",
    "    Predict label for one test vector using cosine kNN.\n",
    "    Returns (pred_label, neighbors).\n",
    "    \"\"\"\n",
    "    neighbors = get_top_k_neighbors_cosine(X_train, y_train, x_test, k=k)\n",
    "    # Find the top-k nearest neighbors of the test vector\n",
    "    # using cosine similarity\n",
    "\n",
    "    pred = majority_vote(neighbors)\n",
    "    # Predict the label by majority voting among the k neighbors\n",
    "\n",
    "    return pred, neighbors\n",
    "    # Return the predicted label along with the neighbor list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d6eaad7e-05b4-41bc-a01c-0e629ba6ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform leave-one-out kNN predictions for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "34bbe752-7298-4865-8d89-3f0536c7cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict_all_loo(X, y, k=3):\n",
    "    \"\"\"\n",
    "    Leave-one-out predictions for all samples.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    # Number of samples in the dataset\n",
    "\n",
    "    y_pred = np.zeros(n, dtype=y.dtype)\n",
    "    # Initialize an array to store predicted labels\n",
    "\n",
    "    for i in range(n):\n",
    "        # Iterate over each sample as the test instance\n",
    "\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        # Remove the i-th sample from the feature matrix to form the training set\n",
    "\n",
    "        y_train = np.delete(y, i, axis=0)\n",
    "        # Remove the corresponding label from the training labels\n",
    "\n",
    "        pred, _ = knn_predict_cosine(X_train, y_train, X[i], k=k)\n",
    "        # Predict the label of the held-out sample using kNN\n",
    "\n",
    "        y_pred[i] = pred\n",
    "        # Store the prediction for the i-th sample\n",
    "\n",
    "    return y_pred\n",
    "    # Return predictions for all samples under leave-one-out evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8b2ed057-31bf-461f-a1e0-65e95680fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Evaluation: accuracy / top-k accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1675c1eb-08a0-4ea0-82cd-5e51995d06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute simple accuracy.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    # Convert true labels to a NumPy array for vectorized comparison\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Convert predicted labels to a NumPy array\n",
    "\n",
    "    return (y_true == y_pred).mean()\n",
    "    # Compare predictions with true labels element-wise,\n",
    "    # and return the proportion of correct predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f8ff729-aca8-4e4d-acd5-37b6d435ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOO top-k hit rate: true label appears among k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0cf921cd-cfae-4785-8df0-1e7b4ee6b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_hit_rate_knn_loo(X, y, k=3):\n",
    "    \"\"\"\n",
    "    Leave-one-out top-k hit rate:\n",
    "    count as correct if the true label appears among the k neighbor labels.\n",
    "    (This is not the same as standard top-k classification accuracy with probabilities.)\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    # Counter for the number of correct hits\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        # Iterate over each sample as the test instance (leave-one-out)\n",
    "\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        # Remove the i-th sample from the feature matrix to form the training set\n",
    "\n",
    "        y_train = np.delete(y, i, axis=0)\n",
    "        # Remove the corresponding label from the training labels\n",
    "\n",
    "        _, neighbors = knn_predict_cosine(X_train, y_train, X[i], k=k)\n",
    "        # Find the top-k nearest neighbors for the held-out sample\n",
    "\n",
    "        neighbor_labels = [lbl for _, lbl, _ in neighbors]\n",
    "        # Extract the labels of the k neighbors\n",
    "\n",
    "        if y[i] in neighbor_labels:\n",
    "            # Check if the true label appears among the neighbor labels\n",
    "            correct += 1\n",
    "            # Count as a correct hit\n",
    "\n",
    "    return correct / len(X)\n",
    "    # Return the proportion of samples whose true label\n",
    "    # appears among the top-k neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "38cf67f3-b41f-48d0-be93-1cfff57f2fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOO Accuracy (k=3): 0.4000\n",
      "LOO Top-3 Hit Rate: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# --- Build TF-IDF feature matrix ---\n",
    "X, y = make_Xy_dense_tfidf(\n",
    "    docs_all,\n",
    "    y,\n",
    "    term2idx,\n",
    "    idf,\n",
    "    tf_mode=\"raw\",\n",
    "    l2_normalize=True\n",
    ")\n",
    "\n",
    "# --- Leave-One-Out kNN predictions ---\n",
    "k = 3\n",
    "y_pred = knn_predict_all_loo(X, y, k=k)\n",
    "\n",
    "# --- Evaluation ---\n",
    "acc = accuracy(y, y_pred)\n",
    "topk_hit = top_k_hit_rate_knn_loo(X, y, k=k)\n",
    "\n",
    "print(f\"LOO Accuracy (k={k}): {acc:.4f}\")\n",
    "print(f\"LOO Top-{k} Hit Rate: {topk_hit:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ef6d13aa-e42f-42d3-9993-a67a6d91219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about': 0, 'after': 1, 'amid': 2, 'among': 3, 'announced': 4, 'boosted': 5, 'breaking': 6, 'caused': 7, 'change': 8, 'company': 9, 'complaints': 10, 'confidence': 11, 'conflicts': 12, 'customer': 13, 'customers': 14, 'delayed': 15, 'drop': 16, 'due': 17, 'employee': 18, 'employees': 19, 'excellent': 20, 'for': 21, 'from': 22, 'frustration': 23, 'have': 24, 'improved': 25, 'in': 26, 'increased': 27, 'internal': 28, 'investor': 29, 'issues': 30, 'its': 31, 'launch': 32, 'new': 33, 'outage': 34, 'performance': 35, 'policy': 36, 'praised': 37, 'product': 38, 'profits': 39, 'quarter': 40, 'quarterly': 41, 'received': 42, 'recently': 43, 'record': 44, 'reported': 45, 'resigned': 46, 'revenue': 47, 'reviews': 48, 'satisfaction': 49, 'service': 50, 'several': 51, 'significant': 52, 'successful': 53, 'system': 54, 'technical': 55, 'the': 56, 'this': 57, 'to': 58, 'update': 59, 'users': 60, 'was': 61}\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(term2idx)\n",
    "print(len(term2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32fdc5-d236-45c5-8f07-c975ebe6b936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
