{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbc8af8-98cd-494d-aeab-3bb6fbc8fcc3",
   "metadata": {},
   "source": [
    "This notebook implements a TF-IDF–based tiered inverted index for efficient top-K retrieval, demonstrated on toy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b16709-2fac-49db-a63e-49605f4dd089",
   "metadata": {},
   "source": [
    "# ==================================================\n",
    "# Pipeline Overview (Tiered Index with TF-IDF)\n",
    "# ==================================================\n",
    "# 0) Toy data\n",
    "#    docs = {doc_id: sentence}\n",
    "#\n",
    "# 1) Tokenizer (leveled-up)\n",
    "#    - lowercase\n",
    "#    - regex: extract alphanumeric tokens\n",
    "#    - (optional) remove stopwords\n",
    "#    - (optional) remove tokens of length <= 1\n",
    "#    -> doc_tokens = {doc_id: [term, term, ...]}\n",
    "#\n",
    "# 2) Build basic inverted index\n",
    "#    inv_index[term][doc_id] = tf\n",
    "#    -> term -> {doc_id: tf}\n",
    "#\n",
    "# 3) Compute DF & IDF\n",
    "#    df(term) = number of documents containing term\n",
    "#    idf(term) = log((N+1)/(df+1)) + 1   (toy smoothing)\n",
    "#    -> idf[term]\n",
    "#\n",
    "# 4) TF-IDF postings (sorted)\n",
    "#    score = tf * idf(term)\n",
    "#    postings_tfidf[term] = [(doc_id, score), ...] sorted desc\n",
    "#\n",
    "# 5) Split into tiers (super simple rule)\n",
    "#    Toy:    tier1 top-1 / tier2 next-1 / tier3 rest\n",
    "#    Real-ish: tier1 top-50 / tier2 next-200 / rest\n",
    "# ==================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32c32185-97dc-4c64-863f-8169f307c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from typing import List\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ff13b934-d82a-4e94-9dcb-a051824a82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Toy data\n",
    "docs = {\n",
    "    0: \"nlp search search model\",\n",
    "    1: \"search engine ranking\",\n",
    "    2: \"nlp language model\",\n",
    "    3: \"cat dog dog\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "48a8f645-b9a1-4057-b6cf-97324b836bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP = {\"the\", \"a\", \"an\", \"is\", \"and\", \"or\", \"to\", \"of\", \"in\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7da8707a-29ea-4e95-979c-5ec88a177de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Advanced tokenizer\n",
    "def tokenize(text: str) -> List[str]:\n",
    "\n",
    "    # 1) lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #2)Regex :extract alphanumeric tokens\n",
    "    tokens = re.findall(r\"[a-z0-9]+\", text)\n",
    "\n",
    "    # 3) remove stopwords and short tokens\n",
    "    tokens = [t for t in tokens if t not in STOP and len(t) > 1]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "36d786b5-19b2-4585-848f-90169d90aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Apply tokenizer to each document (doc_id -> token list)\n",
    "# - Input:  docs = {doc_id: raw_text}\n",
    "# - Output: doc_tokens = {doc_id: [term1, term2, ...]}\n",
    "doc_tokens = {doc_id: tokenize(sentence) for doc_id, sentence in docs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d2fd4794-3039-45b3-9675-0274f1836479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== doc_tokens ===\n",
      "0 : ['nlp', 'search', 'search', 'model']\n",
      "1 : ['search', 'engine', 'ranking']\n",
      "2 : ['nlp', 'language', 'model']\n",
      "3 : ['cat', 'dog', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== doc_tokens ===\")\n",
    "for k, v in doc_tokens.items():\n",
    "    print(k, \":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ddb8c6e-435f-46bd-9c6f-23efec613a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3.Build basic inverted index\n",
    "# - Input:  doc_tokens = {doc_id: [term, term, ...]}\n",
    "# - Output: inv_index = {term: {doc_id: tf}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "80364413-e6d0-48f0-b89c-489f8dc9255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) empty space to store inv_index term → {doc_id: tf}\n",
    "inv_index = defaultdict(dict)\n",
    "\n",
    "# iterate doc_tokens which generated from step2\n",
    "for doc_id, tokens in doc_tokens.items():\n",
    "    tf = Counter(tokens)           # term frequency in this document\n",
    "    for term, freq in tf.items():  # iterate tf and extract term and freq\n",
    "        inv_index[term][doc_id] = freq  # store into inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73721875-bc9f-4077-94cb-9bc48abb5e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inverted Index (pretty view) ===\n",
      "Term: 'cat'\n",
      "  doc 3: tf = 1\n",
      "Term: 'dog'\n",
      "  doc 3: tf = 2\n",
      "Term: 'engine'\n",
      "  doc 1: tf = 1\n",
      "Term: 'language'\n",
      "  doc 2: tf = 1\n",
      "Term: 'model'\n",
      "  doc 0: tf = 1\n",
      "  doc 2: tf = 1\n",
      "Term: 'nlp'\n",
      "  doc 0: tf = 1\n",
      "  doc 2: tf = 1\n",
      "Term: 'ranking'\n",
      "  doc 1: tf = 1\n",
      "Term: 'search'\n",
      "  doc 0: tf = 2\n",
      "  doc 1: tf = 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Inverted Index (pretty view) ===\")\n",
    "for term in sorted(inv_index.keys()):\n",
    "    print(f\"Term: '{term}'\")\n",
    "    for doc_id, tf in inv_index[term].items():\n",
    "        print(f\"  doc {doc_id}: tf = {tf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d069be91-1335-4db6-a178-10c0b1bbc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute DF & IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "83539d0e-549a-4691-89a8-534a77574d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq_from_inv(inv_index):\n",
    "    \"\"\"\n",
    "    Compute document frequency (DF) from inverted index.\n",
    "    DF(term) = number of documents containing the term.\n",
    "    \"\"\"\n",
    "    return {term: len(postings) for term, postings in inv_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1147e2f7-668c-42ea-8c6f-4b5757826f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Document Frequency (DF) ===\n",
      "cat        -> df = 1\n",
      "dog        -> df = 1\n",
      "engine     -> df = 1\n",
      "language   -> df = 1\n",
      "model      -> df = 2\n",
      "nlp        -> df = 2\n",
      "ranking    -> df = 1\n",
      "search     -> df = 2\n"
     ]
    }
   ],
   "source": [
    "# Compute DF from inverted index\n",
    "df = doc_freq_from_inv(inv_index)\n",
    "\n",
    "print(\"=== Document Frequency (DF) ===\")\n",
    "for term in sorted(df.keys()):\n",
    "    print(f\"{term:10s} -> df = {df[term]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "792dbda3-94f7-4315-ac10-bacbade21b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = doc_freq_from_inv(inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4c0c59c5-0555-4ee1-a48c-c26326dd8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute IDF with add-one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b30f104-51bf-47b1-b60c-05259ef6a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Compute IDF with add-one smoothing\n",
    "# idf(term) = log((N + 1) / (df + 1)) + 1\n",
    "def compute_idf(df, N):\n",
    "    \"\"\"\n",
    "    Compute IDF values from document frequency.\n",
    "    N: total number of documents\n",
    "    \"\"\"\n",
    "    return {\n",
    "        term: math.log((N + 1) / (df_t + 1)) + 1.0\n",
    "        for term, df_t in df.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4532c69-55cd-415a-8f88-84e677dfb870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IDF values ===\n",
      "cat        -> idf = 1.9163\n",
      "dog        -> idf = 1.9163\n",
      "engine     -> idf = 1.9163\n",
      "language   -> idf = 1.9163\n",
      "model      -> idf = 1.5108\n",
      "nlp        -> idf = 1.5108\n",
      "ranking    -> idf = 1.9163\n",
      "search     -> idf = 1.5108\n"
     ]
    }
   ],
   "source": [
    "# total number of documents\n",
    "N = len(doc_tokens)\n",
    "\n",
    "# compute DF from inverted index\n",
    "df = doc_freq_from_inv(inv_index)\n",
    "\n",
    "# compute IDF\n",
    "idf = compute_idf(df, N)\n",
    "\n",
    "print(\"=== IDF values ===\")\n",
    "for term in sorted(idf.keys()):\n",
    "    print(f\"{term:10s} -> idf = {idf[term]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "28b3e80f-fd93-48fa-82b8-008071c2b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Build TF-IDF postings (term -> [(doc_id, score)])\n",
    "# - Input:  inv_index (term -> {doc_id: tf}), idf (term -> idf)\n",
    "# - Output: tfidf_postings (term -> [(doc_id, tf-idf), ...]) sorted desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0819dd1e-334e-4716-8a91-798e1d07219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_postings ={}\n",
    "for term, postings in inv_index.items():\n",
    "    list=[]\n",
    "    for doc_id,tf in postings.items():\n",
    "        score = tf * idf[term]\n",
    "        list.append((doc_id, score))\n",
    "\n",
    "    #sort by descending order\n",
    "    list.sort(key=lambda x: (-x[1], x[0]))\n",
    "    tfidf_postings[term] = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "875da3c0-f14a-4f6d-a199-d31ee8a9f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TF-IDF Postings (term -> [(doc_id, score)]) ===\n",
      "cat        -> [(3, 1.9163)]\n",
      "dog        -> [(3, 3.8326)]\n",
      "engine     -> [(1, 1.9163)]\n",
      "language   -> [(2, 1.9163)]\n",
      "model      -> [(0, 1.5108), (2, 1.5108)]\n",
      "nlp        -> [(0, 1.5108), (2, 1.5108)]\n",
      "ranking    -> [(1, 1.9163)]\n",
      "search     -> [(0, 3.0217), (1, 1.5108)]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TF-IDF Postings (term -> [(doc_id, score)]) ===\")\n",
    "for term in sorted(tfidf_postings.keys()):\n",
    "    scores = [(d, round(s, 4)) for d, s in tfidf_postings[term]]\n",
    "    print(f\"{term:10s} -> {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e60396f7-0c8b-4716-9713-9d72329a369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tiered Index (term -> tier1/tier2/tier3) ===\n",
      "cat        -> {'tier1': [(3, 1.916290731874155)], 'tier2': [], 'tier3': []}\n",
      "dog        -> {'tier1': [(3, 3.83258146374831)], 'tier2': [], 'tier3': []}\n",
      "engine     -> {'tier1': [(1, 1.916290731874155)], 'tier2': [], 'tier3': []}\n",
      "language   -> {'tier1': [(2, 1.916290731874155)], 'tier2': [], 'tier3': []}\n",
      "model      -> {'tier1': [(0, 1.5108256237659907)], 'tier2': [(2, 1.5108256237659907)], 'tier3': []}\n",
      "nlp        -> {'tier1': [(0, 1.5108256237659907)], 'tier2': [(2, 1.5108256237659907)], 'tier3': []}\n",
      "ranking    -> {'tier1': [(1, 1.916290731874155)], 'tier2': [], 'tier3': []}\n",
      "search     -> {'tier1': [(0, 3.0216512475319814)], 'tier2': [(1, 1.5108256237659907)], 'tier3': []}\n"
     ]
    }
   ],
   "source": [
    "# Step 6. Split postings into tiers (toy rule: 1 / 1 / rest)\n",
    "def split_tiers(sorted_postings, t1=1, t2=1):\n",
    "    tier1 = sorted_postings[:t1]\n",
    "    tier2 = sorted_postings[t1:t1+t2]\n",
    "    tier3 = sorted_postings[t1+t2:]\n",
    "    return {\"tier1\": tier1, \"tier2\": tier2, \"tier3\": tier3}\n",
    "\n",
    "tiered_index = {\n",
    "    term: split_tiers(postings, t1=1, t2=1)\n",
    "    for term, postings in tfidf_postings.items()\n",
    "}\n",
    "\n",
    "# Print tiered index\n",
    "print(\"=== Tiered Index (term -> tier1/tier2/tier3) ===\")\n",
    "for term in sorted(tiered_index.keys()):\n",
    "    print(f\"{term:10s} -> {tiered_index[term]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c301f8e-1c37-44d3-864e-08f858a328c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tiered Index (docs only) ===\n",
      "cat        -> {'tier1': [3], 'tier2': [], 'tier3': []}\n",
      "dog        -> {'tier1': [3], 'tier2': [], 'tier3': []}\n",
      "engine     -> {'tier1': [1], 'tier2': [], 'tier3': []}\n",
      "language   -> {'tier1': [2], 'tier2': [], 'tier3': []}\n",
      "model      -> {'tier1': [0], 'tier2': [2], 'tier3': []}\n",
      "nlp        -> {'tier1': [0], 'tier2': [2], 'tier3': []}\n",
      "ranking    -> {'tier1': [1], 'tier2': [], 'tier3': []}\n",
      "search     -> {'tier1': [0], 'tier2': [1], 'tier3': []}\n"
     ]
    }
   ],
   "source": [
    "tiered_docs_only = {\n",
    "    term: {\n",
    "        \"tier1\": [d for d, s in tiers[\"tier1\"]],\n",
    "        \"tier2\": [d for d, s in tiers[\"tier2\"]],\n",
    "        \"tier3\": [d for d, s in tiers[\"tier3\"]],\n",
    "    }\n",
    "    for term, tiers in tiered_index.items()\n",
    "}\n",
    "\n",
    "print(\"=== Tiered Index (docs only) ===\")\n",
    "for term in sorted(tiered_docs_only.keys()):\n",
    "    print(f\"{term:10s} -> {tiered_docs_only[term]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a12545-8e22-4e8f-a20a-95ffc92e1b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6ab85-5db9-485e-b825-2eea42c68f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102932c9-cfc0-40ea-8f2d-27951733e4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24c13a-b29f-49ae-b1b2-985cedf07a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
