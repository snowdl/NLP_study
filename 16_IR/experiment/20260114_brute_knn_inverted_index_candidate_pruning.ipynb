{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14f5b8c6-94fb-458d-ab07-7755be5db908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "from time import perf_counter\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e519d-aaf3-4cdd-a7be-13347658e502",
   "metadata": {},
   "source": [
    "A synthetic corpus is required because it allows precise control over term overlap between documents, which is difficult to achieve with real-world data. This control is essential for isolating and evaluating the conditions under which an inverted index improves kNN efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "06361dd4-2909-4eda-b96c-d636dca52e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) # Synthetic corpus generator\n",
    "# - Used to explicitly control the degree of token overlap between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e8c7b37d-0bcd-48e6-9e92-af0b7d658b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synth_corpus(\n",
    "    n_topics=5,         # number of topics to generate\n",
    "    docs_per_topic=200, # number of documents per topic\n",
    "    doc_len=80,         # number of tokens per document\n",
    "    v_common=200,       # size of the shared (common) vocabulary\n",
    "    v_topic=200,        # size of the topic-specific vocabulary\n",
    "    p_common=0.1,       # proportion of common tokens in each document (0–1);\n",
    "                        # e.g., 0.0 = no overlap, 1.0 = all tokens shared\n",
    "    seed=0,             # random seed for reproducibility\n",
    "):\n",
    "\n",
    "    # Vocabulary structure\n",
    "\n",
    "    # This section defines the types of tokens before generating any documents.\n",
    "    #\n",
    "    # 1) Common tokens (c*): create overlap across documents\n",
    "    # 2) Topic-specific tokens (t{topic}_*): appear only within one topic\n",
    "\n",
    "    # Step 1. Initialize RNG for reproducibility\n",
    "    random_state = np.random.default_rng(seed)\n",
    "\n",
    "    # Step 1-1. Create common vocabulary: c0, c1, c2, ...\n",
    "    common_vocab = [f\"c{i}\" for i in range(v_common)]\n",
    "\n",
    "    # Step 1-2. Create topic-specific vocabularies: t0_0, t0_1, t1_0, ...\n",
    "    topic_vocab = {}\n",
    "    for t in range(n_topics):\n",
    "        topic_vocab[t] = [f\"t{t}_{i}\" for i in range(v_topic)]\n",
    "\n",
    "    texts = []   # stores document texts\n",
    "    labels = []  # stores topic labels\n",
    "\n",
    "\n",
    "    # Document generation loop\n",
    "\n",
    "    for topic in range(n_topics):\n",
    "        for _ in range(docs_per_topic):\n",
    "\n",
    "            # Step 2-1. Determine token counts per document\n",
    "            n_common = int(doc_len * p_common)  # number of common tokens\n",
    "            n_topic = doc_len - n_common        # number of topic-specific tokens\n",
    "\n",
    "            # Step 2-2. Initialize token list\n",
    "            words = []\n",
    "\n",
    "            # Step 2-3. Sample common tokens\n",
    "            words += list(\n",
    "                random_state.choice(common_vocab, size=n_common, replace=True)\n",
    "            )\n",
    "\n",
    "            # Step 2-4. Sample topic-specific tokens (no cross-topic overlap)\n",
    "            words += list(\n",
    "                random_state.choice(topic_vocab[topic], size=n_topic, replace=True)\n",
    "            )\n",
    "\n",
    "            # Step 2-5. Shuffle token order\n",
    "            random_state.shuffle(words)\n",
    "\n",
    "            # Step 2-6. Store document as a single string\n",
    "            texts.append(\" \".join(words))\n",
    "\n",
    "            # Step 2-7. Store topic label\n",
    "            labels.append(topic)\n",
    "\n",
    "    # Return generated documents and labels\n",
    "    return texts, np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "724c17ae-3b97-4f54-8cef-765babd116dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 t0_152 t0_90 t0_143 t0_171 t0_86 c17 t0_40 t0_139 t0_147 c130 t0_167 t0_105 c87 t0_25 c154 t0_102 t0_17 t0_157 t0_195 t0_18\n",
      "1 0 t0_89 t0_184 t0_99 t0_148 t0_148 t0_30 t0_155 t0_109 t0_8 t0_151 t0_135 t0_72 c140 t0_178 c70 t0_136 t0_38 c13 c194 t0_93\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "texts, labels = make_synth_corpus(n_topics=2, docs_per_topic=2, doc_len=20, p_common=0.2, seed=42)\n",
    "for i in range(2):\n",
    "    print(i, labels[i], texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642261a7-8ac2-41eb-9b93-23f2d94e9ba2",
   "metadata": {},
   "source": [
    "***Pipeline overview***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219987cd-8eae-46da-aeda-167a958865d2",
   "metadata": {},
   "source": [
    "1. tokenize() Splits raw text into clean tokens.\n",
    "2. vectorize() Transforms tokenized documents into vector representations\n",
    "3. L2 normalization() Normalizes vectors so that document length does not affect similarity.\n",
    "4. Vectorize call (train/test setup) Applies the vectorization pipeline to training and test sets for temporary evaluation and experimentation.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "86beee22-d73c-47c0-9d9e-5477b37bb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords are removed to reduce vocabulary size and noise.\n",
    "STOP = {\n",
    "    \"a\", \"an\", \"the\",\n",
    "    \"and\", \"or\", \"but\",\n",
    "    \"is\", \"are\", \"was\", \"were\",\n",
    "    \"of\", \"to\", \"in\", \"on\", \"for\", \"with\",\n",
    "    \"as\", \"by\", \"at\", \"from\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1ff3d-aa9c-491b-8163-194a1b1f98e9",
   "metadata": {},
   "source": [
    "**Step 1. tokenize():**\n",
    "Splits raw text into clean tokens and removes stopwords\n",
    "to reduce noise and stabilize vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "689e1d32-5338-4ed7-8afb-2154d1a93691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, use_stopwords: bool = True, min_len: int = 2):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"[a-z0-9_]+\", text.lower())\n",
    "\n",
    "    if use_stopwords:\n",
    "        tokens = [t for t in tokens if t not in STOP]\n",
    "\n",
    "    tokens = [t for t in tokens if len(t) >= min_len]\n",
    "\n",
    "    return tokens if tokens else [\"_empty_\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30c1e4-019b-4020-9c14-291c99eb4061",
   "metadata": {},
   "source": [
    "**Step 2 vectorize():**\n",
    "transforms raw text into normalized TF-IDF vectors for similarity-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d72e23ce-beea-467e-aa8f-98a6652212fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train_texts, test_texts, max_features=None):\n",
    "    # Step 2-1. Configure TF-IDF vectorizer\n",
    "    vec = TfidfVectorizer(\n",
    "        tokenizer=tokenize,      # use custom tokenizer\n",
    "        token_pattern=None,      # required when a custom tokenizer is provided\n",
    "        lowercase=False,         # lowercasing is already handled in tokenize()\n",
    "        max_features=max_features\n",
    "    )\n",
    "    \n",
    "    # Step 2-2. Learn vocabulary from training data\n",
    "    # Train–test splitting is intentionally kept outside this function\n",
    "    # to keep the vectorization step modular and reusable.\n",
    "    X_train = vec.fit_transform(train_texts)\n",
    "    X_test = vec.transform(test_texts)\n",
    "\n",
    "    # Step 2-3. L2-normalize vectors to make cosine similarity length-invariant\n",
    "    X_train = normalize(X_train, norm=\"l2\")\n",
    "    X_test = normalize(X_test, norm=\"l2\")\n",
    "\n",
    "    # Step 2-4. Convert TF-IDF matrices to CSR format\n",
    "    # for efficient row-wise operations and similarity computations.\n",
    "    return vec, X_train.tocsr(), X_test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8edc03bb-b1bd-4829-8e73-15e8bcaefe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k IDs: [0 1]\n",
      "Top-k scores: [0.67068255 0.67068255]\n"
     ]
    }
   ],
   "source": [
    "# minimal debug setup\n",
    "vec, X_train, X_test = vectorize(train_texts, test_texts)\n",
    "\n",
    "q_row = X_test[0]\n",
    "k = 2\n",
    "\n",
    "top_ids, top_scores = brute_topk(X_train, q_row, k)\n",
    "print(\"Top-k IDs:\", top_ids)\n",
    "print(\"Top-k scores:\", top_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505e259-8174-4acf-a748-e33b6ec91872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ef034ba-bc16-4994-ae17-92e9f6a49b93",
   "metadata": {},
   "source": [
    "***Evaluation***\n",
    "\n",
    "\n",
    "*BruteForce*\n",
    "\n",
    "\n",
    "*Inverted Index kNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7ece7-8f38-43aa-b320-b80623d6d57e",
   "metadata": {},
   "source": [
    "Brute-force kNN (k-Nearest Neighbors) is the simplest baseline implementation of kNN. For each test (query) document, it computes the similarity (or distance) between that document and every training document, then selects the top-k most similar (or closest) training documents as the “neighbors.” The predicted label is usually determined by majority vote (classification) or averaging (regression) over those k neighbors.\n",
    "\n",
    "In other words, brute-force kNN performs no pruning or indexing: it always evaluates all training documents per query. This makes it straightforward and exact, but potentially slow when N is large. It is commonly used as a baseline to verify that optimized methods (e.g., inverted-index candidate pruning or approximate nearest neighbor search) produce the same kNN results while reducing computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "605bffcf-5cb8-41cd-9d41-9954447b721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Brute-force kNN :This function provides an exact brute-force kNN baseline by computing cosine similarity between a query and all training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "28c9bfca-2f00-461e-a03f-93bef15d0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_topk(X_train, q_row, k):\n",
    "    \"\"\"\n",
    "    Compares a query document (q_row) against all training documents (X_train)\n",
    "    and returns the top-k most similar documents.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Step 1-1 Compute similarity scores\n",
    "\n",
    "    # Cosine similarity is computed via dot product\n",
    "    # Result: one similarity score per training document\n",
    "    scores = cosine_similarity(X_train, q_row).flatten()\n",
    "\n",
    " \n",
    "    # Step 1-2 Sort document indices by score (descending)\n",
    "    # argsort returns indices of sorted values\n",
    "    # We use -scores to sort from highest to lowest\n",
    "    sorted_doc_ids = np.argsort(-scores)\n",
    "\n",
    "\n",
    "    # Step 1-3 Select top-k documents\n",
    "    #e.g:\n",
    "    #scores = [0.82, 0.10, 0.56, 0.33]\n",
    "    #sorted_doc_ids = [0, 2, 3, 1]\n",
    "    # k = 2\n",
    "    \n",
    "    top_k_doc_ids = sorted_doc_ids[:k]\n",
    "    top_k_scores = scores[top_k_doc_ids]\n",
    "\n",
    "    # Return top-k document indices and their similarity scores\n",
    "    return top_k_doc_ids, top_k_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "77978be8-c60b-48ad-b847-d1dc1c86b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 Vote : Given k nearest neighbor document indices (doc_ids),predict the label by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4c5ba877-99a3-49f2-82e3-bfbb4cad32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote(labels, doc_ids):\n",
    "\n",
    "\n",
    "    # Step 2-1.  Retrieve labels of selected documents\n",
    "    selected_labels = labels[doc_ids]\n",
    "\n",
    "\n",
    "    # Step 2-2 Count label occurrences\n",
    "    count = Counter(selected_labels)\n",
    "\n",
    "\n",
    "    # Step 2-3 Select the most frequent label\n",
    "    # most_common(1) returns [(label, frequency)]\n",
    "    predicted_label = count.most_common(1)[0][0]\n",
    "\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fd05e89c-71e0-48a4-8e5e-4a8e8cc4afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k document IDs:\n",
      "[0 1]\n",
      "\n",
      "Top-k scores:\n",
      "[0.67068255 0.67068255]\n",
      "\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "# brute-force kNN \n",
    "top_ids, top_scores = brute_topk(X_train, q_row, k)\n",
    "print(\"Top-k document IDs:\")\n",
    "print(top_ids)\n",
    "print()\n",
    "\n",
    "print(\"Top-k scores:\")\n",
    "print(top_scores)\n",
    "print()\n",
    "\n",
    "\n",
    "# Voting\n",
    "pred_label = vote(labels, top_ids)\n",
    "print(\"Predicted label:\", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a7994ee2-6a8d-4976-887d-6e2457b8470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy train labels (for debugging / pipeline check)\n",
    "y_train = np.arange(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aab8aab4-75fd-444c-98fd-00c45a85341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3.  inverted index :An inverted index first determines which documents are worth comparingby checking whether they share terms with the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bbd9abda-d29a-4586-a561-87fdb63991ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def build_inv_index(X_train: csr_matrix):\n",
    "    \"\"\"\n",
    "    Builds an inverted index from TF-IDF vectorized training data (X_train).\n",
    "\n",
    "    Returns:\n",
    "        inv : dict\n",
    "            term_id -> list of document IDs\n",
    "            A mapping from each term to the documents in which it appears.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3-1. Initialize inverted index\n",
    "    # key   : term_id (column index in the TF-IDF matrix)\n",
    "    # value : array of document IDs containing the term\n",
    "    inv = defaultdict(np.ndarray)\n",
    "\n",
    "    # Step 3-2. Convert CSR matrix to CSC matrix\n",
    "    # CSR: efficient for row-based access (documents)\n",
    "    # CSC: efficient for column-based access (terms)\n",
    "    # Inverted index requires term-wise (column-wise) access\n",
    "    Xc = X_train.tocsc()\n",
    "\n",
    "    # Step 3-3. Iterate over all terms in the vocabulary\n",
    "    # Xc.shape[1] = vocabulary size\n",
    "    for t in range(Xc.shape[1]):\n",
    "\n",
    "        # Extract the column corresponding to term t\n",
    "        col = Xc.getcol(t)\n",
    "\n",
    "        # Step 3-4. Store documents containing the term\n",
    "        # col.nnz = number of non-zero entries (documents containing the term)\n",
    "        if col.nnz:\n",
    "            inv[t] = col.indices\n",
    "\n",
    "    return inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ce07e1b4-3173-488f-a164-62243f12464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inverted Index Sample ===\n",
      "term 0 → documents [1]\n",
      "term 1 → documents [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "# Create inverted index\n",
    "inv = build_inv_index(X_train)\n",
    "\n",
    "# Number of terms to display\n",
    "n_show = 2\n",
    "\n",
    "print(\"=== Inverted Index Sample ===\")\n",
    "for i, (term_id, doc_ids) in enumerate(inv.items()):\n",
    "    print(f\"term {term_id} → documents {doc_ids.tolist()}\")\n",
    "    if i + 1 == n_show:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bc62b4f3-edef-49bb-8aef-ee9b98d31473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def get_candidates(inv, q_row: csr_matrix):\n",
    "\n",
    "    # Step 1. Get term IDs that appear in the query document\n",
    "    # q_row.indices contains term IDs with non-zero TF-IDF values\n",
    "    # e.g., q_row.indices = [1, 5, 9]\n",
    "    terms = q_row.indices\n",
    "\n",
    "    # Step 2. If the query has no terms, return an empty array\n",
    "    if len(terms) == 0:\n",
    "        return np.array([], dtype=np.int32)\n",
    "\n",
    "    # Step 3. Use a set to store candidate document IDs\n",
    "    # (avoids duplicates across multiple query terms)\n",
    "    # e.g., inv[1] -> [doc_ids]\n",
    "    candidate_docs = set()\n",
    "\n",
    "    # Step 4. For each term in the query, collect matching documents\n",
    "    for t in terms:\n",
    "        if t in inv:\n",
    "            candidate_docs.update(inv[t])\n",
    "\n",
    "    # Step 5. Convert the set of document IDs to a NumPy array\n",
    "    return np.fromiter(candidate_docs, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d86298c-50f8-4d0d-9a27-4053a1b97c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query document index: 0\n",
      "Number of candidates: 2\n",
      "Candidate doc IDs (first 10 docs): [0 1]\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "query_id = 0\n",
    "q_row = X_test[query_id]\n",
    "\n",
    "# candidate \n",
    "cand = candidates(inv, q_row)\n",
    "\n",
    "print(\"Query document index:\", query_id)\n",
    "print(\"Number of candidates:\", len(cand))\n",
    "print(\"Candidate doc IDs (first 10 docs):\", cand[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4a633d32-2db5-463a-bbf9-add02aec3f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training documents: 2\n",
      "Candidate ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of training documents:\", X_train.shape[0])\n",
    "print(\"Candidate ratio:\", len(cand) / X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb285e9-92ea-4958-811a-56388589faf5",
   "metadata": {},
   "source": [
    "With brute-force kNN, the query is compared against all 150 training documents.\n",
    "\n",
    "With inverted-index-based candidate selection, only the documents that share at least one term with the query are evaluated.\n",
    "In this setting (p_common = 0.0), the query shares terms only with documents from the same topic, resulting in about 50 candidate documents.\n",
    "\n",
    "As a result, the number of similarity computations is reduced from 150 to 50, achieving roughly a 3× reduction in computation without changing the kNN result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0973a-6d05-4a9e-ab5f-42a8e679e07d",
   "metadata": {},
   "source": [
    "#5) Inverted-index kNN (candidate pruning):\n",
    "\"Instead of comparing the query against every training document, we first collect only the candidate documents that share at least one term with the query, and then run kNN within that reduced set.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3ac5ea9e-4ffd-4910-a71f-94c1929c78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def inv_topk(X_train: csr_matrix, q_row: csr_matrix, inv, k: int):\n",
    "    \"\"\"\n",
    "    kNN with candidate pruning using an inverted index.\n",
    "\n",
    "    1) Collect candidate docs that share at least one term with the query\n",
    "    2) Compute cosine similarity only for candidate docs\n",
    "    3) Return top-k documents among the candidates\n",
    "\n",
    "    Returns:\n",
    "      - top-k document IDs (global doc_ids in X_train)\n",
    "      - top-k scores\n",
    "      - number of candidate documents\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Collect candidate documents (term overlap with the query)\n",
    "    cand = get_candidates(inv, q_row)\n",
    "\n",
    "    # 2) If no candidates exist (no shared terms)\n",
    "    if len(cand) == 0:\n",
    "        return cand, np.array([], dtype=float), 0\n",
    "\n",
    "    # 3) Compute scores only over candidate documents\n",
    "    X_cand = X_train[cand]\n",
    "    scores = cosine_similarity(X_cand, q_row).ravel()\n",
    "\n",
    "    # 4) Select top-k by score (descending)\n",
    "    if len(scores) <= k:\n",
    "        idx = np.argsort(-scores)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores, k - 1)[:k]\n",
    "        idx = idx[np.argsort(-scores[idx])]\n",
    "\n",
    "    # 5) Return top-k docs, scores, and candidate count\n",
    "    return cand[idx], scores[idx], len(cand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4b47017c-adbf-4b03-b14e-e7522a275879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query id: 0 | True label: 0\n",
      "Candidates evaluated: 2 / 2\n",
      "Top-3 (doc_id, score): [(0, 0.6706825478673185), (1, 0.6706825478673185)]\n"
     ]
    }
   ],
   "source": [
    "# inv_topk Debugging\n",
    "query_id = 0\n",
    "k = 5\n",
    "\n",
    "top_ids, top_scores, n_cand = inv_topk(X_train, X_test[query_id], inv, k)\n",
    "\n",
    "print(\"Query id:\", query_id, \"| True label:\", labels[query_id])\n",
    "print(\"Candidates evaluated:\", n_cand, \"/\", X_train.shape[0])\n",
    "print(\"Top-3 (doc_id, score):\", list(zip(top_ids[:3], top_scores[:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7be563e3-4553-46e4-bd29-2a296d60afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f2d6b3cb-9032-4615-bb9b-abd1960048b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: 2\n",
      "Brute pred: 0\n",
      "Inv   pred: 0\n"
     ]
    }
   ],
   "source": [
    "inv = build_inv_index(X_train)\n",
    "\n",
    "qi = 0\n",
    "q = X_test[qi]\n",
    "k = 5\n",
    "\n",
    "b_ids, _ = brute_topk(X_train, q, k)\n",
    "b_pred = vote(y_train, b_ids)\n",
    "\n",
    "i_ids, _, csz = inv_topk(X_train, q, inv, k)\n",
    "i_pred = vote(y_train, i_ids) if len(i_ids) else -1\n",
    "\n",
    "print(\"Candidates:\", csz)\n",
    "print(\"Brute pred:\", b_pred)\n",
    "print(\"Inv   pred:\", i_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b83e8-b347-4eb9-bad2-6ecafb73d7bb",
   "metadata": {},
   "source": [
    "***Demonstration***\n",
    "\n",
    "Out of all training documents, only 2 were selected as candidates for comparison, indicating successful candidate pruning.\n",
    "\n",
    "The brute-force prediction and the inverted-index prediction are identical.\n",
    "This means that the full comparison (brute force) and the candidate-based comparison (inverted index) produce the same result.\n",
    "\n",
    "In other words, accuracy is preserved while computational cost is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62b12c3e-5e23-48c5-807a-4e85762d10fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2a38333-5809-4ae0-ac6e-aec71078aeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_build_time: 0.0005273330025374889\n",
      "avg_brute_time: 0.0010814580018632114\n",
      "avg_inv_time: 0.00043562499922700226\n",
      "avg_cand_size: 2.0\n",
      "median_cand_size: 2.0\n",
      "same_topk_ratio: 1.0\n",
      "same_pred_ratio: 1.0\n",
      "n_queries: 1\n"
     ]
    }
   ],
   "source": [
    "results = run_compare(X_train, labels, X_test, labels, k=5, n_queries=50, seed=42)\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2fb92367-de9d-447b-b5f6-08db45c0f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7) overlap ratio 조절 스윕"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4109a-ed9a-46d3-bf94-726c6255a9da",
   "metadata": {},
   "source": [
    "**Purpose**\n",
    "\n",
    "This experiment analyzes how the degree of token overlap (p_common) affects candidate pruning efficiency and kNN performance.\n",
    "\n",
    "Procedure\n",
    "\n",
    "Generate synthetic corpora while varying p_common to control vocabulary overlap between documents.\n",
    "\n",
    "Vectorize documents using TF-IDF with L2 normalization.\n",
    "\n",
    "Compare brute-force kNN and inverted-index kNN across multiple queries.\n",
    "\n",
    "Measure:\n",
    "\n",
    "average candidate set size\n",
    "\n",
    "candidate ratio (candidates / total documents)\n",
    "\n",
    "runtime of brute vs inverted kNN\n",
    "\n",
    "consistency of top-k results\n",
    "\n",
    "consistency of predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "da1da3ea-0cf0-4ab4-b751-078ab13244ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_common | cand_ratio | avg_cand | brute_time | inv_time | same_topk | same_pred\n",
      "0.0      | 0.333      | 50.0      | 0.000316s  | 0.000444s  | 1.00        | 1.00\n",
      "0.01     | 0.333      | 50.0      | 0.000275s  | 0.000386s  | 1.00        | 1.00\n",
      "0.02     | 0.333      | 50.0      | 0.000250s  | 0.000350s  | 1.00        | 1.00\n",
      "0.05     | 0.347      | 52.0      | 0.000247s  | 0.000346s  | 1.00        | 1.00\n",
      "0.1      | 0.384      | 57.6      | 0.000246s  | 0.000344s  | 1.00        | 1.00\n",
      "0.2      | 0.514      | 77.0      | 0.000246s  | 0.000347s  | 1.00        | 1.00\n"
     ]
    }
   ],
   "source": [
    "# Step 7-0 Sweep different overlap ratios (p_common)\n",
    "p_list = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n",
    "sweep_results = []\n",
    "\n",
    "for p in p_list:\n",
    "   \n",
    "    # Step 7-1 Generate a synthetic corpus with controlled overlap\n",
    "    #    p_common controls the proportion of shared tokens\n",
    "    texts, labels = make_synth_corpus(\n",
    "        n_topics=3,\n",
    "        docs_per_topic=50,\n",
    "        doc_len=40,\n",
    "        p_common=p,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 7-2  Define train / test sets\n",
    "    #    - No explicit split here (controlled synthetic setup)\n",
    "    #    - Use the first 50 documents as queries\n",
    "    train_texts = texts\n",
    "    test_texts = texts[:50]\n",
    "\n",
    "    # Step 7-3 Vectorize documents with TF-IDF + L2 normalization\n",
    "    vec, X_train, X_test = vectorize(train_texts, test_texts)\n",
    "\n",
    "\n",
    "    # Step 7-4 Run brute-force vs inverted-index kNN comparison\n",
    "    result = run_compare(\n",
    "        X_train=X_train,\n",
    "        y_train=labels,\n",
    "        X_test=X_test,\n",
    "        y_test=labels,\n",
    "        k=5,\n",
    "        n_queries=50,\n",
    "        seed=0\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 7-5 Record overlap ratio and normalized candidate size\n",
    "    result[\"p_common\"] = p\n",
    "    result[\"candidate_ratio\"] = result[\"avg_cand_size\"] / X_train.shape[0]\n",
    "\n",
    "    sweep_results.append(result)\n",
    "\n",
    "# Step 7-6 Print a concise summary of results for each p_common\n",
    "print(\"p_common | cand_ratio | avg_cand | brute_time | inv_time | same_topk | same_pred\")\n",
    "for r in sweep_results:\n",
    "    print(\n",
    "        f\"{r['p_common']:<7}  | \"\n",
    "        f\"{r['candidate_ratio']:.3f}      | \"\n",
    "        f\"{r['avg_cand_size']:.1f}      | \"\n",
    "        f\"{r['avg_brute_time']:.6f}s  | \"\n",
    "        f\"{r['avg_inv_time']:.6f}s  | \"\n",
    "        f\"{r['same_topk_ratio']:.2f}        | \"\n",
    "        f\"{r['same_pred_ratio']:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd3b88-4292-4c11-8910-926f820d29db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71b6fb81-d1c5-4406-ac82-60de2cfa2fa9",
   "metadata": {},
   "source": [
    "**My Final Thoughts**\n",
    "\n",
    "As the overlap ratio increases, the average candidate set size grows, reducing the computational advantage of inverted-index–based kNN.\n",
    "When overlap is low, candidate pruning significantly reduces the number of similarity computations while preserving identical top-k neighbors and predictions compared to brute-force kNN.\n",
    "At higher overlap ratios, most documents become candidates, and the benefit of pruning gradually diminishes.\n",
    "\n",
    "This experiment explicitly tests the following hypothesis:\n",
    "\n",
    "As vocabulary overlap increases (p_common ↑), candidate pruning becomes less effective, while prediction consistency remains high.\n",
    "\n",
    "More concretely:\n",
    "\n",
    "Low p_common\n",
    "\n",
    "Very small candidate sets\n",
    "\n",
    "Large speedup over brute force\n",
    "\n",
    "Top-k neighbors and predictions are usually identical\n",
    "\n",
    "High p_common\n",
    "\n",
    "Candidate set size approaches the full corpus\n",
    "\n",
    "Computational speed advantage decreases\n",
    "\n",
    "Prediction accuracy remains stable\n",
    "\n",
    "Overall, these results confirm the expected and desired behavior of an inverted-index–based kNN system:\n",
    "efficient computation under low overlap without sacrificing correctness, and graceful degradation as overlap increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10367c3f-2e18-41ab-b656-46f5605cf05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
