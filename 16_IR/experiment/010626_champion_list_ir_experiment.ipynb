{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884e6017-d609-406c-8690-20ce0194a029",
   "metadata": {},
   "source": [
    "***The pipeline demonstrates how champion lists reduce search cost while preserving ranking quality.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64424de-7086-42f5-b4ac-62272c0ead96",
   "metadata": {},
   "source": [
    "Step 1: build_synthetic_corpus\n",
    "Generates a controlled synthetic corpus for IR experiments.\n",
    "\n",
    "Step 2:doc_frequency(docs)\n",
    "Computes document frequency to inspect term distribution.\n",
    "\n",
    "Step 3:build_sparse_tfidf(docs)\n",
    "Converts documents into sparse TF-IDF vectors.\n",
    "\n",
    "Step 4:build_inverted_index_from_tfidf(tfidf)\n",
    "Builds an inverted index mapping terms to documents.\n",
    "\n",
    "Step 5: build_champion_list(inv_idx, r)\n",
    "Selects top-r high-weight documents per term to reduce candidates.\n",
    "\n",
    "Step 6 : tfidf_to_dense(tfidf, term2idx, N)\n",
    "Converts sparse TF-IDF vectors into a dense matrix.\n",
    "\n",
    "Step 7: cosine_dense(a, b)\n",
    "Computes cosine similarity between two vectors.\n",
    "\n",
    "Step 8: vectorize_query(query_terms, term2idx, idf)\n",
    "Represents a query in the same TF-IDF space as documents.\n",
    "\n",
    "Step 9:full_search_dense(...)\n",
    "Performs exhaustive search over all documents as a baseline.\n",
    "\n",
    "Step 10: champion_search_dense(...)\n",
    "Performs approximate search using champion lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d76ef1e4-2d9a-4c17-987e-b964afa251d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23eeb26d-99c3-413a-ba87-287aa25ace3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : Build build_synthetic_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df6e8d-e42d-4249-92b3-51631fe927a6",
   "metadata": {},
   "source": [
    "We use a synthetic corpus to control DF/TF patterns so that the impact of champion lists on candidate reduction and top-K preservation is clearly observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09e01269-48bf-44f0-93cf-d75773b1a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-1 vocab 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd1f9100-2e00-48fc-98d9-f7885d0ec8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab_sets(\n",
    "\n",
    "    topics_n=5,\n",
    "    topic_vocab_size=120,\n",
    "    common_vocab_size=120,\n",
    "    head_terms_n=25,\n",
    "    super_common_n=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Create controlled token sets for a synthetic corpus.\n",
    "    Returns:\n",
    "      topics: dict[str, list[str]]   (topic -> topic-specific terms)\n",
    "      common: list[str]             (moderately common terms)\n",
    "      head_terms: list[str]         (high-DF terms to create long posting lists)\n",
    "      super_common: list[str]       (stopword-like ultra-high-DF terms)\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over the topic vocabulary size and generate unique terms\n",
    "    # by combining the topic index (i) and the word index (j).\n",
    "    # Topic-specific vocabularies: t{i}_w{j}\n",
    "    topics = {\n",
    "        f\"topic{i}\": [f\"t{i}_w{j}\" for j in range(topic_vocab_size)]\n",
    "        #This loop iterates over the number of topics.\n",
    "        for i in range(topics_n)\n",
    "    }\n",
    "\n",
    "    # Common terms shared across topics: c_w{j}\n",
    "    common = [f\"c_w{j}\" for j in range(common_vocab_size)]\n",
    "\n",
    "    # Head terms: controllable high-frequency tokens\n",
    "    # Head terms simulate ultra-high-frequency tokens to create long posting lists.\n",
    "    head_terms = [f\"head_w{j}\" for j in range(head_terms_n)]\n",
    "\n",
    "    # Super common terms: stopword-like ultra-high DF tokens\n",
    "    super_common = [f\"sw_w{j}\" for j in range(super_common_n)]\n",
    "\n",
    "    return topics, common, head_terms, super_common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0614403a-00ad-41fc-a29a-2297414a375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 build_synthetic_corpus_v2()\n",
    "# → Responsible only for the document generation flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8affa961-f7ff-4609-89ea-a4d79111fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_synthetic_corpus_v2(\n",
    "    n_docs=100,\n",
    "    seed=42,\n",
    "    topics_n=5,\n",
    "    topic_vocab_size=120,\n",
    "    common_vocab_size=120,\n",
    "    doc_len_range=(20, 35),\n",
    "    topic_ratio=0.7,\n",
    "    burst_prob=0.45,\n",
    "    burst_repeat_range=(2, 6),\n",
    "    super_common_n=15,\n",
    "    super_common_ratio=0.12,\n",
    "    super_common_df_target=0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Extended synthetic corpus generator for champion list experiments.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Structure the vocabulary into disjoint sets by role:\n",
    "    # topic-specific, common, head, and super-common.\n",
    "    topics, common, head_terms, super_common = make_vocab_sets(\n",
    "        topics_n=topics_n,\n",
    "        topic_vocab_size=topic_vocab_size,\n",
    "        common_vocab_size=common_vocab_size,\n",
    "        head_terms_n=25,\n",
    "        super_common_n=super_common_n\n",
    "    )\n",
    "\n",
    "    # Store document texts and their topic labels (labels help for kNN/classification).\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    for doc_id in range(n_docs):\n",
    "        # Randomly assign a topic ID to this document\n",
    "        topic_id = random.randrange(topics_n)\n",
    "        topic_name = f\"topic{topic_id}\"\n",
    "        labels.append(topic_id)\n",
    "\n",
    "        # Choose document length\n",
    "        L = random.randint(*doc_len_range)\n",
    "\n",
    "        # Allocate super-common tokens (stopword-like) in most documents,\n",
    "        # so that these terms have very high DF (df ≈ N).\n",
    "        n_super = 0\n",
    "        if random.random() < super_common_df_target:\n",
    "            n_super = max(1, int(L * super_common_ratio))\n",
    "\n",
    "        # Distribute remaining tokens between topic-specific and regular common terms\n",
    "        L_rem = max(1, L - n_super)\n",
    "        n_topic = int(L_rem * topic_ratio)\n",
    "        n_common = L_rem - n_topic\n",
    "\n",
    "        words = []\n",
    "        words += random.choices(topics[topic_name], k=n_topic)\n",
    "        words += random.choices(common, k=n_common)\n",
    "\n",
    "        if n_super > 0:\n",
    "            words += random.choices(super_common, k=n_super)\n",
    "\n",
    "        # Add head terms to create long posting lists\n",
    "        if random.random() < 0.55:\n",
    "            words += random.sample(head_terms, k=random.randint(1, 3))\n",
    "\n",
    "        # Burst to create high TF for some topic terms\n",
    "        if random.random() < burst_prob:\n",
    "            burst_word = random.choice(topics[topic_name])\n",
    "            words += [burst_word] * random.randint(*burst_repeat_range)\n",
    "\n",
    "        random.shuffle(words)\n",
    "        docs.append(\" \".join(words))\n",
    "\n",
    "    return docs, labels, topics, common, head_terms, super_common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bbb9665-15a7-4232-9976-e3c19f7286c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, labels, topics, common, head_terms, super_common = build_synthetic_corpus_v2(\n",
    "    n_docs=100,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6e3483f-a4ba-4e84-84e9-ccaa2a059391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def doc_frequency(docs): Quick inspection of term-level document frequency (DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59b2c1e6-ebc0-40ea-8510-9f91ff157f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample head terms df:\n",
      "head_w0 7\n",
      "head_w1 1\n",
      "head_w2 5\n",
      "head_w3 6\n",
      "head_w4 8\n",
      "head_w5 2\n",
      "head_w6 10\n",
      "head_w7 2\n",
      "head_w8 2\n",
      "head_w9 7\n",
      "\n",
      "Rare-ish terms df (df == 1 ~ 3) examples:\n"
     ]
    }
   ],
   "source": [
    "def doc_frequency(docs):\n",
    "    df = Counter()\n",
    "    for text in docs:\n",
    "        # Each document is split into whitespace-separated tokens,\n",
    "        # and each term contributes at most 1 to document frequency (DF).\n",
    "        df.update(set(text.split()))\n",
    "    return df\n",
    "\n",
    "df = doc_frequency(docs)\n",
    "\n",
    "# Head terms are expected to have relatively large DF values\n",
    "print(\"Sample head terms df:\")\n",
    "for t in head_terms[:10]:\n",
    "    print(t, df[t])\n",
    "\n",
    "# Some terms should be very frequent, while others should be rare,\n",
    "# so that the champion list becomes meaningful\n",
    "print(\"\\nRare-ish terms df (df == 1 ~ 3) examples:\")\n",
    "rare = [t for t, c in df.items() if 1 <= c <= 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ac551ba-2b4f-4a94-affa-33449634cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_sparse_tfidf(docs):  \n",
    "# Build sparse TF-IDF representations and auxiliary indices\n",
    "# for IR and champion list experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a238d81e-9f83-4f92-9a10-5b896a728282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def build_sparse_tfidf(docs):\n",
    "\n",
    "    # Step 1. tokenize (already tokenized; split by space)\n",
    "    # Simple whitespace tokenization is sufficient here\n",
    "    # because the synthetic corpus already consists of pre-tokenized terms.\n",
    "\n",
    "    tokenized = [doc.split() for doc in docs]\n",
    "    N = len(tokenized)\n",
    "\n",
    "    # Step 2. DF\n",
    "    # Use set(toks) so that each term contributes at most once per document,\n",
    "    # which matches the definition of document frequency (DF).\n",
    "\n",
    "    df = Counter()\n",
    "    for toks in tokenized:\n",
    "        df.update(set(toks))\n",
    "\n",
    "    # Step 3. Add-one smoothed IDF to avoid idf=0\n",
    "    # Add-one (Laplace) smoothing:\n",
    "    # - avoids division by zero\n",
    "    # - keeps IDF well-defined even for very frequent terms\n",
    "    idf = {\n",
    "        term: math.log((N + 1) / (df_t + 1)) + 1.0\n",
    "        for term, df_t in df.items()\n",
    "    }\n",
    "\n",
    "    # Step 4. Compute sparse TF-IDF\n",
    "    # Store TF-IDF vectors in sparse form (only non-zero entries).\n",
    "    tfidf = {}  # Output dictionary: doc_id -> {term: tf-idf value}\n",
    "\n",
    "    for doc_id, toks in enumerate(tokenized):\n",
    "        # 1) Compute term frequency (TF)\n",
    "        tf = Counter(toks)\n",
    "\n",
    "        # 2) Compute TF-IDF weights\n",
    "        tfidf[doc_id] = {\n",
    "            term: tf_count * idf[term]\n",
    "            for term, tf_count in tf.items()\n",
    "        }\n",
    "\n",
    "    # Step 5. vocabulary index  (IMPORTANT: outside the loop)\n",
    "    # Build a global vocabulary index for later conversion\n",
    "    # to dense vectors and cosine similarity computation.\n",
    "    vocab = sorted(df.keys())\n",
    "    term2idx = {term: idx for idx, term in enumerate(vocab)}\n",
    "\n",
    "    # Return after all documents are processed\n",
    "    return tfidf, term2idx, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2d73d44-f8ce-4eff-9332-d31f95f0d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf, term2idx, idf = build_sparse_tfidf(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af2291bc-646d-4ea6-88d5-b2d6b38a577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index_from_tfidf(tfidf):\n",
    "    \"\"\"\n",
    "    Build an inverted index from sparse TF-IDF representations.\n",
    "\n",
    "    Args:\n",
    "        tfidf (dict[int, dict[str, float]]):\n",
    "            Mapping from document ID to a sparse TF-IDF vector.\n",
    "            Example:\n",
    "              {\n",
    "                0: {\"t0_w1\": 0.32, \"c_w5\": 0.11},\n",
    "                1: {\"t1_w3\": 0.45, \"head_w2\": 0.08},\n",
    "                ...\n",
    "              }\n",
    "\n",
    "    Returns:\n",
    "        inv_idx (dict[str, list[tuple[int, float]]]):\n",
    "            Inverted index mapping each term to a posting list of\n",
    "            (doc_id, tf-idf weight) pairs.\n",
    "            Example:\n",
    "              {\n",
    "                \"t0_w1\": [(0, 0.32), (5, 0.27), ...],\n",
    "                \"head_w2\": [(1, 0.08), (7, 0.06), ...],\n",
    "                ...\n",
    "              }\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty inverted index.\n",
    "    # defaultdict(list) automatically creates an empty list\n",
    "    # when a new term is encountered.\n",
    "    inv_idx = defaultdict(list)\n",
    "\n",
    "    # Iterate over documents\n",
    "    for doc_id, term_dict in tfidf.items():\n",
    "        # term_dict is the sparse TF-IDF vector for this document:\n",
    "        # {term: tf-idf weight}\n",
    "\n",
    "        # Iterate over terms appearing in the document\n",
    "        for term, weight in term_dict.items():\n",
    "            # Append (doc_id, weight) to the posting list of this term\n",
    "            inv_idx[term].append((doc_id, weight))\n",
    "\n",
    "    return inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4252cb5e-b941-4d69-b3f0-e5cb864d4e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num terms in inv_idx: 720\n"
     ]
    }
   ],
   "source": [
    "inv_idx = build_inverted_index_from_tfidf(tfidf)\n",
    "print(\"num terms in inv_idx:\", len(inv_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8a7b119-5c2e-4972-a91a-21f05068cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_champion_list(inv_idx, r=30):\n",
    "    \"\"\"\n",
    "    Build a champion list for each term.\n",
    "\n",
    "    Args:\n",
    "        inv_idx (dict[str, list[tuple[int, float]]]):\n",
    "            Inverted index mapping each term to a posting list of\n",
    "            (doc_id, tf-idf weight) pairs.\n",
    "\n",
    "        r (int):\n",
    "            Number of top documents to keep per term (champion size).\n",
    "\n",
    "    Returns:\n",
    "        champion (dict[str, list[tuple[int, float]]]):\n",
    "            Champion list mapping each term to its top-r postings,\n",
    "            sorted by descending TF-IDF weight.\n",
    "    \"\"\"\n",
    "\n",
    "    champion = {}\n",
    "\n",
    "    # Iterate over each term and its posting list\n",
    "    for term, postings in inv_idx.items():\n",
    "        # Sort postings by TF-IDF weight in descending order\n",
    "        # and keep only the top-r documents\n",
    "        champion[term] = sorted(\n",
    "            postings,\n",
    "            key=lambda x: x[1],  # x[1] = TF-IDF weight\n",
    "            reverse=True\n",
    "        )[:r]\n",
    "\n",
    "    return champion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fdfd0d4e-e833-4668-8de3-13c75ee77933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of terms: 720\n",
      "Champion list size per term (r): 30\n",
      "\n",
      "Sample term: c_w50\n",
      "Posting list length (full): 8\n",
      "Champion list length: 8\n",
      "\n",
      "Top-5 champion postings (doc_id, tf-idf):\n",
      "doc 0, weight=3.4179\n",
      "doc 4, weight=3.4179\n",
      "doc 24, weight=3.4179\n",
      "doc 28, weight=3.4179\n",
      "doc 41, weight=3.4179\n"
     ]
    }
   ],
   "source": [
    "# Build champion list\n",
    "r = 30\n",
    "champion = build_champion_list(inv_idx, r=r)\n",
    "\n",
    "print(\"Total number of terms:\", len(inv_idx))\n",
    "print(\"Champion list size per term (r):\", r)\n",
    "\n",
    "# Pick a sample term to inspect\n",
    "sample_term = next(iter(champion.keys()))\n",
    "\n",
    "print(\"\\nSample term:\", sample_term)\n",
    "print(\"Posting list length (full):\", len(inv_idx[sample_term]))\n",
    "print(\"Champion list length:\", len(champion[sample_term]))\n",
    "\n",
    "print(\"\\nTop-5 champion postings (doc_id, tf-idf):\")\n",
    "for doc_id, weight in champion[sample_term][:5]:\n",
    "    print(f\"doc {doc_id}, weight={weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "983f454a-945b-4146-96c9-7a587a4fe964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Term: head_w0\n",
      "Full postings: 7\n",
      "Champion postings: 7\n",
      "\n",
      "Term: head_w1\n",
      "Full postings: 1\n",
      "Champion postings: 1\n",
      "\n",
      "Term: head_w2\n",
      "Full postings: 5\n",
      "Champion postings: 5\n"
     ]
    }
   ],
   "source": [
    "for t in head_terms[:3]:\n",
    "    print(\"\\nTerm:\", t)\n",
    "    print(\"Full postings:\", len(inv_idx[t]))\n",
    "    print(\"Champion postings:\", len(champion[t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57236fe3-d441-4392-a631-de957c288cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest term: sw_w3 df= 23\n",
      "Champion len: 23\n",
      "Top 5: [(1, 4.874133372986627), (34, 4.874133372986627), (63, 4.874133372986627), (81, 4.874133372986627), (4, 2.4370666864933135)]\n"
     ]
    }
   ],
   "source": [
    "term_longest = max(inv_idx.keys(), key=lambda t: len(inv_idx[t]))\n",
    "print(\"Longest term:\", term_longest, \"df=\", len(inv_idx[term_longest]))\n",
    "print(\"Champion len:\", len(champion[term_longest]))\n",
    "print(\"Top 5:\", champion[term_longest][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca4a505d-64ac-43b4-a701-112c3b135a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tfidf shape: (100, 720)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_to_dense(tfidf, term2idx, N):\n",
    "    V = len(term2idx)\n",
    "    X = np.zeros((N, V))\n",
    "    for doc_id, term_dict in tfidf.items():\n",
    "        for term, val in term_dict.items():\n",
    "            X[doc_id, term2idx[term]] = val\n",
    "    return X\n",
    "\n",
    "\n",
    "N = len(docs)\n",
    "X_tfidf = tfidf_to_dense(tfidf, term2idx, N)\n",
    "print(\"X_tfidf shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87757717-0c5c-4047-8aa2-c7c344fd71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_dense(a, b):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two dense vectors.\n",
    "\n",
    "    Args:\n",
    "        a (np.ndarray): dense vector (e.g., query vector)\n",
    "        b (np.ndarray): dense vector (e.g., document vector)\n",
    "\n",
    "    Returns:\n",
    "        float: cosine similarity value in [0, 1]\n",
    "    \"\"\"\n",
    "    # Compute L2 norms\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "\n",
    "    # Handle zero vectors\n",
    "    if norm_a == 0.0 or norm_b == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    # Cosine similarity = (a · b) / (||a|| * ||b||)\n",
    "    return float(np.dot(a, b) / (norm_a * norm_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a69332f3-75da-445a-aa47-0eee902b4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def vectorize_query(query_terms, term2idx, idf):\n",
    "    \"\"\"\n",
    "    Convert a query (list of terms) into a dense TF-IDF vector.\n",
    "\n",
    "    Args:\n",
    "        query_terms (List[str]): tokenized query terms\n",
    "        term2idx (dict[str, int]): mapping from term to column index\n",
    "        idf (dict[str, float]): inverse document frequency values\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: dense query vector of shape (V,)\n",
    "    \"\"\"\n",
    "    # Initialize dense query vector\n",
    "    q = np.zeros((len(term2idx),), dtype=float)\n",
    "\n",
    "    # Compute term frequency in the query\n",
    "    q_tf = Counter(query_terms)\n",
    "\n",
    "    # Fill the vector with TF-IDF weights\n",
    "    for term, tf_count in q_tf.items():\n",
    "        #  # Ignore query terms not seen in the vocabulary\n",
    "        if term in term2idx:\n",
    "            q[term2idx[term]] = tf_count * idf.get(term, 0.0)\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "72baa95b-d341-4daf-b0ff-68f4d5a2dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline search that computes cosine similarity against all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cda3921f-e364-49d8-ae65-24b505bb9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_search_dense(query_terms, X_tfidf, term2idx, idf, topk=5):\n",
    "\n",
    "    # Vectorize the query into a dense TF-IDF vector\n",
    "    q = vectorize_query(query_terms, term2idx, idf)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Compute cosine similarity against ALL documents\n",
    "    for doc_id in range(X_tfidf.shape[0]):\n",
    "        s = cosine_dense(q, X_tfidf[doc_id])\n",
    "        if s > 0:\n",
    "            scores.append((doc_id, s))\n",
    "\n",
    "    # Sort documents by similarity score (descending)\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return top-k results and the total number of evaluated documents\n",
    "    return scores[:topk], X_tfidf.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0803dd5-28bb-4130-a17f-2dbb117e63ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query terms: ['t0_w81', 't0_w84', 'head_w6', 'c_w50']\n",
      "FULL candidates evaluated: 100\n",
      "FULL top5 results (doc_id, score):\n",
      "  doc  37  score=0.231855\n",
      "  doc  30  score=0.193575\n",
      "  doc  50  score=0.097691\n",
      "  doc  28  score=0.092566\n",
      "  doc   4  score=0.091946\n",
      "\n",
      "Top-1 document text (first 120 chars):\n",
      "sw_w8 t0_w108 t0_w9 c_w83 c_w68 t0_w84 c_w110 t0_w45 c_w33 t0_w117 t0_w57 sw_w9 t0_w110 c_w32 t0_w77 t0_w107 head_w6 t0_\n"
     ]
    }
   ],
   "source": [
    "# --- Debug / sanity check for full_search_dense ---\n",
    "\n",
    "query_terms = [\"t0_w81\", \"t0_w84\", \"head_w6\", \"c_w50\"]  # adjust as you like\n",
    "topk = 5\n",
    "\n",
    "full_res, full_cands = full_search_dense(query_terms, X_tfidf, term2idx, idf, topk=topk)\n",
    "\n",
    "print(\"Query terms:\", query_terms)\n",
    "print(\"FULL candidates evaluated:\", full_cands)\n",
    "print(f\"FULL top{topk} results (doc_id, score):\")\n",
    "for doc_id, score in full_res:\n",
    "    print(f\"  doc {doc_id:3d}  score={score:.6f}\")\n",
    "\n",
    "# Optional: inspect one returned document snippet\n",
    "if full_res:\n",
    "    best_doc_id = full_res[0][0]\n",
    "    print(\"\\nTop-1 document text (first 120 chars):\")\n",
    "    print(docs[best_doc_id][:120])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320aa12-6e67-4623-82bb-f8c2867835e5",
   "metadata": {},
   "source": [
    "#The champion list reduced the candidate set by 80% while preserving the top-5 ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0568c51a-d032-4b0a-95c0-50849c91d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def champion_search_dense(query_terms, X_tfidf, term2idx, idf, champion, topk=5):\n",
    "    \"\"\"\n",
    "    Perform champion-list-based search:\n",
    "    Instead of scoring all documents, score only candidate documents\n",
    "    collected from the union of champion postings for the query terms.\n",
    "\n",
    "    Returns:\n",
    "        top_results: top-k (doc_id, score)\n",
    "        num_candidates: number of candidate docs actually scored\n",
    "    \"\"\"\n",
    "    # Vectorize query into a dense TF-IDF vector\n",
    "    q = vectorize_query(query_terms, term2idx, idf)\n",
    "\n",
    "    # Collect candidate documents:\n",
    "    # Take the union of champion postings for each unique query term.\n",
    "    cand = set()\n",
    "    for term in set(query_terms):\n",
    "        for doc_id, _ in champion.get(term, []):\n",
    "            cand.add(doc_id)\n",
    "\n",
    "    # Score only candidate documents\n",
    "    scores = []\n",
    "    for doc_id in cand:\n",
    "        s = cosine_dense(q, X_tfidf[doc_id])\n",
    "        if s > 0:\n",
    "            scores.append((doc_id, s))\n",
    "\n",
    "    # Sort by similarity score (descending) and return top-k\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:topk], len(cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8f765-00fc-4d5a-9b9d-3bbd44a9dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Debug / comparison: FULL vs CHAMPION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab33e928-a6c9-43c5-ad9c-d66a42204cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query terms: ['t0_w81', 't0_w84', 'head_w6', 'c_w50']\n",
      "FULL   candidates: 100 topk: [(37, 0.23185539959571902), (30, 0.1935747915713828), (50, 0.0976913145431689), (28, 0.09256649153379709), (4, 0.09194558587983924)]\n",
      "CHAMP  candidates: 20 topk: [(37, 0.23185539959571902), (30, 0.1935747915713828), (50, 0.0976913145431689), (28, 0.09256649153379709), (4, 0.09194558587983924)]\n",
      "top5 overlap: 5 / 5\n",
      "\n",
      "FULL top-1 doc preview: sw_w8 t0_w108 t0_w9 c_w83 c_w68 t0_w84 c_w110 t0_w45 c_w33 t0_w117 t0_w57 sw_w9 t0_w110 c_w32 t0_w77 t0_w107 head_w6 t0_\n",
      "CHAMP top-1 doc preview: sw_w8 t0_w108 t0_w9 c_w83 c_w68 t0_w84 c_w110 t0_w45 c_w33 t0_w117 t0_w57 sw_w9 t0_w110 c_w32 t0_w77 t0_w107 head_w6 t0_\n"
     ]
    }
   ],
   "source": [
    "query_terms = [\"t0_w81\", \"t0_w84\", \"head_w6\", \"c_w50\"]\n",
    "topk = 5\n",
    "\n",
    "full_res, full_cands = full_search_dense(query_terms, X_tfidf, term2idx, idf, topk=topk)\n",
    "ch_res, ch_cands = champion_search_dense(query_terms, X_tfidf, term2idx, idf, champion, topk=topk)\n",
    "\n",
    "print(\"Query terms:\", query_terms)\n",
    "print(\"FULL   candidates:\", full_cands, \"topk:\", full_res)\n",
    "print(\"CHAMP  candidates:\", ch_cands,  \"topk:\", ch_res)\n",
    "\n",
    "# Overlap check for top-k\n",
    "full_top = [d for d, _ in full_res]\n",
    "ch_top = [d for d, _ in ch_res]\n",
    "overlap = len(set(full_top) & set(ch_top))\n",
    "\n",
    "print(f\"top{topk} overlap:\", overlap, \"/\", topk)\n",
    "\n",
    "# Optional: inspect texts of the returned docs\n",
    "if full_res:\n",
    "    print(\"\\nFULL top-1 doc preview:\", docs[full_res[0][0]][:120])\n",
    "if ch_res:\n",
    "    print(\"CHAMP top-1 doc preview:\", docs[ch_res[0][0]][:120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842238f7-f782-4843-b39e-36b1db051f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
