# Embedding Folder Structure

This directory organizes various embedding techniques used in natural language processing (NLP).

## Folder Structure

Embedding/
├── Word_Embedding/
│ ├── word2vec/ # Resources related to Word2Vec
│ ├── glove/ # Resources related to GloVe
│ ├── fasttext/ # Resources related to FastText
│ └── subword_methods/ # Subword-based embedding techniques
│
├── Sentence_Embedding/
│ ├── doc2vec/ # Resources related to Doc2Vec
│ ├── universal_sentence_encoder/ # Universal Sentence Encoder resources
│ ├── sbert/ # Sentence-BERT related resources
│ └── tfidf_lsa/ # Traditional text embeddings (TF-IDF, LSA)
│
└── Transformer_Based_Embedding/
├── bert_embedding/ # BERT-based embeddings
├── gpt_embedding/ # GPT-based embeddings
├── roberta_embedding/ # RoBERTa-based embeddings
└── openai_embedding/ # OpenAI API embedding examples


## Folder Descriptions

- **Word_Embedding**: Contains word-level embedding techniques and related implementations.  
- **Sentence_Embedding**: Includes resources and projects related to sentence and document embeddings.  
- **Transformer_Based_Embedding**: Houses examples and experiments using Transformer-based embedding models.

---

This repository collects a variety of embedding methods that are useful for both NLP research and practical applications.

---

Please explore the respective folders for more detailed content.

